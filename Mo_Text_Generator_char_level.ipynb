{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_Text Generator_char_level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "tensor-flow-2-2",
      "graded_item_id": "4eYSM",
      "launcher_item_id": "HEV6h"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Capstone-Project-Text-Generation/blob/main/Mo_Text_Generator_char_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFRRArflRfR1"
      },
      "source": [
        "## Text Generation RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3wpZrx7RfR-"
      },
      "source": [
        "This program constructs an unsupervised character-level sequence model that can generate text according to a distribution learned from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j_QtDXtRfR4"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "# ML design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text\n",
        "\n",
        "# data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "\n",
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# file management\n",
        "import urllib.request\n",
        "import os\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 692,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkHeu2U8keZ6"
      },
      "source": [
        "##### GLOBAL VARIABLES\r\n",
        "File directories and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fRagWNBR0-E",
        "outputId": "f5b5617f-5177-48ec-c58e-19769fa6ebf7"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "AUTHOR = 'assorted'\n",
        "\n",
        "# file directory structure in Google Drive\n",
        "GDRIVE_DIR = '/content/gdrive/'\n",
        "FILEPATH = GDRIVE_DIR + 'MyDrive/Colab_Notebooks/models/text_generation/' + AUTHOR\n",
        "CHECKPOINT_DIR = FILEPATH + '/checkpoints/'\n",
        "CACHE_DIR = FILEPATH + '/cache/'\n",
        "MODEL_DIR = FILEPATH + '/prediction_model/'\n",
        "DATASETS_DIR = 'https://raw.githubusercontent.com/mvenouziou/text_generator/main/'\n",
        "\n",
        "# dataset files (.txt and .csv allowed)\n",
        "DATASET_PRIMARY = 'robert_frost_assorted.txt'\n",
        "DATASETS = [DATASET_PRIMARY] + \\\n",
        "            [#'call_of_the_wild_jack_london.txt',\n",
        "      #       'treasure_island.txt',\n",
        "      #       'robinsons_crusoe.txt',\n",
        "       #      'wizard_of_oz.txt',\n",
        "       #      'sherlock_holmes.txt',\n",
        "      #       'mark_twain_speeches.txt',\n",
        "         #    'dorian_gray_oscar_wilde.txt',\n",
        "             'ezra_pound_assorted.txt',\n",
        "             'walt_whitman_leaves_of_grass.txt',\n",
        "            'ts_eliot.txt',\n",
        "             'edgar_allen_poe.txt'\n",
        "             \n",
        "            ]\n",
        "\n",
        "# shuffle dataset preference\n",
        "# must be False to use stateful RNN structure\n",
        "SHUFFLE = False   \n",
        "# stateful RNN works best with a single continuous work\n",
        "                \n",
        "\n",
        "# mount google drive:\n",
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_DIR)"
      ],
      "execution_count": 693,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppOL7TDHRfSA"
      },
      "source": [
        "#### Load and inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXwzWcB-RfSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdcc630-921d-42e5-8daf-42a7dde2b948"
      },
      "source": [
        "# Function: loader for .csv files\n",
        "def prepare_csv(filename, datasets_dir=DATASETS_DIR):\n",
        "    # load data\n",
        "    dataframe = pd.read_csv(datasets_dir + filename)\n",
        "    \n",
        "    # cleanup specific to the robert frost set\n",
        "    dataframe = dataframe.rename(columns={'Name ':'Name'})[1:]\n",
        "    dataframe['Name'] = dataframe['Name'].apply(lambda x: x + ': ')\n",
        "    \n",
        "    # merge desired text columns and export as a single string\n",
        "    dataframe['merged'] = dataframe['Name'] + dataframe['Content']\n",
        "    text_list = dataframe['merged'].to_list()\n",
        "    text_list = (' ').join(text_list)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "\n",
        "# Function: Load and standardize data files\n",
        "def load_parse(num_words_per_chunk=30, display_samples=True, \n",
        "               datasets_dir=DATASETS_DIR, datasets=DATASETS):\n",
        "    # NOTE: set shuffle=False if using a stateful model\n",
        "    \n",
        "    # initialize container to store text data\n",
        "    text_data = string.printable\n",
        "\n",
        "    # randomize order of data sources\n",
        "    random.shuffle(datasets)\n",
        "\n",
        "    first_file = True\n",
        "    for file in datasets:\n",
        "        \n",
        "        # for loading .csv files\n",
        "        _, file_extension = os.path.splitext(file)     \n",
        "        if file_extension == '.csv':\n",
        "            new_text = prepare_csv(file)    \n",
        "\n",
        "        # for loading .txt files\n",
        "        else:\n",
        "            if datasets_dir[:4] == 'http':\n",
        "                with urllib.request.urlopen(datasets_dir + '/' + file) as file:\n",
        "                    new_text = file.read().decode('utf-8')\n",
        "\n",
        "            else:\n",
        "                with open(datasets_dir + '/' + file, 'r', encoding='utf-8') as file:\n",
        "                    new_text = file.read()\n",
        "        \n",
        "        # strip empty lines and add comma to any line not ending in punctuation\n",
        "        lines = new_text.splitlines()    \n",
        "        new_text_update = ''\n",
        "        for line in lines:\n",
        "            if line != '' and line[-1] not in string.punctuation:\n",
        "                \n",
        "                line = ''.join([line, \",\"])  # add comma\n",
        "                new_text_update = ' '.join([new_text_update, line])\n",
        "\n",
        "        # merge data sources\n",
        "        # (adjust string length to be no longer than \n",
        "        # a specified multiple of first dataset length)\n",
        "        multiple = 1\n",
        "        if first_file:\n",
        "            max_length = multiple * len(new_text_update)\n",
        "            first_file = False\n",
        "\n",
        "        # enforce max length\n",
        "        new_text_update = new_text_update[: max_length]\n",
        "        text_data = (' ').join([text_data, new_text_update])\n",
        "\n",
        "\n",
        "\n",
        "    # Create a list of chunks of text\n",
        "    # remove paragraph / line marks and split up words\n",
        "    tokenizer = text.WhitespaceTokenizer()\n",
        "\n",
        "    # tokenize data (outputs bytestrings)\n",
        "    words_byte = tokenizer.tokenize(text_data)\n",
        "    words_byte = words_byte.numpy().tolist()   \n",
        "    \n",
        "    # convert data to string format\n",
        "    words = [byte.decode() for byte in words_byte]  \n",
        "    \n",
        "    # combine into lists of consecutive words\n",
        "    text_chunks = [(' ').join(words[i : i + num_words_per_chunk]) \n",
        "                    for i in range(0, len(words), num_words_per_chunk)]\n",
        "\n",
        "    # Display some text samples\n",
        "    if display_samples:\n",
        "        num_samples = 5\n",
        "        inx = np.random.choice(len(text_chunks), num_samples, replace=False)\n",
        "        for chunk in np.array(text_chunks)[inx]:\n",
        "            print(chunk)\n",
        "            print()\n",
        "\n",
        "        print('len(words):', len(words))\n",
        "        print('len(text_chunks):', len(text_chunks))\n",
        "\n",
        "    return text_chunks\n",
        "\n",
        "\n",
        "temp = load_parse(num_words_per_chunk=30, display_samples=True)"
      ],
      "execution_count": 694,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "connect the stars, and of wombs and of the, Seeing, hearing, feeling, are miracles, and each part and tag of me, Divine am I inside and out, and I make\n",
            "\n",
            "to that unfortunate woman, And what they swore in the cupboard, Or is it my tongue that wrongs you, There comes, it seems, and at any rate, Andromeda was offered\n",
            "\n",
            "to us, any more, Than was the hound that came a stranger to us, \"Home is the place where, when you have to go, \"I should have called it, Picked\n",
            "\n",
            "knew half what the flock of them know, Cranberries in bogs and raspberries on top, Of the boulder-strewn mountain, and when they, I met them one day and each had\n",
            "\n",
            "in, Meserve, \"And ready for some more? My wife here, Meserve seemed to heed nothing but the lamp, He pointed with his hand from where it lay, \"That leaf there\n",
            "\n",
            "len(words): 24691\n",
            "len(text_chunks): 824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWObVg6DRfSi"
      },
      "source": [
        "#### Encode data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8s2aR4oRfSb"
      },
      "source": [
        "# Function: Create and fit tokenizer object\n",
        "def create_character_tokenizer(list_of_strings):\n",
        "    \"\"\"\n",
        "    This function takes a list of strings as its argument. It should create \n",
        "    and return a Tokenizer according to the above specifications. \n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize standard keras tokenizer\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                    num_words=None,  # number of tokens is not limited\n",
        "                    filters=None,  # no characters filtered\n",
        "                    lower=False,  # original capitalization retained\n",
        "                    char_level=True,  # tokens created at character level\n",
        "                    )\n",
        "    \n",
        "    # fit tokenizer\n",
        "    tokenizer.fit_on_texts(list_of_strings)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# Function: apply tokenizer\n",
        "def strings_to_sequences(tokenizer, list_of_strings): \n",
        "    return tokenizer.texts_to_sequences(list_of_strings)  "
      ],
      "execution_count": 695,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-4W2iICRfS3"
      },
      "source": [
        "#### Create input and target Datasets for stateful RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLf-9KxWRfSw"
      },
      "source": [
        "# Function: Apply padding for uniform length\n",
        "def make_padded_dataset(sequence_chunks, max_len=750):\n",
        "    \"\"\"\n",
        "    This function takes a list of lists of tokenized sequences, and transforms\n",
        "    them into a 2D numpy array, padding the sequences as necessary according to\n",
        "    the above specification. The function should then return the numpy array.\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                        sequences=sequence_chunks,  # dataset\n",
        "                        maxlen=max_len, \n",
        "                        dtype='int32', \n",
        "                        padding='pre',\n",
        "                        truncating='pre', \n",
        "                        value=0.0\n",
        "                  )"
      ],
      "execution_count": 696,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLY5mT3RfS5"
      },
      "source": [
        "def create_inputs_and_targets(array_of_sequences):\n",
        "    \"\"\"\n",
        "    This function takes a 2D numpy array of token sequences, and returns a tuple of two\n",
        "    elements: the first element is the input array and the second element is the output\n",
        "    array, which are defined according to the above specification.\n",
        "    \"\"\"   \n",
        "    input_arr = array_of_sequences[:, :-1]\n",
        "    target_arr = array_of_sequences[:, 1:]\n",
        "\n",
        "    return input_arr, target_arr\n",
        "    "
      ],
      "execution_count": 697,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcoP9vOGDqlV"
      },
      "source": [
        "Note: Our text data are not independent random samples. Content from earlier in the text can inform future predictions. In this case we can use a 'stateful' model to capture some of this dependence relation.  (Data should be a single continuous (unshuffled) text for this to work. Do not use it in other instances.) \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4gsVamxRfTK"
      },
      "source": [
        "# Function: data prep to create stateful RNN batches\n",
        "def preprocess_stateful(input, target, steps=None, batch_size=BATCH_SIZE):\n",
        "\n",
        "    # Prepare input and output arrays for training the stateful RNN\n",
        "    num_examples = input.shape[0]\n",
        "\n",
        "    # we'll reduce the sample size so all batches have the same dimension\n",
        "    num_processed_examples = num_examples - (num_examples % batch_size)\n",
        "\n",
        "    # restrict data to new sample size\n",
        "    input_cropped = input[:num_processed_examples]\n",
        "    target_cropped = target[:num_processed_examples]\n",
        "\n",
        "    steps = num_processed_examples // batch_size  # num steps needed per epoch \n",
        "                                            # (to process all the batches)\n",
        "\n",
        "    # create array defining the order for samples to be processed\n",
        "    inx = np.empty((0,), dtype=np.int32)  # initialize array object\n",
        "    for i in range(steps):\n",
        "        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \n",
        "                                                 steps)))\n",
        "\n",
        "    # reorder the data\n",
        "    input_seq_stateful = input_cropped[inx]\n",
        "    target_seq_stateful = target_cropped[inx]\n",
        "\n",
        "    return input_seq_stateful, target_seq_stateful, steps"
      ],
      "execution_count": 698,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiLWfyemRfTU"
      },
      "source": [
        "# Function: Split into training and validation Dataset objects\n",
        "def make_Dataset(input, target, validation_split=.2,\n",
        "                 batch_size=BATCH_SIZE, shuffle=SHUFFLE):\n",
        "\n",
        "    sample_size = input.shape[0]\n",
        "    validation_size = int(validation_split * sample_size)\n",
        "\n",
        "    # split into train / validation sets\n",
        "    input_valid = input[:validation_size]\n",
        "    target_valid = target[:validation_size]\n",
        "    \n",
        "    input_train = input[validation_size:]\n",
        "    target_train = target[validation_size:]\n",
        "\n",
        "    # convert to tensorflow Dataset batches\n",
        "    def convert_to_Dataset(input_array, target_array):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "                            (input_array, target_array))\n",
        "        if shuffle:\n",
        "            dataset = dataset.shuffle(buffer_size=640,\n",
        "                                      reshuffle_each_iteration=True)\n",
        "        \n",
        "        dataset = (dataset.batch(batch_size, drop_remainder=True))\n",
        "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        \n",
        "        return dataset\n",
        "\n",
        "    train_dataset = convert_to_Dataset(input_train, target_train)\n",
        "    valid_dataset = convert_to_Dataset(input_valid, target_valid)\n",
        "\n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 699,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKfaWcCCRfTf"
      },
      "source": [
        "#### Define RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oauekqOSRfTi"
      },
      "source": [
        "# Function: Model Definition\n",
        "def get_model(vocab_size, stateful=False, \n",
        "              batch_size=BATCH_SIZE, embedding_dim=EMBEDDING_DIM):\n",
        "    \"\"\" Defines and compiles a Sequential RNN. \n",
        "    note: 'stateful' must be set to True in final prediction model.\n",
        "    (Use False during training if data is shuffled. ) \"\"\"\n",
        "    \n",
        "    from keras.layers import Embedding, GRU, Dense, Dropout\n",
        "\n",
        "    if not SHUFFLE:\n",
        "        stateful=True\n",
        "    if stateful:\n",
        "        steps = STEPS\n",
        "    else:\n",
        "        steps=None\n",
        "\n",
        "    model = keras.Sequential([\n",
        "                Embedding(input_dim=vocab_size, \n",
        "                          output_dim=embedding_dim,\n",
        "                          mask_zero=True, \n",
        "                          batch_input_shape=(batch_size, None)\n",
        "                          ),\n",
        "                GRU(units=1024, \n",
        "                    stateful=stateful, \n",
        "                    return_sequences=True,\n",
        "                    ),\n",
        "               GRU(units=1024, \n",
        "                    stateful=stateful, \n",
        "                    return_sequences=True,\n",
        "                    ),\n",
        "                GRU(units=1024, \n",
        "                    stateful=stateful, \n",
        "                    return_sequences=True,\n",
        "                    name='GRU_1',\n",
        "                    ),\n",
        "                Dense(embedding_dim\n",
        "                      ),\n",
        "                Dropout(rate=.05\n",
        "                         ),\n",
        "                Dense(units=vocab_size, \n",
        "                      activation=None,\n",
        "                      )          \n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                        from_logits=True),\n",
        "                  metrics=['sparse_categorical_accuracy', \n",
        "                           'sparse_categorical_crossentropy'],\n",
        "                  )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 700,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJhnIUHSRfTt"
      },
      "source": [
        "# Function: Train model\n",
        "def train_model(model, train_data, validation_data, epochs=1, \n",
        "                batch_size=BATCH_SIZE, filepath=FILEPATH, \n",
        "                checkpoint_dir=CHECKPOINT_DIR):\n",
        "\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "    # define checkpoint file naming format\n",
        "    checkpoint_filename = checkpoint_dir + \\\n",
        "            'ckpt_{epoch:02d}_{sparse_categorical_accuracy:04f}.ckpt'\n",
        "\n",
        "    # set checkpoint options\n",
        "    checkpoint = ModelCheckpoint(\n",
        "                        filepath=checkpoint_filename,\n",
        "                        monitor='sparse_categorical_accuracy',\n",
        "                        save_weights_only=True,\n",
        "                        save_best_only=False,\n",
        "                        save_freq=(1 + epochs//4) * batch_size,\n",
        "                        verbose=1)\n",
        "\n",
        "    # define early stopping criteria\n",
        "    stopping = EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    min_delta=1e-3,\n",
        "                    #patience=2,\n",
        "                    verbose=1,\n",
        "                )\n",
        "    \n",
        "    sample_size = tf.data.experimental.cardinality(train_data).numpy()\n",
        "\n",
        "    history = model.fit(train_data.repeat(epochs), \n",
        "                        validation_data=valid_data.repeat(epochs), \n",
        "                        epochs=1,\n",
        "                        validation_steps='epoch', \n",
        "                        callbacks=[checkpoint, stopping],\n",
        "                        )\n",
        "    \n",
        "    model.save_weights(checkpoint_dir + + 'end_of_run_checkpoint')\n",
        "\n",
        "    \n",
        "    return model, history"
      ],
      "execution_count": 701,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8_OrNEvKcL"
      },
      "source": [
        "### Create Text Generation Function\r\n",
        "Adapt trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYl_VlGatdzu"
      },
      "source": [
        "# Final Prediction Function #######\r\n",
        "\r\n",
        "# Use the model to generate a token sequence\r\n",
        "def make_prediction(init_string, num_generation_steps, precision_reduction=0,\r\n",
        "                    model_name=AUTHOR, print_result=False):\r\n",
        "    \r\n",
        "    our_model = prediction_model\r\n",
        "    our_tokenizer = tokenizer\r\n",
        "\r\n",
        "    GRU_layer = our_model.get_layer('GRU_1')\r\n",
        "\r\n",
        "    #batch_size=1\r\n",
        "    \r\n",
        "    token_sequence = our_tokenizer.texts_to_sequences([init_string])\r\n",
        "    initial_state = None\r\n",
        "    input_sequence = token_sequence\r\n",
        "    init_len = len(input_sequence[0])\r\n",
        "\r\n",
        "    for i in range(num_generation_steps):\r\n",
        "        logits = get_logits(our_model, input_sequence, initial_state=initial_state)\r\n",
        "        sampled_token = sample_token(logits, precision_reduction)\r\n",
        "        token_sequence[0].append(sampled_token)\r\n",
        "        input_sequence = [[sampled_token]]  # use only last letter because previous model state is carried forward\r\n",
        "        initial_state = GRU_layer.states[0].numpy()\r\n",
        "\r\n",
        "    predicted_text = our_tokenizer.sequences_to_texts(token_sequence)[0][::2]\r\n",
        "    \r\n",
        "    if print_result:\r\n",
        "        print(predicted_text)\r\n",
        "\r\n",
        "    return predicted_text"
      ],
      "execution_count": 702,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI5-5cZphPGz"
      },
      "source": [
        "*************************************\r\n",
        "*************************************\r\n",
        "\r\n",
        "### Implement Functions to Create Models\r\n",
        "\r\n",
        "*************************************\r\n",
        "*************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0sajFK7knqH"
      },
      "source": [
        "##### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2DwhSf7TbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "295f3e35-f5da-419c-9793-5d4eadef3042"
      },
      "source": [
        "def get_tokenizer():\r\n",
        "\r\n",
        "    # Load Data ##########\r\n",
        "    text_chunks = load_parse(num_words_per_chunk=25, \r\n",
        "                             display_samples=True)\r\n",
        "\r\n",
        "    # Encoding Map ##########\r\n",
        "    # Get fitted tokenizer\r\n",
        "    tokenizer = create_character_tokenizer(text_chunks)\r\n",
        "\r\n",
        "    return tokenizer, text_chunks\r\n",
        "\r\n",
        "\r\n",
        "# get tokenizer\r\n",
        "tokenizer, text_chunks = get_tokenizer()\r\n",
        "\r\n",
        "# save tokenizer\r\n",
        "with open(MODEL_DIR + '/tokenizer.pickle', 'rb') as handle:\r\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "execution_count": 703,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shall be, How do you know who shall come from the offspring of his offspring, (Who might you find you have come from yourself, if\n",
            "\n",
            "Of precisely the fashion, Doubtful, somewhat, of the value, Of well-gowned approbation, The edge, uncertain, but a means of blending, With other strata, Conduct, on\n",
            "\n",
            "the jolly one there, and more the silent one with sweat, Not a youngster is taken for larceny but I go up too, and am\n",
            "\n",
            "float, and, 7, For it the globe lay preparing quintillions of years without one, Examine these limbs, red, black, or white, they are cunning in,\n",
            "\n",
            "vivid coloring of life, As in that fleeting, shadowy, misty strife, Of semblance with reality which brings, To the delirious eye, more lovely things, _How\n",
            "\n",
            "len(words): 24691\n",
            "len(text_chunks): 988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzHw4wbCRfSH",
        "outputId": "442e1aa1-03df-4993-c9b2-56781dc3082f"
      },
      "source": [
        "def preprocess_data(tokenizer, text_chunks, cache_dir=CACHE_DIR):\n",
        "\n",
        "    # Create Datasets ##########\n",
        "    # Apply encoding to text (using tokenizer)\n",
        "    seq_chunks = strings_to_sequences(tokenizer, text_chunks)\n",
        "\n",
        "    # Pad sequence with zeros to establish uniform lengths\n",
        "    padded_sequences = make_padded_dataset(seq_chunks)\n",
        "\n",
        "    # Convert sequences into input and target arrays\n",
        "    input_seq, target_seq = create_inputs_and_targets(padded_sequences)\n",
        "\n",
        "    # rearrange data into batches for use in stateful model\n",
        "    # (cannot use if data is shuffled)\n",
        "    if SHUFFLE is False:\n",
        "        input_seq, target_seq, steps = \\\n",
        "            preprocess_stateful(input_seq, target_seq)\n",
        "    else:\n",
        "        steps = None\n",
        "\n",
        "    # Create the training and validation Datasets\n",
        "    train_data, valid_data = \\\n",
        "        make_Dataset(input_seq, target_seq, \n",
        "                     validation_split=.2)\n",
        "                    \n",
        "    # cache datasets\n",
        "    # train_data = train_data.cache(cache_dir)   \n",
        "    # valid_data = valid_data.cache(cache_dir) \n",
        "\n",
        "    print(train_data)\n",
        "\n",
        "    return train_data, valid_data, steps\n",
        "\n",
        "\n",
        "train_data, valid_data, STEPS = \\\n",
        "    preprocess_data(tokenizer, text_chunks)\n",
        "\n",
        "# note STEPS is treated as a global variable from this point forward"
      ],
      "execution_count": 704,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((32, 749), (32, 749)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj2fmp91kvai"
      },
      "source": [
        "##### Create / Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmmEfYo-RfTl"
      },
      "source": [
        "def build_init_model(train_data, valid_data, tokenizer,\n",
        "                     filepath=FILEPATH, show_summary=False):\n",
        "\n",
        "    \"\"\" Initializes and trains a model for 1 epoch, \n",
        "    and saves it to disk \"\"\"\n",
        "\n",
        "    model = get_model(vocab_size=len(tokenizer.word_index) + 1)\n",
        "\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "\n",
        "    # train the model for one epoch\n",
        "    model, history = \\\n",
        "        train_model(model, train_data, valid_data, epochs=1)\n",
        "\n",
        "    # Save model\n",
        "    model.save(filepath)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 705,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeEvkI58p2KE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        },
        "outputId": "b8f56d85-5bc3-4f2c-c73c-a096ef7e2b05"
      },
      "source": [
        "# Load existing model (if it exists). \r\n",
        "# Otherwise, create a new model\r\n",
        "\r\n",
        "try:\r\n",
        "        \r\n",
        "    # reinitialize model\r\n",
        "    init_model = get_model(len(tokenizer.word_index) + 1)\r\n",
        "\r\n",
        "    # load weights from checkpoint\r\n",
        "    checkpoint = tf.train.latest_checkpoint(CHECKPOINT_DIR)\r\n",
        "    init_model.load_weights(checkpoint)\r\n",
        "    init_model.summary()\r\n",
        "   \r\n",
        "    print('\\nLoaded saved model')\r\n",
        "\r\n",
        "except: \r\n",
        "\r\n",
        "    init_model, init_history = \\\r\n",
        "        build_init_model(train_data, valid_data, tokenizer, show_summary=True)"
      ],
      "execution_count": 706,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_67\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_67 (Embedding)     (32, None, 256)           39424     \n",
            "_________________________________________________________________\n",
            "gru_70 (GRU)                 (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "gru_71 (GRU)                 (32, None, 1024)          6297600   \n",
            "_________________________________________________________________\n",
            "GRU_1 (GRU)                  (32, None, 1024)          6297600   \n",
            "_________________________________________________________________\n",
            "dense_134 (Dense)            (32, None, 256)           262400    \n",
            "_________________________________________________________________\n",
            "dropout_67 (Dropout)         (32, None, 256)           0         \n",
            "_________________________________________________________________\n",
            "dense_135 (Dense)            (32, None, 154)           39578     \n",
            "=================================================================\n",
            "Total params: 16,874,906\n",
            "Trainable params: 16,874,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "24/24 [==============================] - ETA: 0s - loss: 0.8694 - sparse_categorical_accuracy: 0.1069 - sparse_categorical_crossentropy: 5.8761"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-706-6eac35d315f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[1;32m   2194\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m       \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_is_hdf5_filepath\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m   2813\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_is_hdf5_filepath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2814\u001b[0;31m   return (filepath.endswith('.h5') or filepath.endswith('.keras') or\n\u001b[0m\u001b[1;32m   2815\u001b[0m           filepath.endswith('.hdf5'))\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'endswith'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-706-6eac35d315f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0minit_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_history\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mbuild_init_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_summary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-705-8884c48de129>\u001b[0m in \u001b[0;36mbuild_init_model\u001b[0;34m(train_data, valid_data, tokenizer, filepath, show_summary)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-701-ac077be44c4f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_data, validation_data, epochs, batch_size, filepath, checkpoint_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                         )\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1130\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_truncate_execution_to_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     should_truncate = (\n\u001b[1;32m   1146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m         self._steps_per_execution_value > self._inferred_steps)\n\u001b[0m\u001b[1;32m   1148\u001b[0m     \u001b[0moriginal_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'int' and 'str'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU0i_C0Dx0Cy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "170c0a13-f238-42cb-dcf2-cc8ae34907f7"
      },
      "source": [
        "# add additional training epochs\r\n",
        "train_more = True\r\n",
        "additional_epochs = 50\r\n",
        "\r\n",
        "if train_more:  \r\n",
        "    init_model, init_history = train_model(init_model, \r\n",
        "                                           train_data, \r\n",
        "                                           valid_data, \r\n",
        "                                           epochs=additional_epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 155/1200 [==>...........................] - ETA: 33:30 - loss: 0.6515 - sparse_categorical_accuracy: 0.1694 - sparse_categorical_crossentropy: 4.4132"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJI3WuGmRyO-"
      },
      "source": [
        "# reinitialize model\r\n",
        "init_model = get_model(len(tokenizer.word_index) + 1)\r\n",
        "\r\n",
        "# add wights from latest checkpoint\r\n",
        "checkpoint = tf.train.latest_checkpoint(CHECKPOINT_DIR)\r\n",
        "init_model.load_weights(checkpoint)\r\n",
        "\r\n",
        "# save model\r\n",
        "init_model.save(FILEPATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n05eWlUeN9T"
      },
      "source": [
        "********************************************************\r\n",
        "********************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msBeLSoRfT7"
      },
      "source": [
        "##### Adapt model to accept single line inputs (batch size = 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzQW69qxRfT_"
      },
      "source": [
        "# Function: produce modified model for making text predictions\n",
        "def get_prediction_model(model_dir=MODEL_DIR, \n",
        "                         checkpoint_dir=CHECKPOINT_DIR):\n",
        "    \n",
        "    # load saved tokenizer\n",
        "    with open(model_dir + '/tokenizer.pickle', 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    \n",
        "    # initialize new model with batch size = 1\n",
        "    user_model = get_model(vocab_size=len(tokenizer.word_index) + 1, \n",
        "                           stateful=True, batch_size=1)\n",
        "\n",
        "    # find newest trained weights\n",
        "    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "    # load weights to prediction model\n",
        "    user_model.load_weights(checkpoint_file)  \n",
        "    user_model.trainable=False\n",
        "\n",
        "    return user_model\n",
        "\n",
        "\n",
        "# Store trained model separate from checkpoints\n",
        "def save_prediction_model(tokenizer, model_dir=MODEL_DIR):\n",
        "\n",
        "    # save model\n",
        "    prediction_model = get_prediction_model(MODEL_DIR, \n",
        "                                            CHECKPOINT_DIR)\n",
        "    prediction_model.save(model_dir)\n",
        "\n",
        "    # save tokenizer\n",
        "    with open(model_dir + '/tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Umv8D0iB95Z9"
      },
      "source": [
        "# Save the prediction model\r\n",
        "# note: this has batch_size=1 for use in generating text\r\n",
        "save_prediction = True\r\n",
        "if save_prediction:\r\n",
        "    save_prediction_model(tokenizer, model_dir=MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A0FOxOTRfUc"
      },
      "source": [
        "# Generate text from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPexjrJCm1h"
      },
      "source": [
        "# load prediction model\r\n",
        "def load_model_update(model_dir=MODEL_DIR):\r\n",
        "   \r\n",
        "    prediction_model =  \\\r\n",
        "        tf.keras.models.load_model(model_dir)\r\n",
        "\r\n",
        "    # load tokenizer\r\n",
        "    with open(model_dir + '/tokenizer.pickle', 'rb') as handle:\r\n",
        "        tokenizer = pickle.load(handle)\r\n",
        "\r\n",
        "    return prediction_model, tokenizer\r\n",
        "\r\n",
        "\r\n",
        "prediction_model, tokenizer = load_model_update() \r\n",
        "prediction_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aHg6y44RfUJ"
      },
      "source": [
        "# Function: Outputs weighted predictions for next token \n",
        "# (as logits / log-odds ration)\n",
        "def get_logits(model, token_sequence, initial_state=None):\n",
        "    \"\"\"\n",
        "    Paramater:\n",
        "    model - our prediction model set with batch size = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # carry forward previous state from GRU layer\n",
        "    GRU_layer = model.get_layer('GRU_1')\n",
        "    GRU_layer.reset_states(initial_state)\n",
        "\n",
        "    # Get the model's next token prediction (as logits)\n",
        "    input = tf.constant(token_sequence)\n",
        "    final_pred = model(input)[:, -1, :]\n",
        "        \n",
        "    return final_pred.numpy()    \n",
        "\n",
        "\n",
        "# Function: selects a value from logits prediction\n",
        "def sample_token(logits, precision_reduction=0):   \n",
        "\n",
        "    # choose a value from logits distribution\n",
        "    # fuzz_factor: adds some imprecision to results\n",
        "    fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n",
        "\n",
        "    sample = tf.random.categorical(\n",
        "                        logits=logits * (1 + precision_reduction * fuzz_factor), \n",
        "                        num_samples=1, \n",
        "                        )\n",
        "\n",
        "    # convert to integer\n",
        "    next_token = sample[0,0].numpy()\n",
        "\n",
        "    return next_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTXyxnDg6Qae"
      },
      "source": [
        "# test model\r\n",
        "def generate_text(starting_text, precision_reduction=0, author='assorted'):\r\n",
        "\r\n",
        "    num_generation_steps = 300  # max number characters to produce\r\n",
        "  \r\n",
        "    prediction = make_prediction(init_string=starting_text, \r\n",
        "                               num_generation_steps=num_generation_steps, \r\n",
        "                               precision_reduction=precision_reduction, \r\n",
        "                               model_name=author, \r\n",
        "                               print_result=True)\r\n",
        "  \r\n",
        "    split_lines_prediction = re.split('([?.,;!:])', prediction)\r\n",
        "\r\n",
        "\r\n",
        "    output = ''\r\n",
        "    i = 0\r\n",
        "    for line in split_lines_prediction:\r\n",
        "        if i%2==1:\r\n",
        "            output= ''.join([output, line])\r\n",
        "        else:\r\n",
        "            output= '\\n'.join([output, line])\r\n",
        "            \r\n",
        "        i += 1\r\n",
        "\r\n",
        "    output= '... '.join([output, line])\r\n",
        "  \r\n",
        "    return output\r\n",
        "\r\n",
        "text_generated = generate_text(starting_text='Hendrix the dog', author='assorted')\r\n",
        "print(text_generated)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}