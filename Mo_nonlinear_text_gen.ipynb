{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_nonlinear_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMu1ngd52xSL5pG2CQYbnWF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Text-Generation/blob/main/Mo_nonlinear_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQyxxivFD7r"
      },
      "source": [
        "## Text Generation RNN\n",
        "\n",
        "This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. \n",
        "\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\n",
        "- See more at https://github.com/mvenouziou/Project-Text-Generation.\n",
        "\n",
        "- See credits /attributions below\n",
        "\n",
        "The code implements two different model architectures: \"linear\" and \"nonlinear.\"\n",
        "The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. \n",
        "\n",
        "---\n",
        "\n",
        "**What's New?**\n",
        "*(These items are original in the sense that I personally have not seen them at the original time of coding. Citations are below for content I have seen elsewhere.)*\n",
        "\n",
        "- Experiments with: Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. \n",
        "\n",
        "- Experiments with: Tensorflow Probability layers to create a more interpretable probability distribution model. (Character-model only). The standard text generation algorithm outputs logits, which we view as a distribution from which to generate the next character. Here, we formalize this as outputing our model as a TF Probability Distribution, using probablistic weights in the Dense layer (instead of scalars) and trained via maximum likelihood. \n",
        "\n",
        "- *(Note: the probabalistic model produces extremely poor results when used with the parallel word-level path. Perhaps it requires a larger number of units in the inner layers to use this more nuanced model. However, the current processing power (and the desire to ultimately run this without GPU) limits our ability to increase network size.)*\n",
        "\n",
        "- Option to implement either the standard linear model architecture (see credits below) or nonlinear architectures.\n",
        "\n",
        "- Manage RNN statefulness for independent data sources. The linear model (credited below) codes' approach to statefulness imposes a dependence relation between samples / batches. This model implements the ability to treat independent works (individual poems, books, authors, etc.) as truly independent samples by resetting RNN states and shuffling independent data sources.\n",
        "\n",
        "- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) \n",
        "\n",
        "- Random crops and start locations to better match training data with desired generated text lengths.\n",
        "\n",
        "- Parameters to perturb learned probabilties in final generation function, to add extra variety to generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzU2QxoMFF7d"
      },
      "source": [
        "---\n",
        "**Credits / Citations / Attributions:**\n",
        "\n",
        "**Linear Model and Shared Code** \n",
        "\n",
        "Other than items noted in previous sections, this python code and linear model structure is based heavily on Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*\n",
        "\n",
        "\n",
        "**Nonlinear Model:**   \n",
        "\n",
        "This utilizes pretrained embeddings:\n",
        "-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" *https://tfhub.dev/google/collections/bert/1)*\n",
        "- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" *https://hub.tensorflow.google.cn/google/electra_small/2)*\n",
        "\n",
        "ELECTRA-Small++ has four times as many paramaters as the Small BERT embedding, producing better results, but at large computational cost.\n",
        "\n",
        "**Web App:** \n",
        "\n",
        "The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "- *'robert_frost_collection.csv'* is a Kaggle dataset available at https://www.kaggle.com/archanghosh/robert-frost-collection. Any other datasets used are public domain works available from Project Gutenberg https://www.gutenberg.org.\n",
        "\n",
        "---\n",
        "\n",
        "**About**\n",
        "\n",
        "Find me online at:\n",
        "- LinkedIn: https://www.linkedin.com/in/movenouziou/ \n",
        "- GitHub: https://github.com/mvenouziou\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1EW3w1GvDLx",
        "outputId": "ded5b9cf-9435-4299-974c-15f0c0d9dd55"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# TF text processing (also required for TF HUB word encoders)\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text  \n",
        "\n",
        "# TF pretrained models (for word encodings)\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# TF probability modules\n",
        "import tensorflow_probability as tfp  \n",
        "from tensorflow_probability import layers as tfpl\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "# TF TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "# data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "# file management\n",
        "import os\n",
        "import bz2\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "\n",
        "# integrations\n",
        "!pip install -q anvil-uplink\n",
        "import anvil.server"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2682GuzqxyW"
      },
      "source": [
        "### Set Model Paramaters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8uz4beXIwcw"
      },
      "source": [
        "Define Paramaters class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b90MaNBijOfU"
      },
      "source": [
        "class Paramaters:\n",
        "    def __init__(self,  \n",
        "                 # integrations\n",
        "                use_gdrive, use_anvil,\n",
        "                 # model architecture\n",
        "                use_probability_layers,  # implements TensorFlow Probability\n",
        "                use_word_path,  # note: TFP layers not recommended with word-level model \n",
        "                use_electra, # use False for BERT embeddings (fewer params, word model only)\n",
        "                # datasets\n",
        "                author, data_files, \n",
        "                datasets_dir='https://raw.githubusercontent.com/mvenouziou/text_generator/main/',\n",
        "                # model params\n",
        "                num_trailing_words=5, padded_example_length=8, batch_size=128):\n",
        "        \n",
        "        # save param choices\n",
        "        # note: additional attributes are added below\n",
        "        self._use_gdrive = use_gdrive\n",
        "        self._use_anvil = use_anvil\n",
        "        self._author = author       \n",
        "        self._num_trailing_words = num_trailing_words\n",
        "        self._padded_example_length = padded_example_length\n",
        "        self._batch_size = batch_size\n",
        "        self._use_probability_layers = use_probability_layers\n",
        "        self._use_word_path = use_word_path\n",
        "        self._use_electra = use_electra\n",
        "        self._data_files = list(data_files)\n",
        "        self._datasets_dir = datasets_dir\n",
        "        \n",
        "        # 3rd party integrations\n",
        "        # Mount Google Drive:\n",
        "        if self._use_gdrive:\n",
        "            self._gdrive_dir = '/content/gdrive/'\n",
        "            from google.colab import drive\n",
        "            drive.mount(self._gdrive_dir)\n",
        "        else:\n",
        "            self._gdrive_dir = ''\n",
        "\n",
        "        # Anvil's web app server\n",
        "        if self._use_anvil:\n",
        "            anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "\n",
        "        # Filepath Structure\n",
        "        # path name conventions due to model structure\n",
        "        if self._use_probability_layers :\n",
        "            self._author +=  '/probability/' \n",
        "        if self._use_word_path:\n",
        "            self._author += '_words_model/'\n",
        "        if self._use_electra:\n",
        "            self._author += 'electra/'\n",
        "\n",
        "        # models / checkpoints directories\n",
        "        # (Google Drive)\n",
        "        self._filepath = self._gdrive_dir + 'MyDrive/Colab_Notebooks/models/text_generation/' + self._author\n",
        "        self._checkpoint_dir = self._filepath + '/checkpoints/'\n",
        "        self._prediction_model_dir = self._filepath + '/prediction_model/'\n",
        "        self._training_model_dir = self._filepath + '/training_model/'\n",
        "        self._processed_data_dir = self._filepath + '/proc_data/'\n",
        "        self._tensorboard_dir = self._checkpoint_dir  + '/logs/'\n",
        "\n",
        "        # Create Tokenizer / Set Vocab Size\n",
        "        # character tokenizer\n",
        "        def create_character_tokenizer():\n",
        "        \n",
        "            char_tokens = string.printable\n",
        "            filters = '#$%&()*+-/<=>@[]^_`{|}~\\t'\n",
        "\n",
        "            # Initialize standard keras tokenizer\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                            num_words=None,  \n",
        "                            filters=filters,\n",
        "                            lower=False,  # conversion to lowercase letters\n",
        "                            char_level=True,\n",
        "                            oov_token=None,  # drop unknown characters\n",
        "                            )      \n",
        "            # fit tokenizer\n",
        "            tokenizer.fit_on_texts(char_tokens)\n",
        "            \n",
        "            return tokenizer\n",
        "\n",
        "        self._character_tokenizer = create_character_tokenizer()\n",
        "        self._vocab_size = len(self._character_tokenizer.word_index) + 1"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A5lFZkNEPVV"
      },
      "source": [
        "Create Paramaters (global) object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf85QK99q2YH",
        "outputId": "73ed3803-1cac-4c05-c4b6-ee0a24237157"
      },
      "source": [
        "# paramater customizationss\n",
        "author='tests'\n",
        "data_files=['robert_frost_collection.csv']\n",
        "use_gdrive=True\n",
        "use_anvil=False\n",
        "use_probability_layers=False\n",
        "use_word_path=False\n",
        "use_electra=False\n",
        "\n",
        "# create paramaters object\n",
        "PARAMATERS = Paramaters(use_gdrive=use_gdrive, use_anvil=use_anvil, \n",
        "                        author=author, data_files=data_files,\n",
        "                        use_probability_layers=use_probability_layers,\n",
        "                        use_word_path=use_word_path, use_electra=use_electra)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39FgKokRXBe1"
      },
      "source": [
        "### Define Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl6b4S8zeFdi"
      },
      "source": [
        "Character-Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqQHC1D6Uwp"
      },
      "source": [
        "def make_padded_array(text_blocks, paramaters):\n",
        "    # Tokenizes and applies padding for uniform length\n",
        "\n",
        "    # load tokenizer if one is not supplied\n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "\n",
        "    # tokenize\n",
        "    token_blocks = tokenizer.texts_to_sequences(text_blocks)\n",
        "\n",
        "    # zero padding\n",
        "    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                        sequences=token_blocks,  # dataset\n",
        "                        maxlen=paramaters._padded_example_length, \n",
        "                        dtype='int32', \n",
        "                        padding='pre',\n",
        "                        truncating='pre', \n",
        "                        value=0.0\n",
        "                        )\n",
        "    \n",
        "    return padded_blocks"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqkR0FveIBt"
      },
      "source": [
        "Word-Level (BERT or Electra pre-trained embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC-8wxwtxDD"
      },
      "source": [
        "def get_word_encoder(paramaters):\n",
        "\n",
        "    # Word Embeddings path (bert encoder)\n",
        "    if paramaters._use_electra:\n",
        "        encoder_url = 'https://tfhub.dev/google/electra_small/2'\n",
        "    else:\n",
        "        encoder_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                            + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
        "    preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                        + 'bert_en_uncased_preprocess/3'\n",
        "                \n",
        "    # preprocessing layer\n",
        "    # get BERT components\n",
        "    preprocessor = hub.load(preprocessor_url)\n",
        "    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize,\n",
        "                                    name='bert_tokenizer')\n",
        "    bert_packer = hub.KerasLayer(\n",
        "                    preprocessor.bert_pack_inputs,\n",
        "                    arguments=dict(seq_length=paramaters._num_trailing_words),\n",
        "                    name='bert_input_packer')\n",
        "    word_encoder = hub.KerasLayer(\n",
        "                        encoder_url, \n",
        "                        trainable=False, \n",
        "                        name='Word_encoder')\n",
        "    \n",
        "    return bert_tokenizer, bert_packer, word_encoder"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8EYMa9-XKJY"
      },
      "source": [
        "### Define Data Pre-processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqJnhind4a_"
      },
      "source": [
        "Load and Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzwvJaIXQ3q"
      },
      "source": [
        "# Function: loader for .csv files\n",
        "def prepare_csv(filename, paramaters, content_columns=['Name', 'Content'], \n",
        "                shuffle_rows=True):\n",
        "    \n",
        "    # load data into DataFrame\n",
        "    dataframe = pd.read_csv(paramaters._datasets_dir + filename).dropna()\n",
        "    \n",
        "    # extract titles and content\n",
        "    # note: column headings must match those below\n",
        "    if 'Name ' in dataframe.columns:  # required for the Robert Frost set\n",
        "        dataframe.rename(columns={'Name ':'Name'})\n",
        "    \n",
        "    # prepare titles\n",
        "    try: \n",
        "        dataframe['Name'] = dataframe['Name'].apply(\n",
        "                            lambda x: x.upper() + ':\\n')\n",
        "    except:\n",
        "        # no titles found\n",
        "        content_columns = ['Content']\n",
        "\n",
        "    # prepare content\n",
        "    dataframe['Content'] = dataframe['Content'].apply(\n",
        "                    lambda x: x + '\\n')\n",
        "\n",
        "    # restrict dataset\n",
        "    dataframe = dataframe[content_columns]\n",
        "\n",
        "    # shuffle entries (rows)\n",
        "    if shuffle_rows:\n",
        "        dataframe = dataframe.sample(frac=1)\n",
        "    \n",
        "    # data cleanup\n",
        "    dataframe = dataframe[content_columns]\n",
        "    \n",
        "    # merge desired text columns\n",
        "    dataframe['merge'] = dataframe[content_columns[0]]\n",
        "    for i in range(1, len(content_columns)):\n",
        "        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\n",
        "\n",
        "    # convert to list of strings\n",
        "    data_list = dataframe['merge'].tolist()\n",
        "    \n",
        "    return data_list   \n",
        "\n",
        "\n",
        "# Function: Load and standardize data files\n",
        "def load_parse(data_list, display_samples=True):  \n",
        "\n",
        "    # remove paragraph / line marks and split up words  \n",
        "    tokenizer = text.WhitespaceTokenizer()\n",
        "\n",
        "    # tokenize data (outputs bytestrings)\n",
        "    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]\n",
        "\n",
        "    # convert data back to string format\n",
        "    num_entries = len(cleaned_list_byte)\n",
        "\n",
        "    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) \n",
        "                    for i in range(num_entries)]\n",
        "\n",
        "    # Display text samples\n",
        "    if display_samples:\n",
        "        num_samples = 5\n",
        "        inx = np.random.choice(len(clean_list), num_samples, replace=False)\n",
        "        for example in np.array(clean_list)[inx]:\n",
        "            print(example)\n",
        "            print()\n",
        "\n",
        "        print('len(text_chunks):', len(clean_list))\n",
        "\n",
        "    return clean_list"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1HAd-j7J9eq"
      },
      "source": [
        "def create_input_target_blocks(full_examples, paramaters):\n",
        "    \"\"\" converts text into sliding n-grams of words and characters\n",
        "    returning input / target sets \"\"\"\n",
        "\n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "    max_len = paramaters._padded_example_length\n",
        "    num_words = paramaters._num_trailing_words\n",
        "\n",
        "\n",
        "    # helper function to create word-level inputs\n",
        "    def update_word_char_lists(text, chars_list, words_list):\n",
        "        \n",
        "        words_input = text.split(' ')  # separate words into list\n",
        "        words_input = words_input[-num_words-1: -1]  # select trailing words\n",
        "\n",
        "        # convert words list back to string (tensor)\n",
        "        words_input = ' '.join(words_input)\n",
        "\n",
        "        # add values to lists\n",
        "        chars_list.append(text)\n",
        "        words_list.append([words_input])\n",
        "        \n",
        "        return None\n",
        "\n",
        "    blocks = []\n",
        "    for example in full_examples:      \n",
        "\n",
        "        char_block = []\n",
        "        word_block = []\n",
        "        example_length = len(example)\n",
        "\n",
        "        # small blocks at start (will be zero-padded later)\n",
        "        leading_characters = 1  # min chars to seed predictions\n",
        "        for i in range(leading_characters, example_length - max_len - 1):\n",
        "            text = example[: i]\n",
        "            update_word_char_lists(text, char_block, word_block)\n",
        "\n",
        "        # full length blocks\n",
        "        for i in range(example_length - max_len - 1):\n",
        "            # create n-gram\n",
        "            text = example[i: max_len + i]\n",
        "            update_word_char_lists(text, char_block, word_block)\n",
        "\n",
        "        # small blocks at end (will be zero-padded later)\n",
        "        for i in range(example_length - max_len - 1, example_length-1):\n",
        "            text = example[i: ]\n",
        "            update_word_char_lists(text, char_block, word_block)\n",
        "    \n",
        "        # tokenize and add pre-padding\n",
        "        char_block = make_padded_array(char_block, paramaters)#tokenizer, max_len=max_len)\n",
        "\n",
        "        # separate into inputs and targets\n",
        "        inputs_char = char_block[:, :-1]\n",
        "        targets_char = char_block[:, 1:]\n",
        "\n",
        "        # update blocks\n",
        "        word_block = np.array(word_block)\n",
        "        blocks.append((inputs_char, word_block, targets_char))\n",
        "\n",
        "    return blocks"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9aQIGZu6te"
      },
      "source": [
        "# Function: data prep to create stateful RNN batches\n",
        "# note: This will be applied separately on each example text, \n",
        "# so that RNN can reset internal state / distinguish between unrelated passages\n",
        "# note: This code is taken directly from Imperial College London's \n",
        "# Coursera course cited above\n",
        "\n",
        "def preprocess_stateful(char_input, word_input, target, paramaters):\n",
        "\n",
        "    batch_size = paramaters._batch_size\n",
        "\n",
        "    # Prepare input and output arrays for training the stateful RNN\n",
        "    num_examples = char_input.shape[0]\n",
        "\n",
        "    # adjust for batch size to divide evenly into sample size\n",
        "    num_processed_examples = num_examples - (num_examples % batch_size)\n",
        "    input_cropped = char_input[:num_processed_examples]\n",
        "    target_cropped = target[:num_processed_examples]\n",
        "\n",
        "    # separate out samples so rows of data match up across epochs\n",
        "    # 'steps' measures how to space them out\n",
        "    steps = num_processed_examples // batch_size  \n",
        "\n",
        "    # define reordering\n",
        "    inx = np.empty((0,), dtype=np.int32)  # initialize empty array object\n",
        "    \n",
        "    for i in range(steps):\n",
        "        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \n",
        "                                                    steps)))\n",
        "\n",
        "    # reorder the data\n",
        "    input_char_stateful = input_cropped[inx]\n",
        "    input_word_stateful = word_input[inx]\n",
        "    target_seq_stateful = target_cropped[inx]\n",
        "\n",
        "    return input_char_stateful, input_word_stateful, target_seq_stateful"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlzYCN71orw"
      },
      "source": [
        "Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOSV4SWyyoS"
      },
      "source": [
        "def input_pipeline(paramaters, verbose=True, fresh_process=False):\n",
        "\n",
        "    # unpack param\n",
        "    saved_proc_dir = paramaters._processed_data_dir\n",
        "    filepath = paramaters._datasets_dir\n",
        "\n",
        "    # load previously processed data (pbz2 compressed file format)\n",
        "    try:    \n",
        "        assert(fresh_process is False)  # otherwise create dataset from files\n",
        "\n",
        "        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\n",
        "            data_dict = cPickle.load(file)\n",
        "\n",
        "        X_data_list = data_dict['X_data_list']\n",
        "        Y_data_list = data_dict['Y_data_list']\n",
        "\n",
        "        print('loaded saved pre-processed data')\n",
        "\n",
        "    except:       \n",
        "\n",
        "        # load data file\n",
        "        data_list = []\n",
        "        for filename in paramaters._data_files:\n",
        "            print(filename)\n",
        "            print(filepath + '/' + filename)\n",
        "\n",
        "            # check file extension and select loader (csv or txt)\n",
        "            _, file_extension = os.path.splitext(filename)     \n",
        "\n",
        "            if file_extension == '.csv':   \n",
        "                data = prepare_csv(filename, paramaters=paramaters,\n",
        "                                   content_columns=['Name', 'Content'], \n",
        "                                   shuffle_rows=True)\n",
        "            \n",
        "            else: # file_extension == '.txt':\n",
        "                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:\n",
        "                    data = file.readlines()\n",
        "\n",
        "            # add extracted list of texts to data list\n",
        "            data_list += data\n",
        "\n",
        "        if verbose:\n",
        "            print('PROGRESS: data_list created')\n",
        "        \n",
        "        # clean data\n",
        "        clean_list = load_parse(data_list, display_samples=False)\n",
        "        if verbose:\n",
        "            print('PROGRESS: clean_list created')\n",
        "        \n",
        "        # preprocess data\n",
        "        tokenizer = paramaters._character_tokenizer\n",
        "        blocks = create_input_target_blocks(full_examples=clean_list, \n",
        "                                            paramaters=paramaters)\n",
        "        \n",
        "        if verbose:\n",
        "            print('PROGRESS: blocks created')\n",
        "        \n",
        "        # create separate input / target pairs for each block\n",
        "        X_data_list = []\n",
        "        Y_data_list = []\n",
        "\n",
        "        i=0\n",
        "        for block in blocks:\n",
        "            if i % 10 == 0:\n",
        "                print(f'PROGRESS: processing block {i} of {len(blocks)}')\n",
        "\n",
        "            char_input = block[0] \n",
        "            word_input = block[1] \n",
        "            target = block[2]\n",
        "\n",
        "            input_char_stateful, input_word_stateful, target_seq_stateful = \\\n",
        "                                    preprocess_stateful(char_input=char_input, \n",
        "                                                        word_input=word_input, \n",
        "                                                        target=target, \n",
        "                                                        paramaters=paramaters)\n",
        "\n",
        "            # group for model input\n",
        "            X = [input_char_stateful, input_word_stateful]\n",
        "            Y = target_seq_stateful\n",
        "\n",
        "            X_data_list.append(X)\n",
        "            Y_data_list.append(Y)\n",
        "\n",
        "            # advance index\n",
        "            i += 1\n",
        "\n",
        "        # save file (pbz2 compressed file format)\n",
        "        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\n",
        "            cPickle.dump({'X_data_list': X_data_list, \n",
        "                          'Y_data_list': Y_data_list}, sfile)\n",
        "\n",
        "    return X_data_list, Y_data_list"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpjX19aWvU6"
      },
      "source": [
        "### Define Models and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEC8q6pYYBue"
      },
      "source": [
        "Compiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLj9Q1VOfG6"
      },
      "source": [
        "#VOCAB_SIZE = len(create_character_tokenizer().word_index) + 1\n",
        "\n",
        "def neg_log_likely_logits(y_true, y_pred, paramaters):\n",
        "    \"\"\" loss function for probabalistic model \"\"\"\n",
        "\n",
        "    # convert labels to one=hot vectors\n",
        "    y_true_hot = tf.one_hot(y_true, \n",
        "                            depth=paramaters._vocab_size,\n",
        "                            axis=-1)\n",
        "\n",
        "    # return negative log likelihood\n",
        "    return -y_pred.log_prob(y_true_hot)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loz4fJP1YA78"
      },
      "source": [
        "def compile_model(model, learning_rate, paramaters):\n",
        "    from keras.optimizers import RMSprop\n",
        "    from keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "    metrics=['sparse_categorical_accuracy']\n",
        "\n",
        "    # select loss function\n",
        "    if paramaters._use_probability_layers:\n",
        "        # use negative log likelihood for probabalistic model\n",
        "        loss_fn = lambda y_true, y_pred: neg_log_likely_logits(\n",
        "                    y_true, y_pred, paramaters=paramaters)\n",
        "        \n",
        "        optimizer=RMSprop(learning_rate=learning_rate)\n",
        "        metrics=['sparse_categorical_accuracy', \n",
        "                 'sparse_top_k_categorical_accuracy']\n",
        "                           ]\n",
        "    else:\n",
        "        loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
        "        metrics=['sparse_categorical_accuracy']\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=RMSprop(learning_rate=learning_rate),\n",
        "                  loss=loss_fn,\n",
        "                  metrics=metrics)\n",
        "    return model"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtYOFDwndkK4"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFYArtLttwEF"
      },
      "source": [
        "# Function: Model Definition\n",
        "def get_training_model(paramaters, for_prediction=False, \n",
        "                       load_fresh=True, verbose=True):\n",
        "    \n",
        "    \"\"\" Defines and compiles our stateful RNN model. \n",
        "    Note: batch size is required argument for stateful RNN. \"\"\"\n",
        "\n",
        "    # parameters\n",
        "    if for_prediction:\n",
        "        batch_size=1\n",
        "    else:\n",
        "        batch_size = paramaters._batch_size\n",
        "\n",
        "    use_word_path = paramaters._use_word_path\n",
        "    use_probability_layers = paramaters._use_probability_layers\n",
        "    num_words = paramaters._num_trailing_words\n",
        "\n",
        "    vocab_size = paramaters._vocab_size\n",
        "    if use_word_path:\n",
        "        embedding_dim = 32*6\n",
        "    else:\n",
        "        embedding_dim = 32*8\n",
        "    merge_dim = embedding_dim\n",
        "\n",
        "    # load and return saved model\n",
        "    if load_fresh is False:\n",
        "        model = tf.keras.models.load_model(paramaters._training_model_dir)\n",
        "        return model\n",
        "    \n",
        "    from keras.layers import Input, Embedding, Concatenate, Dense, GRU,\\\n",
        "                             Average, AveragePooling1D, Dropout, \\\n",
        "                             BatchNormalization, Lambda\n",
        "\n",
        "    # pre-trained encoder\n",
        "    bert_tokenizer, bert_packer, bert_encoder = \\\n",
        "            get_word_encoder(paramaters)\n",
        "    \n",
        "    # Build model\n",
        "    # define input shapes\n",
        "    input_1 = Input(shape=(None, ), \n",
        "                    batch_size=batch_size,\n",
        "                    dtype=tf.int32, \n",
        "                    name='char_input')\n",
        "    \n",
        "    input_2 = Input(shape=(), \n",
        "                    batch_size=batch_size,\n",
        "                    dtype=tf.string, \n",
        "                    name='word_input')\n",
        "\n",
        "    # travel individual paths\n",
        "    # Character Level Path\n",
        "    # ## Char: Embedding\n",
        "    x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
        "                   mask_zero=True, batch_input_shape=(batch_size, None),\n",
        "                   name='char_embedding',)(input_1)\n",
        "\n",
        "    # ## Char: GRU 1\n",
        "    x1 = GRU(units=embedding_dim, stateful=True, \n",
        "             return_sequences=True, name='char_GRU_1',)(x1)\n",
        "    #x1 = Dropout(rate=.10, name='char_Dropout_1')(x1)\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_1')(x1)\n",
        "    \n",
        "    # ## Char: GRU Final --  must use output_dim = merge_dim!\n",
        "    x1 = GRU(units=merge_dim, stateful=True, \n",
        "             return_sequences=True, name='char_GRU_final',)(x1)\n",
        "    #x1 = Dropout(rate=.10, name='char_Dropout_final')(x1)\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_final')(x1)\n",
        "\n",
        "    # Word Encoding Path\n",
        "    if use_word_path:\n",
        "        \n",
        "        x2 = bert_tokenizer(input_2)  # tokenize\n",
        "        x2 = bert_packer([x2])  # pack inputs for encoder\n",
        "        x2 = bert_encoder(x2)['sequence_output'] # encoding\n",
        "\n",
        "        # ## Word: GRU 1\n",
        "        x2 = GRU(units=32, stateful=True, \n",
        "                 return_sequences=True, name='word_GRU_1',)(x2)\n",
        "        x2 = Dropout(rate=.10, name='word_Dropout_1')(x2)\n",
        "        x2 = BatchNormalization(name='word_Batch_Norm_1')(x2)\n",
        "\n",
        "        # ## Word: GRU 2\n",
        "        x2 = GRU(units=32, stateful=True, \n",
        "                 return_sequences=True, name='word_GRU_2',)(x2)\n",
        "        x2 = Dropout(rate=.10, name='word_Dropout_2')(x2)\n",
        "        x2 = BatchNormalization(name='word_Batch_Norm_2')(x2)\n",
        "\n",
        "        # ## Word: Required conversion to valid merge output dim = merge_dim!\n",
        "        x2 = Dense(units=num_words, activation=None, \n",
        "                   name='word_Dense_pre_final')(x2)\n",
        "        x2 = AveragePooling1D(pool_size=5, padding='same', \n",
        "                              name='word_pooling_final')(x2)\n",
        "\n",
        "        # prepare  and word paths for merge\n",
        "        x1 = Dense(units=merge_dim, activation=None, \n",
        "                   name='word_Dense_final')(x1)\n",
        "\n",
        "        x2 = Dense(units=merge_dim, activation=None, \n",
        "                   name='word_Dense_final')(x2)\n",
        "\n",
        "        # Merge Paths\n",
        "        x = Average(name='merged_layers')([x1, x2])\n",
        "\n",
        "    else:  # update variable id to match next step\n",
        "        x = Lambda(lambda x: x, name='rename_variable')(x1)  \n",
        "    \n",
        "    # Final GRU layer\n",
        "    x = GRU(units=embedding_dim, stateful=True, \n",
        "            return_sequences=True, name='GRU_OUTPUT')(x)          \n",
        "\n",
        "    # Character prediction (logits)\n",
        "    # Note: Tensorflow Probability produces very poor results \n",
        "    # when paired with the word path in this architecture\n",
        "    \n",
        "    if use_probability_layers:\n",
        "        # Dense layer with probabalistic weights\n",
        "        x = tfpl.DenseReparameterization(\n",
        "                units=tfpl.OneHotCategorical.params_size(vocab_size),\n",
        "                activation=None)(x)          \n",
        "        \n",
        "        outputs = tfpl.OneHotCategorical(\n",
        "                    vocab_size,\n",
        "                    convert_to_tensor_fn=tfd.OneHotCategorical.logits,\n",
        "                    name='Decoding')(x) \n",
        "    else:\n",
        "        outputs = Dense(units=vocab_size, \n",
        "                        activation=None, \n",
        "                        name='Decoding')(x)       \n",
        "\n",
        "    # create model\n",
        "    model = keras.Model(inputs=[input_1, input_2], outputs=outputs)\n",
        "\n",
        "    if verbose:\n",
        "        print(model.summary())\n",
        "\n",
        "    model = compile_model(model, learning_rate=.01, paramaters=paramaters)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5YFw5SJut1K"
      },
      "source": [
        "Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7pwjfSAusua"
      },
      "source": [
        "def get_prediction_model(trained_model, verbose, paramaters):\n",
        "\n",
        "    \"\"\" enforces batch size = 1, only returns last character prediction\n",
        "     and loads any saved weights \"\"\"\n",
        "\n",
        "    # create model\n",
        "    prediction_model = get_training_model(for_prediction=True,\n",
        "                                          paramaters=paramaters,\n",
        "                                          load_fresh=True,\n",
        "                                          verbose=verbose)\n",
        "\n",
        "    # load weights from pre-trained model\n",
        "    if trained_model is not None:        \n",
        "        trained_weights = trained_model.get_weights()\n",
        "        prediction_model.set_weights(trained_weights)\n",
        "\n",
        "    return prediction_model"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6KD4XoYoQG"
      },
      "source": [
        "Checkpoint Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K09LDfJYnla"
      },
      "source": [
        "# checkpoint manager\n",
        "def create_checkpoint_manager(model, paramaters):\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\n",
        "\n",
        "    checkpoint_manager = tf.train.CheckpointManager(\n",
        "                            checkpoint=checkpoint, \n",
        "                            directory=paramaters._checkpoint_dir, \n",
        "                            max_to_keep=4, \n",
        "                            keep_checkpoint_every_n_hours=None,\n",
        "                            checkpoint_name='ckpt', \n",
        "                            step_counter=None, \n",
        "                            checkpoint_interval=None,\n",
        "                            init_fn=None\n",
        "                            )\n",
        "    \n",
        "    return checkpoint, checkpoint_manager"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWkTKUaXZbm"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZWbiojXXpS"
      },
      "source": [
        "# Function: Train model\n",
        "def train_model(model, X_data_list, Y_data_list,\n",
        "                num_epochs, \n",
        "                num_datasets_to_use,\n",
        "                checkpoint, \n",
        "                checkpoint_manager,\n",
        "                learning_rate,\n",
        "                paramaters):\n",
        "\n",
        "    # get params\n",
        "    batch_size = paramaters._batch_size\n",
        "\n",
        "    # compile model\n",
        "    model = compile_model(model=model, learning_rate=learning_rate, \n",
        "                          paramaters=paramaters)\n",
        "\n",
        "    # set checkpoint manager\n",
        "    if checkpoint is None or checkpoint_manager is None:\n",
        "        checkpoint, checkpoint_manager = \\\n",
        "                    create_checkpoint_manager(model=model, paramaters=paramaters)\n",
        "                    \n",
        "    # set callbacks\n",
        "    # TensorBoard callback\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "                                log_dir=paramaters._tensorboard_dir, \n",
        "                                histogram_freq=1,\n",
        "                                )\n",
        "    \n",
        "    # organize training data\n",
        "    num_blocks = len(X_data_list)\n",
        "    train_datasets_list = list(zip(X_data_list, Y_data_list)) \n",
        "\n",
        "    # if not specified, use all datasets\n",
        "    if num_datasets_to_use is None \\\n",
        "            or num_datasets_to_use == -1 \\\n",
        "            or num_datasets_to_use > len(X_data_list):\n",
        "        num_datasets_to_use = len(X_data_list)      \n",
        "    \n",
        "    # begin training loop\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(f'Epoch: {epoch}')\n",
        "\n",
        "        # shuffle dataset order\n",
        "        random.shuffle(train_datasets_list)\n",
        "        print('shuffled datasets')\n",
        "\n",
        "\n",
        "        # apply any filters\n",
        "        \"\"\"\n",
        "        filtered_datasets_list = train_datasets_list  # no filter\n",
        "        \"\"\"\n",
        "        \n",
        "        # impose minimum sample size:\n",
        "        # (avoids overtraining to small sample sets)\n",
        "        min_size = random.choice(range(14*batch_size, 40*batch_size, batch_size))\n",
        "        filtered_datasets_list = [train_datasets_list[i] \n",
        "                                  for i in range(len(X_data_list))\n",
        "                                  if train_datasets_list[i][1].shape[0] > min_size]\n",
        "\n",
        "        # compute min batches found among chosen datasets\n",
        "        # this will be used to impose balanced sample sizes amond training sets\n",
        "        min_batches = np.min([filtered_datasets_list[i][1].shape[0]\n",
        "                              for i in range(num_datasets_to_use)])\n",
        "        min_batches = min_batches // batch_size\n",
        "        \n",
        "        x_data_list = []\n",
        "        y_data_list = []\n",
        "        \n",
        "        print('Preparing epoch datasets')\n",
        "        for i in range(num_datasets_to_use):\n",
        "\n",
        "            # select dataset\n",
        "            data = filtered_datasets_list[i]\n",
        "            X = data[0]\n",
        "            Y = data[1]\n",
        "\n",
        "            \n",
        "            # crop to uniform length, from random starting points\n",
        "            # (avoids overtraining to large sample sets,\n",
        "            # or to the start of each sample,\n",
        "            # and improves training speed)\n",
        "            multiple = max(Y.shape[0] // (min_batches * batch_size), 1)\n",
        "            rand = np.random.randint(multiple) \n",
        "            start = rand * min_batches * batch_size\n",
        "            end = (1 + rand) * min_batches * batch_size\n",
        "\n",
        "            x_data_list.append([X[0][start:end, : ], X[1][start:end, : ]])\n",
        "            y_data_list.append(Y[start:end, : ])\n",
        "\n",
        "        \n",
        "        for i in range(num_datasets_to_use):\n",
        "            print(f'dataset: {i}')\n",
        "\n",
        "            dataset = tf.data.Dataset.from_tensor_slices(\n",
        "                ((x_data_list[i][0] , x_data_list[i][1]), y_data_list[i])\n",
        "                )\n",
        "            \n",
        "            dataset = dataset.batch(batch_size, drop_remainder=True)\\\n",
        "                             .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "                \n",
        "            # train model\n",
        "            history = model.fit(dataset,\n",
        "                                shuffle=False,\n",
        "                                epochs=1,\n",
        "                                verbose=1,                                \n",
        "                                #callbacks=[tensorboard_callback],\n",
        "                                )\n",
        "            \n",
        "            # reset RNN hidden states\n",
        "            model.reset_states()\n",
        "\n",
        "            # save checkpoint\n",
        "            checkpoint_manager.save()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rFetpOUqwQ"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tf4x1q8UpfD"
      },
      "source": [
        "# Store trained model separate from checkpoints\n",
        "def save_model(model, model_type, paramaters):\n",
        "\n",
        "    if model_type == 'training':\n",
        "        model_dir = paramaters._training_model_dir\n",
        "    elif model_type == 'prediction':\n",
        "        model_dir = paramaters._prediction_model_dir\n",
        "\n",
        "    # save model\n",
        "    model.save(model_dir)\n",
        "\n",
        "    return None"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMNy3XTW2Be"
      },
      "source": [
        "### Define Implementation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuUcE7H4snz"
      },
      "source": [
        "def convert_to_input(last_token, text_string, paramaters):\n",
        "        \n",
        "    # words\n",
        "    if paramaters._use_word_path:\n",
        "        num_words = paramaters._num_trailing_words\n",
        "\n",
        "        words_input = text_string.split(' ')  # separate words \n",
        "        words_input = words_input[-num_words-1:-1]  # get trailing words\n",
        "        words_input = tf.constant(' '.join(words_input))  # convert to tensor\n",
        "        \n",
        "    else:\n",
        "        words_input=tf.constant(' ')\n",
        "\n",
        "    # pad token sequence\n",
        "    inputs_char=tf.constant(last_token)\n",
        "\n",
        "    # create separate input / target pairs for each block\n",
        "    X = [inputs_char, words_input]\n",
        "\n",
        "    return X"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2atvRiGv-z"
      },
      "source": [
        "def generator(input_text, \n",
        "              prediction_model, \n",
        "              precision_reduction, \n",
        "              num_characters, \n",
        "              print_result,\n",
        "              paramaters):\n",
        "\n",
        "    # get tokenizer (if not supplied)      \n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "    \n",
        "    # initialize generated text\n",
        "    last_token =  tokenizer.texts_to_sequences([input_text])\n",
        "    output_text = input_text.upper() + '\\n'\n",
        "    generated_text = list(output_text)\n",
        "    \n",
        "    # text generation loop\n",
        "    initial_state = None\n",
        "\n",
        "    for _ in range(num_characters):\n",
        "        \n",
        "        # prepare input for model\n",
        "        inputs = convert_to_input(last_token=last_token, \n",
        "                                  text_string=output_text,\n",
        "                                  paramaters=paramaters)\n",
        "            \n",
        "        # pass forward final GRU layer state\n",
        "        GRU_layer = prediction_model.get_layer('GRU_OUTPUT')\n",
        "        GRU_layer.reset_states(initial_state)\n",
        "        \n",
        "        # run model and get logits of last character prediction\n",
        "        logits = prediction_model(inputs)[0, -1, :]\n",
        "        \n",
        "        # perturb probabilities before selecting token\n",
        "        if precision_reduction != 0:   \n",
        "            \n",
        "            # perturb logits\n",
        "            logits = logits.numpy()   \n",
        "            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n",
        "            logits = logits * (1 + precision_reduction * fuzz_factor)\n",
        "\n",
        "        # select token\n",
        "        last_token = tf.random.categorical(logits=[logits], num_samples=1)       \n",
        "        last_token = last_token.numpy().tolist()\n",
        "\n",
        "        # replace any invalid character token\n",
        "        if last_token==[[0]]:  \n",
        "            last_token==[[1]]\n",
        "\n",
        "        #  get GRU state for next character prediction\n",
        "        initial_state = GRU_layer.states[0].numpy()\n",
        "        \n",
        "        # get input for next character prediction\n",
        "        input_text = tokenizer.sequences_to_texts(last_token)\n",
        "        input_text = input_text[0]\n",
        "\n",
        "        # record generated character\n",
        "        generated_text.append(input_text)\n",
        "\n",
        "        # reset for next run\n",
        "        output_text = ''.join(generated_text)\n",
        "        \n",
        "    if print_result:\n",
        "        print(output_text)\n",
        "    \n",
        "    return output_text"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiLYUGJ4uvk"
      },
      "source": [
        "Final generation function for end user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCvZ7x8D4rz1"
      },
      "source": [
        "def generate_text(starting_text, \n",
        "                  precision_reduction,\n",
        "                  prediction_model,\n",
        "                  print_result,\n",
        "                  paramaters,\n",
        "                  num_generation_steps=150): # set length of generated text\n",
        "\n",
        "    # format user input\n",
        "    starting_text = starting_text.upper() + ': '\n",
        "\n",
        "    # get generated text\n",
        "    # note: very rarely this produces a line indexing error. \n",
        "    # This while loop reruns prediction if needed\n",
        "\n",
        "    prediction = generator(input_text=starting_text, \n",
        "                                prediction_model=prediction_model, \n",
        "                                precision_reduction=precision_reduction, \n",
        "                                num_characters=num_generation_steps, \n",
        "                                print_result=print_result,\n",
        "                                paramaters=paramaters)\n",
        "                                    \n",
        "   \n",
        "    # define formatting rules\n",
        "    split_on = ['?', '.', ',', ';', '!', ':']\n",
        "    splits = '([' + ''.join(split_on) + '])'\n",
        "    split_lines_prediction = re.split(splits, prediction)\n",
        "\n",
        "    # format output\n",
        "    output = ''\n",
        "    for line in split_lines_prediction:\n",
        "\n",
        "        # capitalize first word of each line   \n",
        "        if len(line) >= 1:\n",
        "            line_update = line[0].upper()  \n",
        "\n",
        "            # add capitalized letter to remainder of line\n",
        "            if len(line) >= 2:\n",
        "                line_update += line[1:]\n",
        "        else:\n",
        "            line_update = ''\n",
        "                \n",
        "        # update output text\n",
        "        if (len(line_update) >= 1 and line_update[-1] in split_on) \\\n",
        "          or (len(line_update) >= 2 and line_update[-2:] == '\\n'):\n",
        "                output = ''.join([output, line_update])\n",
        "        else:\n",
        "            output = '\\n'.join([output, line_update])\n",
        "\n",
        "    \n",
        "    return output + '... '"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO27cbff77C4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJkLK0N8Bm-"
      },
      "source": [
        "Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDSYqGeR78qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3490f11b-9b21-43dd-d3f8-54c79e3253ec"
      },
      "source": [
        "require_fresh_process = False\n",
        "\n",
        "try:\n",
        "    # check if prepared datasets already in memory\n",
        "    assert(fresh_process is False)\n",
        "    assert(len(X_data_list) > 0)\n",
        "    print('dataset already loaded')\n",
        "\n",
        "except:\n",
        "    print('Preparing dataset')\n",
        "    X_data_list, Y_data_list = input_pipeline(paramaters=PARAMATERS, \n",
        "                                              fresh_process=require_fresh_process)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset\n",
            "loaded saved pre-processed data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigiJJno8Hm3"
      },
      "source": [
        "Initialize Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Zd591P8Kvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aadfeb1-47e2-44f2-ee4d-757aa584808f"
      },
      "source": [
        "training_model = get_training_model(paramaters=PARAMATERS)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4b3773e050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4b3773e050> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4b409984d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7f4b409984d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "char_input (InputLayer)         [(128, None)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "char_embedding (Embedding)      (128, None, 256)     25856       char_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_1 (GRU)                (128, None, 256)     394752      char_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_1 (BatchNormali (128, None, 256)     1024        char_GRU_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_final (GRU)            (128, None, 256)     394752      char_Batch_Norm_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_final (BatchNor (128, None, 256)     1024        char_GRU_final[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "rename_variable (Lambda)        (128, None, 256)     0           char_Batch_Norm_final[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "GRU_OUTPUT (GRU)                (128, None, 256)     394752      rename_variable[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "word_input (InputLayer)         [(128,)]             0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Decoding (Dense)                (128, None, 101)     25957       GRU_OUTPUT[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 1,238,117\n",
            "Trainable params: 1,237,093\n",
            "Non-trainable params: 1,024\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAjRSZ4WaDD"
      },
      "source": [
        "Load Latest Training Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhcsgO3WZfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ea9424-de42-41f4-85f6-166996ec929b"
      },
      "source": [
        "load_checkpoint=False\n",
        "\n",
        "try:\n",
        "    # load from checkpoint\n",
        "    assert(load_checkpoint is True)\n",
        "    checkpoint, checkpoint_manager = \\\n",
        "        create_checkpoint_manager(model=training_model, paramaters=PARAMATERS)\n",
        "\n",
        "    checkpoint_manager.restore_or_initialize()\n",
        "    print('loaded checkpoint')\n",
        "\n",
        "except:\n",
        "    \n",
        "    print('No matching checkpoints')\n",
        "    checkpoint=None \n",
        "    checkpoint_manager=None"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No matching checkpoints\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KIYAQj8OZx"
      },
      "source": [
        "Train Model\n",
        "\n",
        "*Caution with the nonlinear model: overfitting can be a major problem where the model can eventually memorize and return complete segments from the source material. A precision reduction factor can be used to partially compensate. (This paramater in my final prediction function randomly perturbs learned probabilities)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacay__l8M4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b5c4612-1b18-4e2d-bf7d-340e3904fe73"
      },
      "source": [
        "train_model_now = True\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 200\n",
        "num_datasets_per_epoch = 1 # small numbers are best so that some epochs\n",
        "                           # have larger quantity of elements per batch. \n",
        "                           # Use '-1' to include all datasets\n",
        "\n",
        "# train model\n",
        "if train_model_now:\n",
        "    training_model = train_model(training_model, X_data_list, Y_data_list,\n",
        "                                num_epochs=num_epochs, \n",
        "                                learning_rate=learning_rate,\n",
        "                                num_datasets_to_use=num_datasets_per_epoch,\n",
        "                                paramaters=PARAMATERS,\n",
        "                                checkpoint=checkpoint, \n",
        "                                checkpoint_manager=checkpoint_manager)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "120/120 [==============================] - 11s 29ms/step - loss: 1.7604 - sparse_categorical_accuracy: 0.5238\n",
            "Epoch: 1\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "158/158 [==============================] - 5s 30ms/step - loss: 1.2910 - sparse_categorical_accuracy: 0.6289\n",
            "Epoch: 2\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "150/150 [==============================] - 4s 30ms/step - loss: 1.1661 - sparse_categorical_accuracy: 0.6637\n",
            "Epoch: 3\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 4s 30ms/step - loss: 1.1597 - sparse_categorical_accuracy: 0.6792\n",
            "Epoch: 4\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "80/80 [==============================] - 2s 30ms/step - loss: 1.1106 - sparse_categorical_accuracy: 0.6879\n",
            "Epoch: 5\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 3s 30ms/step - loss: 1.0389 - sparse_categorical_accuracy: 0.7057\n",
            "Epoch: 6\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "158/158 [==============================] - 5s 30ms/step - loss: 1.0709 - sparse_categorical_accuracy: 0.6991\n",
            "Epoch: 7\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 1.0266 - sparse_categorical_accuracy: 0.7277\n",
            "Epoch: 8\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 28ms/step - loss: 1.0465 - sparse_categorical_accuracy: 0.7188\n",
            "Epoch: 9\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 29ms/step - loss: 1.0691 - sparse_categorical_accuracy: 0.7029\n",
            "Epoch: 10\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "64/64 [==============================] - 2s 30ms/step - loss: 1.0089 - sparse_categorical_accuracy: 0.7149\n",
            "Epoch: 11\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "39/39 [==============================] - 1s 25ms/step - loss: 1.0531 - sparse_categorical_accuracy: 0.7167\n",
            "Epoch: 12\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 1s 28ms/step - loss: 1.2067 - sparse_categorical_accuracy: 0.6693\n",
            "Epoch: 13\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 1.1133 - sparse_categorical_accuracy: 0.6937\n",
            "Epoch: 14\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "28/28 [==============================] - 1s 30ms/step - loss: 1.0117 - sparse_categorical_accuracy: 0.7260\n",
            "Epoch: 15\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 1.0131 - sparse_categorical_accuracy: 0.7233\n",
            "Epoch: 16\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "75/75 [==============================] - 1s 15ms/step - loss: 1.0193 - sparse_categorical_accuracy: 0.7300\n",
            "Epoch: 17\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 19ms/step - loss: 1.1090 - sparse_categorical_accuracy: 0.6969\n",
            "Epoch: 18\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "21/21 [==============================] - 1s 30ms/step - loss: 1.1953 - sparse_categorical_accuracy: 0.6765\n",
            "Epoch: 19\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "158/158 [==============================] - 5s 31ms/step - loss: 0.9573 - sparse_categorical_accuracy: 0.7306\n",
            "Epoch: 20\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 1.0222 - sparse_categorical_accuracy: 0.7266\n",
            "Epoch: 21\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 3s 31ms/step - loss: 0.9548 - sparse_categorical_accuracy: 0.7329\n",
            "Epoch: 22\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 0.9914 - sparse_categorical_accuracy: 0.7315\n",
            "Epoch: 23\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "39/39 [==============================] - 1s 26ms/step - loss: 1.0763 - sparse_categorical_accuracy: 0.7099\n",
            "Epoch: 24\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 30ms/step - loss: 1.0316 - sparse_categorical_accuracy: 0.7200\n",
            "Epoch: 25\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "107/107 [==============================] - 3s 31ms/step - loss: 0.9808 - sparse_categorical_accuracy: 0.7311\n",
            "Epoch: 26\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 17ms/step - loss: 1.0755 - sparse_categorical_accuracy: 0.7080\n",
            "Epoch: 27\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.9709 - sparse_categorical_accuracy: 0.7491\n",
            "Epoch: 28\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 1.0270 - sparse_categorical_accuracy: 0.7147\n",
            "Epoch: 29\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 27ms/step - loss: 0.9912 - sparse_categorical_accuracy: 0.7331\n",
            "Epoch: 30\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 17ms/step - loss: 0.7680 - sparse_categorical_accuracy: 0.7995\n",
            "Epoch: 31\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 32ms/step - loss: 1.0828 - sparse_categorical_accuracy: 0.7032\n",
            "Epoch: 32\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 31ms/step - loss: 0.9451 - sparse_categorical_accuracy: 0.7414\n",
            "Epoch: 33\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 4s 31ms/step - loss: 0.9167 - sparse_categorical_accuracy: 0.7444\n",
            "Epoch: 34\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "107/107 [==============================] - 3s 31ms/step - loss: 0.9263 - sparse_categorical_accuracy: 0.7458\n",
            "Epoch: 35\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 3s 30ms/step - loss: 0.7455 - sparse_categorical_accuracy: 0.7897\n",
            "Epoch: 36\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 0.9498 - sparse_categorical_accuracy: 0.7426\n",
            "Epoch: 37\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 29ms/step - loss: 1.0585 - sparse_categorical_accuracy: 0.7097\n",
            "Epoch: 38\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 3s 31ms/step - loss: 0.6795 - sparse_categorical_accuracy: 0.8069\n",
            "Epoch: 39\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 1.0707 - sparse_categorical_accuracy: 0.7229\n",
            "Epoch: 40\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "69/69 [==============================] - 2s 31ms/step - loss: 1.1168 - sparse_categorical_accuracy: 0.6936\n",
            "Epoch: 41\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "27/27 [==============================] - 1s 31ms/step - loss: 1.1081 - sparse_categorical_accuracy: 0.6911\n",
            "Epoch: 42\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 0.8479 - sparse_categorical_accuracy: 0.7674\n",
            "Epoch: 43\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 31ms/step - loss: 0.9633 - sparse_categorical_accuracy: 0.7356\n",
            "Epoch: 44\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.9846 - sparse_categorical_accuracy: 0.7375\n",
            "Epoch: 45\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "21/21 [==============================] - 1s 35ms/step - loss: 1.0592 - sparse_categorical_accuracy: 0.7142\n",
            "Epoch: 46\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 29ms/step - loss: 0.6461 - sparse_categorical_accuracy: 0.8218\n",
            "Epoch: 47\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.9672 - sparse_categorical_accuracy: 0.7418\n",
            "Epoch: 48\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 29ms/step - loss: 1.1058 - sparse_categorical_accuracy: 0.6988\n",
            "Epoch: 49\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "80/80 [==============================] - 3s 33ms/step - loss: 0.9918 - sparse_categorical_accuracy: 0.7280\n",
            "Epoch: 50\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "69/69 [==============================] - 2s 31ms/step - loss: 1.0528 - sparse_categorical_accuracy: 0.7060\n",
            "Epoch: 51\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 32ms/step - loss: 0.9033 - sparse_categorical_accuracy: 0.7463\n",
            "Epoch: 52\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 0.9256 - sparse_categorical_accuracy: 0.7570\n",
            "Epoch: 53\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.8488 - sparse_categorical_accuracy: 0.7695\n",
            "Epoch: 54\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 17ms/step - loss: 1.0784 - sparse_categorical_accuracy: 0.7077\n",
            "Epoch: 55\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 31ms/step - loss: 0.9307 - sparse_categorical_accuracy: 0.7454\n",
            "Epoch: 56\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 1.1089 - sparse_categorical_accuracy: 0.7107\n",
            "Epoch: 57\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 4s 31ms/step - loss: 0.8733 - sparse_categorical_accuracy: 0.7566\n",
            "Epoch: 58\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 29ms/step - loss: 0.9893 - sparse_categorical_accuracy: 0.7265\n",
            "Epoch: 59\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "146/146 [==============================] - 5s 32ms/step - loss: 0.8991 - sparse_categorical_accuracy: 0.7568\n",
            "Epoch: 60\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 1s 30ms/step - loss: 1.2300 - sparse_categorical_accuracy: 0.6621\n",
            "Epoch: 61\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 30ms/step - loss: 1.0397 - sparse_categorical_accuracy: 0.7204\n",
            "Epoch: 62\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "120/120 [==============================] - 4s 31ms/step - loss: 0.8806 - sparse_categorical_accuracy: 0.7675\n",
            "Epoch: 63\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 4s 32ms/step - loss: 0.8522 - sparse_categorical_accuracy: 0.7614\n",
            "Epoch: 64\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 4s 31ms/step - loss: 0.8663 - sparse_categorical_accuracy: 0.7714\n",
            "Epoch: 65\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "48/48 [==============================] - 1s 28ms/step - loss: 1.0506 - sparse_categorical_accuracy: 0.7171\n",
            "Epoch: 66\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 19ms/step - loss: 1.1199 - sparse_categorical_accuracy: 0.6986\n",
            "Epoch: 67\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 17ms/step - loss: 0.5973 - sparse_categorical_accuracy: 0.8358\n",
            "Epoch: 68\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "107/107 [==============================] - 3s 31ms/step - loss: 0.9905 - sparse_categorical_accuracy: 0.7297\n",
            "Epoch: 69\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 32ms/step - loss: 1.0163 - sparse_categorical_accuracy: 0.7239\n",
            "Epoch: 70\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 33ms/step - loss: 0.5936 - sparse_categorical_accuracy: 0.8340\n",
            "Epoch: 71\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 0.9771 - sparse_categorical_accuracy: 0.7412\n",
            "Epoch: 72\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 18ms/step - loss: 0.9370 - sparse_categorical_accuracy: 0.7421\n",
            "Epoch: 73\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 18ms/step - loss: 0.4717 - sparse_categorical_accuracy: 0.8685\n",
            "Epoch: 74\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 1s 35ms/step - loss: 1.0299 - sparse_categorical_accuracy: 0.7269\n",
            "Epoch: 75\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "80/80 [==============================] - 3s 32ms/step - loss: 1.0061 - sparse_categorical_accuracy: 0.7240\n",
            "Epoch: 76\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 30ms/step - loss: 0.9943 - sparse_categorical_accuracy: 0.7376\n",
            "Epoch: 77\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 5s 33ms/step - loss: 0.8490 - sparse_categorical_accuracy: 0.7761\n",
            "Epoch: 78\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "80/80 [==============================] - 3s 32ms/step - loss: 0.8296 - sparse_categorical_accuracy: 0.7671\n",
            "Epoch: 79\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "80/80 [==============================] - 2s 31ms/step - loss: 0.4659 - sparse_categorical_accuracy: 0.8664\n",
            "Epoch: 80\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 0.9642 - sparse_categorical_accuracy: 0.7442\n",
            "Epoch: 81\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "146/146 [==============================] - 5s 32ms/step - loss: 0.9114 - sparse_categorical_accuracy: 0.7549\n",
            "Epoch: 82\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 4s 31ms/step - loss: 0.9220 - sparse_categorical_accuracy: 0.7463\n",
            "Epoch: 83\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "21/21 [==============================] - 1s 33ms/step - loss: 1.1097 - sparse_categorical_accuracy: 0.7138\n",
            "Epoch: 84\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "24/24 [==============================] - 0s 19ms/step - loss: 1.2124 - sparse_categorical_accuracy: 0.6787\n",
            "Epoch: 85\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 0.9876 - sparse_categorical_accuracy: 0.7341\n",
            "Epoch: 86\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 29ms/step - loss: 1.0810 - sparse_categorical_accuracy: 0.7057\n",
            "Epoch: 87\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 28ms/step - loss: 0.9754 - sparse_categorical_accuracy: 0.7389\n",
            "Epoch: 88\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 19ms/step - loss: 1.0337 - sparse_categorical_accuracy: 0.7179\n",
            "Epoch: 89\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "268/268 [==============================] - 8s 32ms/step - loss: 1.1524 - sparse_categorical_accuracy: 0.6872\n",
            "Epoch: 90\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 19ms/step - loss: 0.9740 - sparse_categorical_accuracy: 0.7535\n",
            "Epoch: 91\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 1.0385 - sparse_categorical_accuracy: 0.7240\n",
            "Epoch: 92\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 0.9261 - sparse_categorical_accuracy: 0.7491\n",
            "Epoch: 93\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "158/158 [==============================] - 5s 31ms/step - loss: 0.9881 - sparse_categorical_accuracy: 0.7276\n",
            "Epoch: 94\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 33ms/step - loss: 1.0717 - sparse_categorical_accuracy: 0.7055\n",
            "Epoch: 95\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "146/146 [==============================] - 5s 32ms/step - loss: 0.8869 - sparse_categorical_accuracy: 0.7600\n",
            "Epoch: 96\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "107/107 [==============================] - 3s 32ms/step - loss: 0.9982 - sparse_categorical_accuracy: 0.7275\n",
            "Epoch: 97\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "33/33 [==============================] - 1s 29ms/step - loss: 0.9629 - sparse_categorical_accuracy: 0.7289\n",
            "Epoch: 98\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "23/23 [==============================] - 1s 31ms/step - loss: 1.1613 - sparse_categorical_accuracy: 0.7049\n",
            "Epoch: 99\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "158/158 [==============================] - 5s 32ms/step - loss: 0.8864 - sparse_categorical_accuracy: 0.7522\n",
            "Epoch: 100\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.9361 - sparse_categorical_accuracy: 0.7477\n",
            "Epoch: 101\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 18ms/step - loss: 1.1480 - sparse_categorical_accuracy: 0.6925\n",
            "Epoch: 102\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 31ms/step - loss: 0.6995 - sparse_categorical_accuracy: 0.8073\n",
            "Epoch: 103\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.9477 - sparse_categorical_accuracy: 0.7607\n",
            "Epoch: 104\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 1.0232 - sparse_categorical_accuracy: 0.7191\n",
            "Epoch: 105\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 1.0269 - sparse_categorical_accuracy: 0.7260\n",
            "Epoch: 106\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 0.5930 - sparse_categorical_accuracy: 0.8372\n",
            "Epoch: 107\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 4s 32ms/step - loss: 0.8889 - sparse_categorical_accuracy: 0.7662\n",
            "Epoch: 108\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 4s 31ms/step - loss: 0.6276 - sparse_categorical_accuracy: 0.8308\n",
            "Epoch: 109\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 0.9960 - sparse_categorical_accuracy: 0.7352\n",
            "Epoch: 110\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.9976 - sparse_categorical_accuracy: 0.7380\n",
            "Epoch: 111\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 31ms/step - loss: 0.9039 - sparse_categorical_accuracy: 0.7565\n",
            "Epoch: 112\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 31ms/step - loss: 1.0913 - sparse_categorical_accuracy: 0.7015\n",
            "Epoch: 113\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "64/64 [==============================] - 2s 32ms/step - loss: 0.9669 - sparse_categorical_accuracy: 0.7322\n",
            "Epoch: 114\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "268/268 [==============================] - 8s 31ms/step - loss: 1.1808 - sparse_categorical_accuracy: 0.6812\n",
            "Epoch: 115\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "22/22 [==============================] - 1s 28ms/step - loss: 1.2086 - sparse_categorical_accuracy: 0.6835\n",
            "Epoch: 116\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 1.0318 - sparse_categorical_accuracy: 0.7278\n",
            "Epoch: 117\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "27/27 [==============================] - 1s 29ms/step - loss: 1.1406 - sparse_categorical_accuracy: 0.6923\n",
            "Epoch: 118\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.9162 - sparse_categorical_accuracy: 0.7532\n",
            "Epoch: 119\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "146/146 [==============================] - 5s 31ms/step - loss: 0.8985 - sparse_categorical_accuracy: 0.7579\n",
            "Epoch: 120\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 33ms/step - loss: 1.0620 - sparse_categorical_accuracy: 0.7123\n",
            "Epoch: 121\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "113/113 [==============================] - 4s 32ms/step - loss: 0.9333 - sparse_categorical_accuracy: 0.7430\n",
            "Epoch: 122\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 30ms/step - loss: 0.9887 - sparse_categorical_accuracy: 0.7377\n",
            "Epoch: 123\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 32ms/step - loss: 0.8849 - sparse_categorical_accuracy: 0.7564\n",
            "Epoch: 124\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 0.9992 - sparse_categorical_accuracy: 0.7321\n",
            "Epoch: 125\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 31ms/step - loss: 0.6041 - sparse_categorical_accuracy: 0.8303\n",
            "Epoch: 126\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 0.9219 - sparse_categorical_accuracy: 0.7492\n",
            "Epoch: 127\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 0.8729 - sparse_categorical_accuracy: 0.7618\n",
            "Epoch: 128\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "28/28 [==============================] - 1s 29ms/step - loss: 1.0695 - sparse_categorical_accuracy: 0.7142\n",
            "Epoch: 129\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 30ms/step - loss: 1.1108 - sparse_categorical_accuracy: 0.7076\n",
            "Epoch: 130\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.9806 - sparse_categorical_accuracy: 0.7350\n",
            "Epoch: 131\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 1s 30ms/step - loss: 1.3377 - sparse_categorical_accuracy: 0.6431\n",
            "Epoch: 132\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 0s 27ms/step - loss: 1.2044 - sparse_categorical_accuracy: 0.6853\n",
            "Epoch: 133\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 33ms/step - loss: 1.1186 - sparse_categorical_accuracy: 0.6918\n",
            "Epoch: 134\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "18/18 [==============================] - 1s 30ms/step - loss: 0.9108 - sparse_categorical_accuracy: 0.7406\n",
            "Epoch: 135\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 2s 31ms/step - loss: 0.6703 - sparse_categorical_accuracy: 0.8120\n",
            "Epoch: 136\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "24/24 [==============================] - 1s 31ms/step - loss: 1.0889 - sparse_categorical_accuracy: 0.7097\n",
            "Epoch: 137\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 31ms/step - loss: 0.8698 - sparse_categorical_accuracy: 0.7619\n",
            "Epoch: 138\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "48/48 [==============================] - 1s 29ms/step - loss: 0.9133 - sparse_categorical_accuracy: 0.7528\n",
            "Epoch: 139\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "27/27 [==============================] - 1s 32ms/step - loss: 1.1279 - sparse_categorical_accuracy: 0.6903\n",
            "Epoch: 140\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "38/38 [==============================] - 1s 29ms/step - loss: 1.0124 - sparse_categorical_accuracy: 0.7278\n",
            "Epoch: 141\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "28/28 [==============================] - 1s 32ms/step - loss: 0.9541 - sparse_categorical_accuracy: 0.7357\n",
            "Epoch: 142\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "27/27 [==============================] - 1s 31ms/step - loss: 0.7876 - sparse_categorical_accuracy: 0.7735\n",
            "Epoch: 143\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "39/39 [==============================] - 1s 28ms/step - loss: 1.0616 - sparse_categorical_accuracy: 0.7166\n",
            "Epoch: 144\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 1.0387 - sparse_categorical_accuracy: 0.7230\n",
            "Epoch: 145\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "27/27 [==============================] - 1s 30ms/step - loss: 0.6788 - sparse_categorical_accuracy: 0.8047\n",
            "Epoch: 146\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "75/75 [==============================] - 1s 16ms/step - loss: 1.0014 - sparse_categorical_accuracy: 0.7417\n",
            "Epoch: 147\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 30ms/step - loss: 0.8635 - sparse_categorical_accuracy: 0.7586\n",
            "Epoch: 148\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.9304 - sparse_categorical_accuracy: 0.7480\n",
            "Epoch: 149\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 19ms/step - loss: 1.0172 - sparse_categorical_accuracy: 0.7439\n",
            "Epoch: 150\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "77/77 [==============================] - 2s 32ms/step - loss: 1.0365 - sparse_categorical_accuracy: 0.7231\n",
            "Epoch: 151\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.7876 - sparse_categorical_accuracy: 0.7828\n",
            "Epoch: 152\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "120/120 [==============================] - 4s 32ms/step - loss: 0.9223 - sparse_categorical_accuracy: 0.7598\n",
            "Epoch: 153\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "150/150 [==============================] - 5s 32ms/step - loss: 0.9159 - sparse_categorical_accuracy: 0.7488\n",
            "Epoch: 154\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 19ms/step - loss: 1.1075 - sparse_categorical_accuracy: 0.7137\n",
            "Epoch: 155\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "48/48 [==============================] - 1s 30ms/step - loss: 1.0570 - sparse_categorical_accuracy: 0.7148\n",
            "Epoch: 156\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "66/66 [==============================] - 1s 18ms/step - loss: 0.9664 - sparse_categorical_accuracy: 0.7526\n",
            "Epoch: 157\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "49/49 [==============================] - 1s 18ms/step - loss: 1.2322 - sparse_categorical_accuracy: 0.6672\n",
            "Epoch: 158\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "64/64 [==============================] - 2s 31ms/step - loss: 1.0280 - sparse_categorical_accuracy: 0.7151\n",
            "Epoch: 159\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 33ms/step - loss: 1.0241 - sparse_categorical_accuracy: 0.7182\n",
            "Epoch: 160\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 5s 32ms/step - loss: 0.9123 - sparse_categorical_accuracy: 0.7590\n",
            "Epoch: 161\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "41/41 [==============================] - 1s 33ms/step - loss: 1.2330 - sparse_categorical_accuracy: 0.6700\n",
            "Epoch: 162\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.9156 - sparse_categorical_accuracy: 0.7507\n",
            "Epoch: 163\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "69/69 [==============================] - 2s 30ms/step - loss: 1.2104 - sparse_categorical_accuracy: 0.6699\n",
            "Epoch: 164\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "72/72 [==============================] - 2s 33ms/step - loss: 1.1354 - sparse_categorical_accuracy: 0.6867\n",
            "Epoch: 165\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "78/78 [==============================] - 3s 33ms/step - loss: 0.9693 - sparse_categorical_accuracy: 0.7305\n",
            "Epoch: 166\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "268/268 [==============================] - 9s 32ms/step - loss: 1.1973 - sparse_categorical_accuracy: 0.6727\n",
            "Epoch: 167\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "142/142 [==============================] - 4s 31ms/step - loss: 0.8736 - sparse_categorical_accuracy: 0.7664\n",
            "Epoch: 168\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "150/150 [==============================] - 5s 31ms/step - loss: 0.9159 - sparse_categorical_accuracy: 0.7464\n",
            "Epoch: 169\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "123/123 [==============================] - 4s 32ms/step - loss: 0.9151 - sparse_categorical_accuracy: 0.7520\n",
            "Epoch: 170\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 1s 18ms/step - loss: 1.1465 - sparse_categorical_accuracy: 0.7006\n",
            "Epoch: 171\n",
            "shuffled datasets\n",
            "Preparing epoch datasets\n",
            "dataset: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shv0hEXg8YQ9"
      },
      "source": [
        "Create Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy9kNuIz8XoD"
      },
      "source": [
        "prediction_model = get_prediction_model(trained_model=training_model, \n",
        "                                        verbose=True, \n",
        "                                        paramaters=PARAMATERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikV04Jc_85RI"
      },
      "source": [
        "Test Output: Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpDzr0x81-3"
      },
      "source": [
        "starting_text = 'The road less'#AI is becoming accessible'\n",
        "precision_reduction = 0 \n",
        "\n",
        "gen = generate_text(starting_text=starting_text, \n",
        "                    prediction_model=prediction_model,\n",
        "                    precision_reduction=precision_reduction,\n",
        "                    print_result=True,\n",
        "                    paramaters=PARAMATERS,\n",
        "                    num_generation_steps=150)\n",
        "\n",
        "print(gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFowEIJgUHN8"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQK0uZ4jUGbf"
      },
      "source": [
        "save_model_now = True\n",
        "\n",
        "if save_model_now:\n",
        "    # training model\n",
        "    save_model(training_model, model_type='training', paramaters=PARAMATERS)\n",
        "\n",
        "    # prediction model\n",
        "    save_model(prediction_model, model_type='prediction', paramaters=PARAMATERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-75YmvdqmG"
      },
      "source": [
        "Anvil Web App Server Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS26o45XdoiI"
      },
      "source": [
        "if USE_ANVIL:\n",
        "\n",
        "    # get tokenizer\n",
        "    tokenizer = create_character_tokenizer()\n",
        "\n",
        "    @anvil.server.callable\n",
        "    def anvil_callable(starting_text, precision_reduction=0,\n",
        "                        paramaters=PARAMATERS,\n",
        "                        prediction_tokenizer=PARAMATERS._character_tokenizer, ####### remove this\n",
        "                        prediction_model=prediction_model,\n",
        "                        print_result=True,\n",
        "                        author='assorted',\n",
        "                        num_generation_steps=150):\n",
        "\n",
        "        return generate_text(starting_text=starting_text, \n",
        "                                precision_reduction=precision_reduction,\n",
        "                                prediction_model=prediction_model,\n",
        "                                print_result=print_result,\n",
        "                                paramaters=paramaters,\n",
        "                                num_generation_steps=num_generation_steps)\n",
        "\n",
        "    # start persistent connection to server\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}