{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_Text Generator_char_level.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "tensor-flow-2-2",
      "graded_item_id": "4eYSM",
      "launcher_item_id": "HEV6h"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/text_generator/blob/main/Mo_Text_Generator_char_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFRRArflRfR1"
      },
      "source": [
        "## Text Generation RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3wpZrx7RfR-"
      },
      "source": [
        "This program constructs an unsupervised character-level sequence model that can generate text according to a distribution learned from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j_QtDXtRfR4"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "# ML design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text\n",
        "\n",
        "# data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import random\n",
        "\n",
        "# other\n",
        "import urllib.request\n",
        "import os\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkHeu2U8keZ6"
      },
      "source": [
        "##### GLOBAL VARIABLES\r\n",
        "File directories and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fRagWNBR0-E",
        "outputId": "34132b14-db31-439c-e58c-f6a05063d39e"
      },
      "source": [
        "# GLOBAL VARIABLES\n",
        "\n",
        "# hyperparameters\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "AUTHOR = 'assorted'\n",
        "\n",
        "# file directory structure in Google Drive\n",
        "GDRIVE_DIR = '/content/gdrive/'\n",
        "FILEPATH = GDRIVE_DIR + 'MyDrive/Colab_Notebooks/models/' + AUTHOR\n",
        "CHECKPOINT_DIR = FILEPATH + '/checkpoints/'\n",
        "CACHE_DIR = FILEPATH + '/cache/'\n",
        "MODEL_DIR = FILEPATH + '/prediction_model/'\n",
        "DATASETS_DIR = 'https://raw.githubusercontent.com/mvenouziou/text_generator/main/'\n",
        "\n",
        "# dataset files (.txt and .csv allowed)\n",
        "DATASET_PRIMARY = 'call_of_the_wild_jack_london.txt'\n",
        "DATASETS = [DATASET_PRIMARY] + \\\n",
        "            ['sherlock_holmes.txt',\n",
        "            'mark_twain_speeches.txt',\n",
        "             'robert_frost_assorted.txt',\n",
        "             'dorian_gray_oscar_wilde.txt',\n",
        "            ]\n",
        "\n",
        "# shuffle dataset preference\n",
        "# must be False to use stateful RNN structure\n",
        "SHUFFLE = True   \n",
        "# stateful RNN works best with a single continuous work\n",
        "if SHUFFLE is False and len(DATASETS) > 1:\n",
        "    SHUFFLE = True\n",
        "                \n",
        "\n",
        "# mount google drive:\n",
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_DIR)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppOL7TDHRfSA"
      },
      "source": [
        "#### Load and inspect the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXwzWcB-RfSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd34cfaa-163b-4b41-8423-4c21ba7592f6"
      },
      "source": [
        "# Function: loader for .csv files\n",
        "def prepare_csv(filename, datasets_dir=DATASETS_DIR):\n",
        "    # load data\n",
        "    dataframe = pd.read_csv(datasets_dir + filename)\n",
        "    \n",
        "    # cleanup specific to the robert frost set\n",
        "    dataframe = dataframe.rename(columns={'Name ':'Name'})[1:]\n",
        "    dataframe['Name'] = dataframe['Name'].apply(lambda x: x + ': ')\n",
        "    \n",
        "    # merge desired text columns and export as a single string\n",
        "    dataframe['merged'] = dataframe['Name'] + dataframe['Content']\n",
        "    text_list = dataframe['merged'].to_list()\n",
        "    text_list = (' ').join(text_list)\n",
        "\n",
        "    return text_list\n",
        "\n",
        "\n",
        "# Function: Load and standardize data files\n",
        "def load_parse(num_words_per_chunk=30, display_samples=True, \n",
        "               shuffle=SHUFFLE, datasets_dir=DATASETS_DIR, datasets=DATASETS):\n",
        "    # NOTE: set shuffle=False if using a stateful model\n",
        "    \n",
        "    # initialize container to store text data\n",
        "    text_data=''\n",
        "\n",
        "    first_file = True\n",
        "    for file in datasets:\n",
        "        \n",
        "        # for loading .csv files\n",
        "        _, file_extension = os.path.splitext(file)     \n",
        "        if file_extension == '.csv':\n",
        "            new_text = prepare_csv(file)    \n",
        "\n",
        "        # for loading .txt files\n",
        "        else:\n",
        "            if datasets_dir[:4] == 'http':\n",
        "                with urllib.request.urlopen(datasets_dir + '/' + file) as file:\n",
        "                    new_text = file.read().decode('utf-8')\n",
        "\n",
        "            else:\n",
        "                with open(datasets_dir + '/' + file, 'r', encoding='utf-8') as file:\n",
        "                    new_text = file.read()\n",
        "        \n",
        "        # merge data sources\n",
        "        # (adjust string length to be no longer than \n",
        "        # a specified multiple of first dataset length)\n",
        "        multiple = 4\n",
        "        if first_file:\n",
        "            max_length = multiple * len(new_text)\n",
        "        \n",
        "        text_data = (' ').join([text_data, new_text[: max_length]])\n",
        "\n",
        "        first_file = False\n",
        "\n",
        "    # Create a list of chunks of text\n",
        "    # remove paragraph / line marks and split up words\n",
        "    tokenizer = text.WhitespaceTokenizer()\n",
        "\n",
        "    # tokenize data (outputs bytestrings)\n",
        "    words_byte = tokenizer.tokenize(text_data)\n",
        "    words_byte = words_byte.numpy().tolist()   \n",
        "    \n",
        "    # convert data to string format\n",
        "    words = [byte.decode() for byte in words_byte]  \n",
        "    \n",
        "    # combine into lists of consecutive words\n",
        "    text_chunks = [(' ').join(words[i : i + num_words_per_chunk]) \n",
        "                    for i in range(0, len(words), num_words_per_chunk)]\n",
        "    \n",
        "    # shuffle data\n",
        "    if shuffle:\n",
        "        random.shuffle(text_chunks)\n",
        "\n",
        "    # Display some text samples\n",
        "    if display_samples:\n",
        "        num_samples = 5\n",
        "        inx = np.random.choice(len(text_chunks), num_samples, replace=False)\n",
        "        for chunk in np.array(text_chunks)[inx]:\n",
        "            print(chunk)\n",
        "            print()\n",
        "\n",
        "        print('len(words):', len(words))\n",
        "        print('len(text_chunks):', len(text_chunks))\n",
        "\n",
        "    return text_chunks\n",
        "\n",
        "\n",
        "temp = load_parse(num_words_per_chunk=30, shuffle=True, display_samples=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the Quakers William Robinson, Marmaduke Stevenson, et al. Your tribe chased them out of the country for their religion's sake; promised them death if they came back; for your ancestors\n",
            "\n",
            "was snapping like a demon. Once, his teeth closed on the fore leg of a husky, and he crunched down through the bone. Pike, the malingerer, leaped upon the crippled\n",
            "\n",
            "United States.” I thank you all out of my heart for this fraternal welcome, and it seems almost too fine, almost too magnificent, for a humble Missourian such as I\n",
            "\n",
            "No, I don't mean that. It was the first time I ever withdrew a watermelon. It was the first time I ever extracted a watermelon. That is exactly the word\n",
            "\n",
            "in the wheel-pit?\" \"They say sometime was wasted on the belt—- Old streak of leather—doesn't love me much Because I made him spit fire at my knuckles, The way Ben\n",
            "\n",
            "len(words): 327797\n",
            "len(text_chunks): 10927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWObVg6DRfSi"
      },
      "source": [
        "#### Encode data for model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8s2aR4oRfSb"
      },
      "source": [
        "# Function: Create and fit tokenizer object\n",
        "def create_character_tokenizer(list_of_strings):\n",
        "    \"\"\"\n",
        "    This function takes a list of strings as its argument. It should create \n",
        "    and return a Tokenizer according to the above specifications. \n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize standard keras tokenizer\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                    num_words=None,  # number of tokens is not limited\n",
        "                    filters=None,  # no characters filtered\n",
        "                    lower=False,  # original capitalization retained\n",
        "                    char_level=True,  # tokens created at character level\n",
        "                    )\n",
        "    \n",
        "    # fit tokenizer\n",
        "    tokenizer.fit_on_texts(list_of_strings)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# Function: apply tokenizer\n",
        "def strings_to_sequences(tokenizer, list_of_strings): \n",
        "    return tokenizer.texts_to_sequences(list_of_strings)  "
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-4W2iICRfS3"
      },
      "source": [
        "#### Create input and target Datasets for stateful RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLf-9KxWRfSw"
      },
      "source": [
        "# Function: Apply padding for uniform length\n",
        "def make_padded_dataset(sequence_chunks, max_len=500):\n",
        "    \"\"\"\n",
        "    This function takes a list of lists of tokenized sequences, and transforms\n",
        "    them into a 2D numpy array, padding the sequences as necessary according to\n",
        "    the above specification. The function should then return the numpy array.\n",
        "    \"\"\"\n",
        "    \n",
        "    return tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                        sequences=sequence_chunks,  # dataset\n",
        "                        maxlen=max_len, \n",
        "                        dtype='int32', \n",
        "                        padding='pre',\n",
        "                        truncating='pre', \n",
        "                        value=0.0\n",
        "                  )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzLY5mT3RfS5"
      },
      "source": [
        "def create_inputs_and_targets(array_of_sequences):\n",
        "    \"\"\"\n",
        "    This function takes a 2D numpy array of token sequences, and returns a tuple of two\n",
        "    elements: the first element is the input array and the second element is the output\n",
        "    array, which are defined according to the above specification.\n",
        "    \"\"\"   \n",
        "    input_arr = array_of_sequences[:, :-1]\n",
        "    target_arr = array_of_sequences[:, 1:]\n",
        "\n",
        "    return input_arr, target_arr\n",
        "    "
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcoP9vOGDqlV"
      },
      "source": [
        "Note: Our text data are not independent random samples. Content from earlier in the text can inform future predictions. In this case we can use a 'stateful' model to capture some of this dependence relation.  (Data should be a single continuous (unshuffled) text for this to work. Do not use it in other instances.) \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4gsVamxRfTK"
      },
      "source": [
        "# Function: data prep to create stateful RNN batches\n",
        "def preprocess_stateful(input, target, batch_size=BATCH_SIZE):\n",
        "\n",
        "    # Prepare input and output arrays for training the stateful RNN\n",
        "    num_examples = input.shape[0]\n",
        "\n",
        "    # we'll reduce the sample size so all batches have the same dimension\n",
        "    num_processed_examples = num_examples - (num_examples % batch_size)\n",
        "\n",
        "    # restrict data to new sample size\n",
        "    input_cropped = input[:num_processed_examples]\n",
        "    target_cropped = target[:num_processed_examples]\n",
        "\n",
        "    steps = num_processed_examples // 32  # num steps needed per epoch \n",
        "                                            # (to process all the batches)\n",
        "\n",
        "    # create array defining the order for samples to be processed\n",
        "    inx = np.empty((0,), dtype=np.int32)  # initialize array object\n",
        "    for i in range(steps):\n",
        "        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \n",
        "                                                 steps)))\n",
        "\n",
        "    # reorder the data\n",
        "    input_seq_stateful = input_cropped[inx]\n",
        "    target_seq_stateful = target_cropped[inx]\n",
        "\n",
        "    return input_seq_stateful, target_seq_stateful"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiLWfyemRfTU"
      },
      "source": [
        "# Function: Split into training and validation Dataset objects\n",
        "def make_Dataset(input, target, validation_split=.2,\n",
        "                 batch_size=BATCH_SIZE):\n",
        "\n",
        "    sample_size = input.shape[0]\n",
        "    validation_size = int(validation_split * sample_size)\n",
        "\n",
        "    # split into train / validation sets\n",
        "    input_valid = input[:validation_size]\n",
        "    target_valid = target[:validation_size]\n",
        "    \n",
        "    input_train = input[validation_size:]\n",
        "    target_train = target[validation_size:]\n",
        "\n",
        "    # convert to tensorflow Dataset batches\n",
        "    def convert_to_Dataset(input_array, target_array):\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(\n",
        "                            (input_array, target_array))\n",
        "        \n",
        "        dataset = (dataset.batch(batch_size, drop_remainder=True)\\\n",
        "                          .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "        return dataset\n",
        "\n",
        "    train_dataset = convert_to_Dataset(input_train, target_train)\n",
        "    valid_dataset = convert_to_Dataset(input_valid, target_valid)\n",
        "\n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKfaWcCCRfTf"
      },
      "source": [
        "#### Define RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oauekqOSRfTi"
      },
      "source": [
        "# Function: Model Definition\n",
        "def get_model(vocab_size, batch_size=BATCH_SIZE, embedding_dim=EMBEDDING_DIM):\n",
        "    \"\"\" Defines and compiles a Sequential RNN \"\"\"\n",
        "    \n",
        "    from keras.layers import Embedding, GRU, Dense, Dropout\n",
        "\n",
        "    model = keras.Sequential([\n",
        "                Embedding(input_dim=vocab_size, \n",
        "                          output_dim=embedding_dim,\n",
        "                          mask_zero=True, \n",
        "                          batch_input_shape=(batch_size, None)\n",
        "                          ),\n",
        "                GRU(units=1024, \n",
        "                    stateful=True, \n",
        "                    return_sequences=True,\n",
        "                    ),\n",
        "                GRU(units=1024, \n",
        "                    stateful=True, \n",
        "                    return_sequences=True,\n",
        "                    name='GRU_1',\n",
        "                    ),\n",
        "                Dense(embedding_dim\n",
        "                      ),\n",
        "                Dropout(rate=.05\n",
        "                         ),\n",
        "                Dense(units=vocab_size, \n",
        "                      activation=None,\n",
        "                      )          \n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', \n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "                                                        from_logits=True),\n",
        "                  metrics=['sparse_categorical_accuracy', \n",
        "                           'sparse_categorical_crossentropy']\n",
        "                  )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJhnIUHSRfTt"
      },
      "source": [
        "# Function: Train model\n",
        "def train_model(model, train_data, validation_data, epochs, \n",
        "                batch_size=BATCH_SIZE, filepath=FILEPATH, \n",
        "                checkpoint_dir=CHECKPOINT_DIR):\n",
        "\n",
        "    from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "    # define checkpoint file naming format\n",
        "    checkpoint_filename = checkpoint_dir + \\\n",
        "            'ckpt_{epoch:02d}_{sparse_categorical_accuracy:04f}.ckpt'\n",
        "\n",
        "    # set checkpoint options\n",
        "    save_freq=4*BATCH_SIZE\n",
        "    checkpoint = ModelCheckpoint(\n",
        "                        filepath=checkpoint_filename,\n",
        "                        monitor='sparse_categorical_accuracy',\n",
        "                        save_weights_only=True,\n",
        "                        save_best_only=False,\n",
        "                        save_freq=save_freq,\n",
        "                        verbose=1)\n",
        "\n",
        "    # define early stopping criteria\n",
        "    stopping = EarlyStopping(\n",
        "                    monitor=\"val_loss\",\n",
        "                    min_delta=1e-2,\n",
        "                    patience=2,\n",
        "                    verbose=1,\n",
        "                )\n",
        "\n",
        "    history = model.fit(train_data.repeat(100), \n",
        "                        validation_data=valid_data.repeat(100), \n",
        "                        epochs=epochs, \n",
        "                        validation_steps=save_freq, \n",
        "                        callbacks=[checkpoint, stopping],\n",
        "                        batch_size=batch_size,\n",
        "                        )\n",
        "    \n",
        "    return model, history"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf8_OrNEvKcL"
      },
      "source": [
        "### Create Text Generation Function\r\n",
        "Adapt trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYl_VlGatdzu"
      },
      "source": [
        "# Final Prediction Function #######\r\n",
        "\r\n",
        "# Use the model to generate a token sequence\r\n",
        "def make_prediction(init_string, num_generation_steps, precision_reduction=0,\r\n",
        "                    model_name=AUTHOR, print_result=False):\r\n",
        "    \r\n",
        "    our_model = prediction_model\r\n",
        "    our_tokenizer = tokenizer\r\n",
        "\r\n",
        "    GRU_layer = our_model.get_layer('GRU_1')\r\n",
        "\r\n",
        "    #batch_size=1\r\n",
        "    \r\n",
        "    token_sequence = our_tokenizer.texts_to_sequences([init_string])\r\n",
        "    initial_state = None\r\n",
        "    input_sequence = token_sequence\r\n",
        "    init_len = len(input_sequence[0])\r\n",
        "\r\n",
        "    for i in range(num_generation_steps):\r\n",
        "        logits = get_logits(our_model, input_sequence, initial_state=initial_state)\r\n",
        "        sampled_token = sample_token(logits, precision_reduction)\r\n",
        "        token_sequence[0].append(sampled_token)\r\n",
        "        input_sequence = [[sampled_token]]  # use only last letter because previous model state is carried forward\r\n",
        "        initial_state = GRU_layer.states[0].numpy()\r\n",
        "\r\n",
        "    predicted_text = our_tokenizer.sequences_to_texts(token_sequence)[0][::2]\r\n",
        "    \r\n",
        "    if print_result:\r\n",
        "        print(predicted_text)\r\n",
        "\r\n",
        "    return predicted_text"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI5-5cZphPGz"
      },
      "source": [
        "*************************************\r\n",
        "*************************************\r\n",
        "\r\n",
        "### Implement Functions to Create Models\r\n",
        "\r\n",
        "*************************************\r\n",
        "*************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0sajFK7knqH"
      },
      "source": [
        "##### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_2DwhSf7TbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0318281-e5ce-498a-9d98-038a0d825651"
      },
      "source": [
        "def get_tokenizer():\r\n",
        "\r\n",
        "    # Load Data ##########\r\n",
        "    text_chunks = load_parse(num_words_per_chunk=25, \r\n",
        "                             display_samples=True, shuffle=SHUFFLE)\r\n",
        "\r\n",
        "    # Encoding Map ##########\r\n",
        "    # Get fitted tokenizer\r\n",
        "    tokenizer = create_character_tokenizer(text_chunks)\r\n",
        "\r\n",
        "    return tokenizer, text_chunks\r\n",
        "\r\n",
        "\r\n",
        "tokenizer, text_chunks = get_tokenizer()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is Lestrade! Good-afternoon, Lestrade! You will find an extra tumbler upon the sideboard, and there are cigars in the box.” The official detective was attired\n",
            "\n",
            "natural, and affectionate then. You were the most unspoiled creature in the whole world. Now, I don't know what has come over you. You talk\n",
            "\n",
            "Each day the sun rose earlier and set later. It was dawn by three in the morning, and twilight lingered till nine at night. The\n",
            "\n",
            "several times and waited. No: everything was still. It was merely the sound of his own footsteps. When he reached the library, he saw the\n",
            "\n",
            "will leave the country, sir. Then the charge against him will break down.” “Hum! We will talk about that. And now let us hear a\n",
            "\n",
            "len(words): 327797\n",
            "len(text_chunks): 13112\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzHw4wbCRfSH",
        "outputId": "e28b7088-ce70-483b-bd16-3d02316690be"
      },
      "source": [
        "def preprocess_data(tokenizer, text_chunks, cache_dir=CACHE_DIR):\n",
        "\n",
        "    # Create Datasets ##########\n",
        "    # Apply encoding to text (using tokenizer)\n",
        "    seq_chunks = strings_to_sequences(tokenizer, text_chunks)\n",
        "\n",
        "    # Pad sequence with zeros to establish uniform lengths\n",
        "    padded_sequences = make_padded_dataset(seq_chunks)\n",
        "\n",
        "    # Convert sequences into input and target arrays\n",
        "    input_seq, target_seq = create_inputs_and_targets(padded_sequences)\n",
        "\n",
        "    # rearrange data into batches for use in stateful model\n",
        "    # (cannot use if data is shuffled)\n",
        "    if SHUFFLE==False:\n",
        "        input_seq, target_seq = \\\n",
        "            preprocess_stateful(input_seq, target_seq)\n",
        "\n",
        "    # Create the training and validation Datasets\n",
        "    train_data, valid_data = \\\n",
        "        make_Dataset(input_seq, target_seq, \n",
        "                     validation_split=.2)\n",
        "                    \n",
        "    # save / load datasets\n",
        "    #train_data = train_data.cache(cache_dir)   \n",
        "    #valid_data = valid_data.cache(cache_dir) \n",
        "\n",
        "    print(train_data)\n",
        "\n",
        "    return train_data, valid_data\n",
        "\n",
        "\n",
        "train_data, valid_data = \\\n",
        "    preprocess_data(tokenizer, text_chunks)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PrefetchDataset shapes: ((32, 499), (32, 499)), types: (tf.int32, tf.int32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj2fmp91kvai"
      },
      "source": [
        "##### Create / Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmmEfYo-RfTl"
      },
      "source": [
        "def build_init_model(train_data, valid_data, tokenizer,\n",
        "                     filepath=FILEPATH, show_summary=False):\n",
        "\n",
        "    \"\"\" Initializes and trains a model for 1 epoch, \n",
        "    and saves it to disk \"\"\"\n",
        "\n",
        "    model = get_model(len(tokenizer.word_index) + 1)\n",
        "\n",
        "    if show_summary:\n",
        "        model.summary()\n",
        "\n",
        "    # train the model for one epoch\n",
        "    model, history = \\\n",
        "        train_model(model, train_data, valid_data, epochs=1)\n",
        "\n",
        "    # Save model\n",
        "    model.save(filepath)\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeEvkI58p2KE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ef542c-0ba8-4a61-fddb-0bfebe5af502"
      },
      "source": [
        "# Load existing model (if it exists). \r\n",
        "# Otherwise, create a new model\r\n",
        "\r\n",
        "try:\r\n",
        "    # get model weights\r\n",
        "    checkpoint = tf.train.latest_checkpoint(CHECKPOINT_DIR)\r\n",
        "    \r\n",
        "    # recreate model\r\n",
        "    init_model = get_model(len(tokenizer.word_index) + 1)\r\n",
        "    init_model.load_weights(checkpoint)\r\n",
        "    init_model.summary()\r\n",
        "\r\n",
        "except: \r\n",
        "\r\n",
        "    init_model, init_history = \\\r\n",
        "        build_init_model(train_data, valid_data, tokenizer, show_summary=True)\r\n",
        "        \r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (32, None, 256)           24576     \n",
            "_________________________________________________________________\n",
            "gru_2 (GRU)                  (32, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "GRU_1 (GRU)                  (32, None, 1024)          6297600   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (32, None, 256)           262400    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (32, None, 256)           0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (32, None, 96)            24672     \n",
            "=================================================================\n",
            "Total params: 10,547,552\n",
            "Trainable params: 10,547,552\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "  128/32700 [..............................] - ETA: 9:27:08 - loss: 0.8803 - sparse_categorical_accuracy: 0.1945 - sparse_categorical_crossentropy: 4.1819\n",
            "Epoch 00001: saving model to /content/gdrive/MyDrive/Colab_Notebooks/models/assorted/checkpoints/ckpt_01_0.265755.ckpt\n",
            "  149/32700 [..............................] - ETA: 9:29:04 - loss: 0.8578 - sparse_categorical_accuracy: 0.2057 - sparse_categorical_crossentropy: 4.1069"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU0i_C0Dx0Cy"
      },
      "source": [
        "# add additional training epochs\r\n",
        "train_more = True\r\n",
        "additional_epochs = 1\r\n",
        "\r\n",
        "if train_more:  \r\n",
        "    init_model, init_history = train_model(init_model, \r\n",
        "                                           train_data, \r\n",
        "                                           valid_data, \r\n",
        "                                           epochs=additional_epochs)\r\n",
        "\r\n",
        "    # save model \r\n",
        "    # note: model weights also saved via checkpoint callback\r\n",
        "    init_model.save(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n05eWlUeN9T"
      },
      "source": [
        "********************************************************\r\n",
        "********************************************************"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msBeLSoRfT7"
      },
      "source": [
        "##### Adapt model to accept single line inputs (batch size = 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzQW69qxRfT_"
      },
      "source": [
        "# Function: produce modified model for making text predictions\n",
        "def get_prediction_model(tokenizer, author=AUTHOR, \n",
        "                         checkpoint_dir=CHECKPOINT_DIR):\n",
        "    \n",
        "    # get model, setting batch size = 1\n",
        "    our_tokenizer = tokenizer\n",
        "    print(our_tokenizer)\n",
        "    user_model = get_model(len(our_tokenizer.word_index) + 1, batch_size=1)\n",
        "    checkpoint_file = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "\n",
        "    # load weights\n",
        "    user_model.load_weights(checkpoint_file)  \n",
        "    user_model.trainable=False\n",
        "\n",
        "    return user_model\n",
        "\n",
        "\n",
        "# Store trained model separate from checkpoints\n",
        "def save_model_update(model_dir=MODEL_DIR):\n",
        "\n",
        "    # save model\n",
        "    prediction_model = get_prediction_model(tokenizer)\n",
        "    prediction_model.save(model_dir)\n",
        "\n",
        "    # save tokenizer\n",
        "    with open(model_dir + '/tokenizer.pickle', 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A0FOxOTRfUc"
      },
      "source": [
        "# Generate text from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPexjrJCm1h"
      },
      "source": [
        "# load prediction model\r\n",
        "def load_model_update(model_dir=MODEL_DIR):\r\n",
        "   \r\n",
        "    prediction_model =  \\\r\n",
        "        tf.keras.models.load_model(model_dir)\r\n",
        "\r\n",
        "    # load tokenizer\r\n",
        "    with open(model_dir + '/tokenizer.pickle', 'rb') as handle:\r\n",
        "        tokenizer = pickle.load(handle)\r\n",
        "\r\n",
        "    return prediction_model, tokenizer\r\n",
        "\r\n",
        "\r\n",
        "prediction_model, tokenizer = load_model_update() \r\n",
        "prediction_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aHg6y44RfUJ"
      },
      "source": [
        "# Function: Outputs weighted predictions for next token \n",
        "# (as logits / log-odds ration)\n",
        "def get_logits(model, token_sequence, initial_state=None):\n",
        "    \"\"\"\n",
        "    Paramater:\n",
        "    model - our prediction model set with batch size = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # carry forward previous state from GRU layer\n",
        "    GRU_layer = model.get_layer('GRU_1')\n",
        "    GRU_layer.reset_states(initial_state)\n",
        "\n",
        "    # Get the model's next token prediction (as logits)\n",
        "    input = tf.constant(token_sequence)\n",
        "    final_pred = model(input)[:, -1, :]\n",
        "        \n",
        "    return final_pred.numpy()    \n",
        "\n",
        "\n",
        "# Function: selects a value from logits prediction\n",
        "def sample_token(logits, precision_reduction=0):   \n",
        "\n",
        "    # choose a value from logits distribution\n",
        "    # fuzz_factor: adds some imprecision to results\n",
        "    fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n",
        "\n",
        "    sample = tf.random.categorical(\n",
        "                        logits=logits * (1 + precision_reduction * fuzz_factor), \n",
        "                        num_samples=1, \n",
        "                        )\n",
        "\n",
        "    # convert to integer\n",
        "    next_token = sample[0,0].numpy()\n",
        "\n",
        "    return next_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgXNTiaYRfUd"
      },
      "source": [
        "# test model\n",
        "init_string = 'EMMY:'  # starting point for prediction\n",
        "num_generation_steps = 300  # max number characters to produce\n",
        "\n",
        "\n",
        "make_prediction(init_string, num_generation_steps, \n",
        "                precision_reduction=5, print_result=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTXyxnDg6Qae"
      },
      "source": [
        "def generate_text(starting_text, precision_reduction=0, author='buy_local'):\r\n",
        "\r\n",
        "  num_generation_steps = 350  # max number characters to produce\r\n",
        "  \r\n",
        "  prediction = make_prediction(init_string=starting_text, \r\n",
        "                               num_generation_steps=num_generation_steps, \r\n",
        "                               precision_reduction=precision_reduction, \r\n",
        "                               model_name=author, \r\n",
        "                               print_result=False)\r\n",
        "  \r\n",
        "  return prediction + '...'\r\n",
        "\r\n",
        "\r\n",
        "generate_text(starting_text='hendrix', author='buy_local')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKJkajU_ebMj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}