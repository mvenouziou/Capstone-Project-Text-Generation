{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_nonlinear_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUq53WdfyaneZYxgkv0kTX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Text-Generation/blob/main/Mo_nonlinear_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQyxxivFD7r"
      },
      "source": [
        "## Text Generation RNN\r\n",
        "\r\n",
        "This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. \r\n",
        "\r\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\r\n",
        "- See more at https://raw.githubusercontent.com/mvenouziou/text_generator.\r\n",
        "\r\n",
        "- See credits /attributions below\r\n",
        "\r\n",
        "The code implements two different model architectures: \"linear\" and \"nonlinear.\"\r\n",
        "The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**What's New?**\r\n",
        "*(Although very likely that others have created similar models, I personally have not seen them and these can be considered independent constructions. Citations for other content are below:)*\r\n",
        "\r\n",
        "- Option to implement either the standard linear model architecture (see credits below) or nonlinear architectures.\r\n",
        "\r\n",
        "- Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. \r\n",
        "\r\n",
        "- Manage RNN statefulness for independent data sources. The linear models credited below use a single continuous work, which necessarily implies a dependence relation between samples / batches. This model implements the ability to treat independent works (individual poems, books, authors, etc.) as truly independent samples by resetting RNN states and shuffling independent data sources.\r\n",
        "\r\n",
        "- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzU2QxoMFF7d"
      },
      "source": [
        "---\r\n",
        "**Credits / Citations / Attributions:**\r\n",
        "\r\n",
        "**Linear Model and Shared Code** \r\n",
        "Other than items noted in previous sections, this python code and linear model structure is based heavily on code found in Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*\r\n",
        "\r\n",
        "\r\n",
        "**Nonlinear Model:**   \r\n",
        "\r\n",
        "This utilizes the pretrained embeddings:\r\n",
        "-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" *https://tfhub.dev/google/collections/bert/1)*\r\n",
        "- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" *https://hub.tensorflow.google.cn/google/electra_small/2)*\r\n",
        "\r\n",
        "**Web App** \r\n",
        "The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**About**\r\n",
        "\r\n",
        "Find me online at:\r\n",
        "- LinkedIn: https://www.linkedin.com/in/movenouziou/ \r\n",
        "- GitHub: https://github.com/mvenouziou\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1EW3w1GvDLx"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\r\n",
        "# ML design\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "!pip install -q tensorflow-text\r\n",
        "import tensorflow_text as text  # text processing / required for BERT encoder\r\n",
        "import tensorflow_hub as hub  # for BERT encoder\r\n",
        "\r\n",
        "# data handling\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import random\r\n",
        "\r\n",
        "# file management\r\n",
        "import os\r\n",
        "import bz2\r\n",
        "import pickle\r\n",
        "import _pickle as cPickle"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjhCdyMaX1Wh",
        "outputId": "c69a8110-4b64-46d9-c4da-5a50f0ec42a2"
      },
      "source": [
        "# 3rd party integrations\r\n",
        "\r\n",
        "# Mount Google Drive:\r\n",
        "GDRIVE_DIR = '/content/gdrive/'\r\n",
        "from google.colab import drive\r\n",
        "drive.mount(GDRIVE_DIR)\r\n",
        "\r\n",
        "# Anvil's web app server\r\n",
        "USE_ANVIL = False  \r\n",
        "\r\n",
        "if USE_ANVIL:\r\n",
        "    !pip install -q anvil-uplink\r\n",
        "    import anvil.server\r\n",
        "\r\n",
        "if USE_ANVIL:\r\n",
        "    anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3iL0mYZcgEL"
      },
      "source": [
        "# set author / file paths\r\n",
        "AUTHOR = 'tests'\r\n",
        "\r\n",
        "# model structure\r\n",
        "USE_WORD_PATH = True\r\n",
        "if USE_WORD_PATH:\r\n",
        "    AUTHOR = AUTHOR + '_words_model/'\r\n",
        "else: PATH_EXTENSION = ''\r\n",
        "\r\n",
        "# saving models/ checkpoints\r\n",
        "# (Google Drive)\r\n",
        "FILEPATH = GDRIVE_DIR + 'MyDrive/Colab_Notebooks/models/text_generation/' + AUTHOR\r\n",
        "CHECKPOINT_DIR = FILEPATH + '/checkpoints/'\r\n",
        "PREDICTION_MODEL_DIR = FILEPATH + '/prediction_model/'\r\n",
        "TRAINING_MODEL_DIR = FILEPATH + '/training_model/'\r\n",
        "PROCESSED_DATA_DIR = FILEPATH + '/proc_data/'\r\n",
        "\r\n",
        "# online dataset repository\r\n",
        "DATASETS_DIR = 'https://raw.githubusercontent.com/mvenouziou/text_generator/main/'\r\n",
        "DATA_FILES = ['robert_frost_collection.csv']"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzpBe-DQv4Nr"
      },
      "source": [
        "# GLOBAL PARAMATERS\r\n",
        "\r\n",
        "NUM_TRAILING_WORDS = 5  # for word model path\r\n",
        "PADDED_EXAMPLE_LENGTH = 500  # for character model path\r\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39FgKokRXBe1"
      },
      "source": [
        "### Define Encoders / Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl6b4S8zeFdi"
      },
      "source": [
        "Character-Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN13GwsjyB9z"
      },
      "source": [
        "def create_character_tokenizer():\r\n",
        "    \"\"\"\r\n",
        "    This function takes a list of strings as its argument. It should create \r\n",
        "    and return a Tokenizer according to the above specifications. \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    char_tokens = string.printable\r\n",
        "    filters = '#$%&()*+-/<=>@[]^_`{|}~\\t'\r\n",
        "\r\n",
        "    # Initialize standard keras tokenizer\r\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "                    num_words=None,  \r\n",
        "                    filters=filters,\r\n",
        "                    lower=False,  # conversion to lowercase letters\r\n",
        "                    char_level=True,\r\n",
        "                    oov_token=None,  # drop unknown characters\r\n",
        "                    )\r\n",
        "    \r\n",
        "    # fit tokenizer\r\n",
        "    tokenizer.fit_on_texts(char_tokens)\r\n",
        "\r\n",
        "    return tokenizer"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqQHC1D6Uwp"
      },
      "source": [
        "def make_padded_array(text_blocks, tokenizer=None, max_len=PADDED_EXAMPLE_LENGTH):\r\n",
        "    # Tokenizes and applies padding for uniform length\r\n",
        "\r\n",
        "    # load tokenizer if one is not supplied\r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "    # tokenize\r\n",
        "    token_blocks = tokenizer.texts_to_sequences(text_blocks)\r\n",
        "\r\n",
        "    # zero padding\r\n",
        "    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "                        sequences=token_blocks,  # dataset\r\n",
        "                        maxlen=max_len, \r\n",
        "                        dtype='int32', \r\n",
        "                        padding='pre',\r\n",
        "                        truncating='pre', \r\n",
        "                        value=0.0\r\n",
        "                        )\r\n",
        "    \r\n",
        "    return padded_blocks"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqkR0FveIBt"
      },
      "source": [
        "Word-Level (BERT or Electra pre-trained embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC-8wxwtxDD"
      },
      "source": [
        "def get_bert_encoder(seq_length=NUM_TRAILING_WORDS, use_electra=False):\r\n",
        "\r\n",
        "    # Word Embeddings path (bert encoder)\r\n",
        "    if use_electra:\r\n",
        "        encoder_url = 'https://tfhub.dev/google/electra_small/2'\r\n",
        "    else:\r\n",
        "        encoder_url = 'https://tfhub.dev/tensorflow/' \\\r\n",
        "                            + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\r\n",
        "    preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\r\n",
        "                        + 'bert_en_uncased_preprocess/3'\r\n",
        "                \r\n",
        "    # preprocessing layer\r\n",
        "    # get BERT components\r\n",
        "    preprocessor = hub.load(preprocessor_url)\r\n",
        "    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize,\r\n",
        "                                    name='bert_tokenizer')\r\n",
        "    bert_packer = hub.KerasLayer(preprocessor.bert_pack_inputs,\r\n",
        "                                 arguments=dict(seq_length=seq_length),\r\n",
        "                                 name='bert_input_packer')\r\n",
        "    bert_encoder = hub.KerasLayer(encoder_url, trainable=False, \r\n",
        "                             name='BERT_encoder')\r\n",
        "    \r\n",
        "    return bert_tokenizer, bert_packer, bert_encoder"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8EYMa9-XKJY"
      },
      "source": [
        "### Define Data Pre-processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqJnhind4a_"
      },
      "source": [
        "Load and Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzwvJaIXQ3q"
      },
      "source": [
        "# Function: loader for .csv files\r\n",
        "def prepare_csv(filename, datasets_dir=DATASETS_DIR, \r\n",
        "                content_columns=['Name', 'Content'], shuffle_rows=True):\r\n",
        "    \r\n",
        "    # load data into DataFrame\r\n",
        "    dataframe = pd.read_csv(datasets_dir + filename).dropna()\r\n",
        "    \r\n",
        "    # extract titles and content\r\n",
        "    # note: column headings must match those below\r\n",
        "    if 'Name ' in dataframe.columns:  # required for the Robert Frost set\r\n",
        "        dataframe.rename(columns={'Name ':'Name'})\r\n",
        "    \r\n",
        "    # prepare titles\r\n",
        "    try: \r\n",
        "        dataframe['Name'] = dataframe['Name'].apply(\r\n",
        "                            lambda x: x.upper() + ':\\n')\r\n",
        "    except:\r\n",
        "        # no titles found\r\n",
        "        content_columns = ['Content']\r\n",
        "\r\n",
        "    # prepare content\r\n",
        "    dataframe['Content'] = dataframe['Content'].apply(\r\n",
        "                    lambda x: x + '\\n')\r\n",
        "\r\n",
        "    # restrict dataset\r\n",
        "    dataframe = dataframe[content_columns]\r\n",
        "\r\n",
        "    # shuffle entries (rows)\r\n",
        "    if shuffle_rows:\r\n",
        "        dataframe = dataframe.sample(frac=1)\r\n",
        "    \r\n",
        "    # data cleanup\r\n",
        "    dataframe = dataframe[content_columns]\r\n",
        "    \r\n",
        "    # merge desired text columns\r\n",
        "    dataframe['merge'] = dataframe[content_columns[0]]\r\n",
        "    for i in range(1, len(content_columns)):\r\n",
        "        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\r\n",
        "\r\n",
        "    # convert to list of strings\r\n",
        "    data_list = dataframe['merge'].tolist()\r\n",
        "    \r\n",
        "    return data_list   \r\n",
        "\r\n",
        "\r\n",
        "# Function: Load and standardize data files\r\n",
        "def load_parse(data_list, display_samples=True):  \r\n",
        "\r\n",
        "    # remove paragraph / line marks and split up words  \r\n",
        "    tokenizer = text.WhitespaceTokenizer()\r\n",
        "\r\n",
        "    # tokenize data (outputs bytestrings)\r\n",
        "    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]\r\n",
        "\r\n",
        "    # convert data back to string format\r\n",
        "    num_entries = len(cleaned_list_byte)\r\n",
        "\r\n",
        "    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) \r\n",
        "                    for i in range(num_entries)]\r\n",
        "\r\n",
        "    # Display some text samples\r\n",
        "    if display_samples:\r\n",
        "        num_samples = 5\r\n",
        "        inx = np.random.choice(len(clean_list), num_samples, replace=False)\r\n",
        "        for example in np.array(clean_list)[inx]:\r\n",
        "            print(example)\r\n",
        "            print()\r\n",
        "\r\n",
        "        print('len(text_chunks):', len(clean_list))\r\n",
        "\r\n",
        "    return clean_list"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1HAd-j7J9eq"
      },
      "source": [
        "def create_input_target_blocks(full_examples, tokenizer=None,\r\n",
        "                               max_len=PADDED_EXAMPLE_LENGTH,\r\n",
        "                               num_words=NUM_TRAILING_WORDS):\r\n",
        "    # converts text into sliding n-grams of words and characters\r\n",
        "    # returning input / target sets\r\n",
        "\r\n",
        "    # helper function to create word-level inputs\r\n",
        "    def update_word_char_lists(text, chars_list, words_list):\r\n",
        "        words_input = text.split(' ')  # separate words\r\n",
        "        words_input = words_input[-num_words-1: -1]  # get trailing words\r\n",
        "\r\n",
        "        # convert words to string (tensor)\r\n",
        "        words_input = ' '.join(words_input)\r\n",
        "\r\n",
        "        # add values to lists\r\n",
        "        chars_list.append(text)\r\n",
        "        words_list.append([words_input])\r\n",
        "        \r\n",
        "        return None\r\n",
        "\r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "    blocks = []\r\n",
        "    for example in full_examples:      \r\n",
        "\r\n",
        "        char_block = []\r\n",
        "        word_block = []\r\n",
        "        example_length = len(example)\r\n",
        "\r\n",
        "        # small blocks at start (will be zero-padded later)\r\n",
        "        leading_characters = 1  # min chars to seed predictions\r\n",
        "        for i in range(leading_characters, example_length - max_len - 1):\r\n",
        "            text = example[: i]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "\r\n",
        "        # full length blocks\r\n",
        "        for i in range(example_length - max_len - 1):\r\n",
        "            # create n-gram\r\n",
        "            text = example[i: max_len + i]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "\r\n",
        "        # small blocks at end (will be zero-padded later)\r\n",
        "        for i in range(example_length - max_len - 1, example_length-1):\r\n",
        "            text = example[i: ]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "    \r\n",
        "        # tokenize and add pre-padding\r\n",
        "        char_block = make_padded_array(char_block, tokenizer, max_len=max_len)\r\n",
        "\r\n",
        "        # separate into inputs and targets\r\n",
        "        inputs_char = char_block[:, :-1]\r\n",
        "        targets_char = char_block[:, 1:]\r\n",
        "\r\n",
        "        # update blocks\r\n",
        "        word_block = np.array(word_block)\r\n",
        "        blocks.append((inputs_char, word_block, targets_char))\r\n",
        "\r\n",
        "    return blocks"
      ],
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9aQIGZu6te"
      },
      "source": [
        "# Function: data prep to create stateful RNN batches\r\n",
        "# note: This will be applied separately on each example text, \r\n",
        "# so that RNN can reset internal state / distinguish between unrelated passages\r\n",
        "# note: This code is taken directly from Imperial College London's \r\n",
        "# Coursera course cited above\r\n",
        "\r\n",
        "def preprocess_stateful(char_input, word_input, target, batch_size=BATCH_SIZE):\r\n",
        "\r\n",
        "    # Prepare input and output arrays for training the stateful RNN\r\n",
        "    num_examples = char_input.shape[0]\r\n",
        "\r\n",
        "    # adjust for batch size to divide evenly into sample size\r\n",
        "    num_processed_examples = num_examples - (num_examples % batch_size)\r\n",
        "    input_cropped = char_input[:num_processed_examples]\r\n",
        "    target_cropped = target[:num_processed_examples]\r\n",
        "\r\n",
        "    # separate out samples so rows of data match up across epochs\r\n",
        "    # 'steps' measures how to space them out\r\n",
        "    steps = num_processed_examples // batch_size  \r\n",
        "\r\n",
        "    # define reordering\r\n",
        "    inx = np.empty((0,), dtype=np.int32)  # initialize empty array object\r\n",
        "    \r\n",
        "    for i in range(steps):\r\n",
        "        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \r\n",
        "                                                    steps)))\r\n",
        "\r\n",
        "    # reorder the data\r\n",
        "    input_char_stateful = input_cropped[inx]\r\n",
        "    input_word_stateful = word_input[inx]\r\n",
        "    target_seq_stateful = target_cropped[inx]\r\n",
        "\r\n",
        "    return input_char_stateful, input_word_stateful, target_seq_stateful"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlzYCN71orw"
      },
      "source": [
        "Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOSV4SWyyoS"
      },
      "source": [
        "def input_pipeline(data_files=DATA_FILES, verbose=True, batch_size=BATCH_SIZE, \r\n",
        "                   max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS,\r\n",
        "                   datasets_dir=DATASETS_DIR, saved_proc_dir=PROCESSED_DATA_DIR):\r\n",
        "\r\n",
        "    # load previously processed data (pbz2 compressed file format)\r\n",
        "    try:    \r\n",
        "        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\r\n",
        "            data_dict = cPickle.load(file)\r\n",
        "\r\n",
        "        X_data_list = data_dict['X_data_list']\r\n",
        "        Y_data_list = data_dict['Y_data_list']\r\n",
        "\r\n",
        "        print('loaded saved pre-processed data')\r\n",
        "\r\n",
        "    except:       \r\n",
        "\r\n",
        "        # load data file\r\n",
        "        data_list = []\r\n",
        "        for filename in data_files:\r\n",
        "\r\n",
        "            # check file extension and select loader (csv or txt)\r\n",
        "            _, file_extension = os.path.splitext(filename)     \r\n",
        "\r\n",
        "            if file_extension == '.csv':   \r\n",
        "                data = prepare_csv(filename, \r\n",
        "                                datasets_dir=datasets_dir, \r\n",
        "                                content_columns=['Name', 'Content'], \r\n",
        "                                shuffle_rows=True)\r\n",
        "                \r\n",
        "            else: # file_extension == '.txt':\r\n",
        "                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:\r\n",
        "                    data = file.readlines()\r\n",
        "\r\n",
        "            # add extracted list of texts to data list\r\n",
        "            data_list += data\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: data_list created')\r\n",
        "        \r\n",
        "        # clean data\r\n",
        "        clean_list = load_parse(data_list, display_samples=False)\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: clean_list created')\r\n",
        "        \r\n",
        "        # preprocess data\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "        blocks = create_input_target_blocks(full_examples=clean_list, \r\n",
        "                                            tokenizer=tokenizer,\r\n",
        "                                            max_len=max_len,\r\n",
        "                                            num_words=num_words)\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: blocks created')\r\n",
        "        \r\n",
        "        # create separate input / target pairs for each block\r\n",
        "        X_data_list = []\r\n",
        "        Y_data_list = []\r\n",
        "\r\n",
        "        i=0\r\n",
        "        for block in blocks:\r\n",
        "            if i % 10 == 0:\r\n",
        "                print(f'PROGRESS: processing block {i} of {len(blocks)}')\r\n",
        "\r\n",
        "            char_input = block[0] \r\n",
        "            word_input = block[1] \r\n",
        "            target = block[2]\r\n",
        "\r\n",
        "            input_char_stateful, input_word_stateful, target_seq_stateful = \\\r\n",
        "                                    preprocess_stateful(char_input=char_input, \r\n",
        "                                                        word_input=word_input, \r\n",
        "                                                        target=target, \r\n",
        "                                                        batch_size=batch_size)\r\n",
        "\r\n",
        "            # group for model input\r\n",
        "            X = [input_char_stateful, input_word_stateful]\r\n",
        "            Y = target_seq_stateful\r\n",
        "\r\n",
        "            X_data_list.append(X)\r\n",
        "            Y_data_list.append(Y)\r\n",
        "\r\n",
        "            # advance index\r\n",
        "            i += 1\r\n",
        "\r\n",
        "        # save file (pbz2 compressed file format)\r\n",
        "        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\r\n",
        "            cPickle.dump({'X_data_list': X_data_list, \r\n",
        "                          'Y_data_list': Y_data_list}, sfile)\r\n",
        "\r\n",
        "    return X_data_list, Y_data_list"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpjX19aWvU6"
      },
      "source": [
        "### Define Models and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtYOFDwndkK4"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFYArtLttwEF"
      },
      "source": [
        "# Function: Model Definition\r\n",
        "def get_training_model(use_word_path=USE_WORD_PATH,\r\n",
        "                       verbose=True,\r\n",
        "                       batch_size=BATCH_SIZE, \r\n",
        "                       padded_examples=PADDED_EXAMPLE_LENGTH,\r\n",
        "                       num_words=NUM_TRAILING_WORDS):\r\n",
        "    \r\n",
        "    \"\"\" Defines and compiles our stateful RNN model. \r\n",
        "    Note: batch size is required argument for stateful RNN. \"\"\"\r\n",
        "    \r\n",
        "    from keras.layers import Input, Embedding, Concatenate, Dense, GRU,\\\r\n",
        "                             Average, Dropout, BatchNormalization, Lambda\r\n",
        "\r\n",
        "    # parameters\r\n",
        "    vocab_size = len(create_character_tokenizer().word_index) + 1\r\n",
        "    embedding_dim = 256\r\n",
        "    merge_dim = 128\r\n",
        "    \r\n",
        "    # Build model\r\n",
        "    # define input shapes\r\n",
        "    input_1 = Input(shape=(None, ), #(padded_examples-1, ), \r\n",
        "                    batch_size=batch_size,\r\n",
        "                    dtype=tf.int32, \r\n",
        "                    name='char_input')\r\n",
        "    \r\n",
        "    input_2 = Input(shape=(), \r\n",
        "                    batch_size=batch_size,\r\n",
        "                    dtype=tf.string, \r\n",
        "                    name='word_input')\r\n",
        "\r\n",
        "    # travel individual paths\r\n",
        "    # Character Level Path\r\n",
        "    # ## Char: Embedding\r\n",
        "    x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \r\n",
        "                   mask_zero=True, batch_input_shape=(batch_size, None),\r\n",
        "                   name='char_embedding',)(input_1)\r\n",
        "\r\n",
        "    # ## Char: GRU 1\r\n",
        "    x1 = GRU(units=embedding_dim, stateful=True, \r\n",
        "             return_sequences=True, name='char_GRU_1',)(x1)\r\n",
        "    x1 = Dropout(rate=.10, name='char_Dropout_1')(x1)\r\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_1')(x1)\r\n",
        "    \r\n",
        "    # ## Char: GRU Final --  must use output_dim = merge_dim!\r\n",
        "    x1 = GRU(units=merge_dim, stateful=True, \r\n",
        "             return_sequences=True, name='char_GRU_final',)(x1)\r\n",
        "    x1 = Dropout(rate=.10, name='char_Dropout_final')(x1)\r\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_final')(x1)\r\n",
        "\r\n",
        "    # Word Encoding Path\r\n",
        "    if use_word_path:\r\n",
        "        \r\n",
        "        bert_tokenizer, bert_packer, bert_encoder = get_bert_encoder(use_electra=True)\r\n",
        "        x2 = bert_tokenizer(input_2)  # tokenize\r\n",
        "        x2 = bert_packer([x2])  # pack inputs for encoder\r\n",
        "        x2 = bert_encoder(x2)['sequence_output'] # encoding\r\n",
        "\r\n",
        "        # ## Word: GRU 1\r\n",
        "        x2 = GRU(units=32, stateful=True, \r\n",
        "                 return_sequences=True, name='word_GRU_1',)(x2)\r\n",
        "        x2 = Dropout(rate=.10, name='word_Dropout_1')(x2)\r\n",
        "        x2 = BatchNormalization(name='word_Batch_Norm_1')(x2)\r\n",
        "\r\n",
        "        # ## Word: Required conversion to valid merge output dim = merge_dim!\r\n",
        "        x2 = Dense(units=num_words, activation=None, \r\n",
        "                   name='word_Dense_pre_final')(x2)\r\n",
        "        x2 = tf.keras.layers.AveragePooling1D(\r\n",
        "                pool_size=5, padding='same', name='word_pooling_final')(x2)\r\n",
        "        x2 = Dense(units=merge_dim, activation=None, \r\n",
        "                   name='word_Dense_final')(x2)\r\n",
        "\r\n",
        "        # Merge Paths\r\n",
        "        x = Average(name='merged_layers')([x1, x2])\r\n",
        "\r\n",
        "    else:\r\n",
        "        x = x1  # update variable id to match next step\r\n",
        "    \r\n",
        "    # Final GRU layer\r\n",
        "    x = GRU(units=embedding_dim, stateful=True, \r\n",
        "            return_sequences=True, name='GRU_OUTPUT')(x)          \r\n",
        "\r\n",
        "    # Character prediction (logits)\r\n",
        "    outputs = Dense(units=vocab_size, activation=None, \r\n",
        "                    name='Decoding')(x)       \r\n",
        "    \r\n",
        "    # create model\r\n",
        "    model = keras.Model(inputs=[input_1, input_2], outputs=outputs)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "        print(model.summary())\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5YFw5SJut1K"
      },
      "source": [
        "Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7pwjfSAusua"
      },
      "source": [
        "def get_prediction_model(trained_model=None, use_word_path=USE_WORD_PATH,\r\n",
        "                         padded_examples=PADDED_EXAMPLE_LENGTH, verbose=False):\r\n",
        "    \"\"\" enforces batch size = 1, only returns last character prediction\r\n",
        "     and loads any saved weights \"\"\"\r\n",
        "\r\n",
        "    # set paramaters\r\n",
        "    batch_size=1\r\n",
        "\r\n",
        "    # create model\r\n",
        "    prediction_model = get_training_model(batch_size=batch_size, \r\n",
        "                                          use_word_path=use_word_path,\r\n",
        "                                          padded_examples=padded_examples,\r\n",
        "                                          verbose=verbose)\r\n",
        "\r\n",
        "\r\n",
        "    # load weights from pre-trained model\r\n",
        "    if trained_model is not None:        \r\n",
        "        trained_weights = trained_model.get_weights()\r\n",
        "        prediction_model.set_weights(trained_weights)\r\n",
        "\r\n",
        "    return prediction_model"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEC8q6pYYBue"
      },
      "source": [
        "Compiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loz4fJP1YA78"
      },
      "source": [
        "def compile_model(model, learning_rate):\r\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adamax(\r\n",
        "                                learning_rate=learning_rate),\r\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "                                                    from_logits=True),\r\n",
        "                  metrics=['sparse_categorical_accuracy', \r\n",
        "                        'sparse_categorical_crossentropy'],\r\n",
        "                 )\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6KD4XoYoQG"
      },
      "source": [
        "Checkpoint Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K09LDfJYnla"
      },
      "source": [
        "# checkpoint manager\r\n",
        "def create_checkpoint_manager(model, checkpoint_dir=CHECKPOINT_DIR):\r\n",
        "\r\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\r\n",
        "\r\n",
        "    checkpoint_manager = tf.train.CheckpointManager(\r\n",
        "                            checkpoint=checkpoint, \r\n",
        "                            directory=checkpoint_dir, \r\n",
        "                            max_to_keep=4, \r\n",
        "                            keep_checkpoint_every_n_hours=None,\r\n",
        "                            checkpoint_name='ckpt', \r\n",
        "                            step_counter=None, \r\n",
        "                            checkpoint_interval=None,\r\n",
        "                            init_fn=None\r\n",
        "                            )\r\n",
        "    \r\n",
        "    return checkpoint, checkpoint_manager"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWkTKUaXZbm"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZWbiojXXpS"
      },
      "source": [
        "# Function: Train model\r\n",
        "def train_model(model, X_data_list, Y_data_list,\r\n",
        "                num_epochs=1, \r\n",
        "                num_datasets_to_use=1,\r\n",
        "                checkpoint=None, \r\n",
        "                checkpoint_manager=None,\r\n",
        "                learning_rate=0.001,\r\n",
        "                batch_size=BATCH_SIZE, \r\n",
        "                filepath=FILEPATH, \r\n",
        "                checkpoint_dir=CHECKPOINT_DIR):\r\n",
        "\r\n",
        "    # compile model\r\n",
        "    model = compile_model(model, learning_rate=learning_rate)\r\n",
        "\r\n",
        "    # set checkpoint manager\r\n",
        "    if checkpoint is None or checkpoint_manager is None:\r\n",
        "        checkpoint, checkpoint_manager = \\\r\n",
        "                    create_checkpoint_manager(model=model, \r\n",
        "                                              checkpoint_dir=checkpoint_dir)\r\n",
        "    \r\n",
        "    # organize training data\r\n",
        "    num_blocks = len(X_data_list)\r\n",
        "    train_datasets_list = list(zip(X_data_list, Y_data_list))       \r\n",
        "    \r\n",
        "    # begin training loop\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "\r\n",
        "        print(f'Epoch: {epoch}')\r\n",
        "\r\n",
        "        # shuffle dataset order\r\n",
        "        random.shuffle(train_datasets_list)\r\n",
        "        print('shuffled datasets')\r\n",
        "\r\n",
        "        for i in range(num_datasets_to_use):\r\n",
        "            print(f'dataset: {i}')\r\n",
        "\r\n",
        "            # select dataset\r\n",
        "            data = train_datasets_list[i]\r\n",
        "            X = data[0]\r\n",
        "            Y = data[1]\r\n",
        "\r\n",
        "            # train model\r\n",
        "            history = model.fit(x=X, y=Y,\r\n",
        "                                shuffle=False,\r\n",
        "                                epochs=1,\r\n",
        "                                verbose=1)\r\n",
        "            \r\n",
        "            # reset RNN hidden states\r\n",
        "            model.reset_states()\r\n",
        "\r\n",
        "            # save checkpoint\r\n",
        "            checkpoint_manager.save()\r\n",
        "\r\n",
        "        # save full model at end of each epoch\r\n",
        "        model.save(checkpoint_dir + 'saved_model_epoch_' + str(epoch))\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMNy3XTW2Be"
      },
      "source": [
        "### Define Implementation Functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuUcE7H4snz"
      },
      "source": [
        "def convert_to_input(last_token, trunc_text, \r\n",
        "                     prepare_words=USE_WORD_PATH,\r\n",
        "                     max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "                     num_words=NUM_TRAILING_WORDS):\r\n",
        "    \r\n",
        "    # words\r\n",
        "    if prepare_words:\r\n",
        "        words_input = trunc_text.split(' ')  # separate words\r\n",
        "        words_input = words_input[-num_words-1:-1]  # get trailing words\r\n",
        "        words_input = tf.constant(' '.join(trunc_text))  # convert to tensor\r\n",
        "    else:\r\n",
        "        words_input=tf.constant(' ')\r\n",
        "\r\n",
        "    # pad token sequence\r\n",
        "    inputs_char=tf.constant(last_token)\r\n",
        "    \"\"\"\r\n",
        "    length = max_len - 1\r\n",
        "    inputs_char = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "                        sequences=inputs_char,  # dataset\r\n",
        "                        maxlen=length, \r\n",
        "                        dtype='int32', \r\n",
        "                        padding='pre',\r\n",
        "                        truncating='pre', \r\n",
        "                        value=0.0\r\n",
        "                        )\r\n",
        "    \"\"\"\r\n",
        "    # create separate input / target pairs for each block\r\n",
        "    X = [inputs_char, words_input]\r\n",
        "\r\n",
        "    return X"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2atvRiGv-z"
      },
      "source": [
        "def generator(input_text, prediction_model, precision_reduction=0, \r\n",
        "              num_characters=250, tokenizer=None, \r\n",
        "              max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS, \r\n",
        "              print_result=True):\r\n",
        "\r\n",
        "    # get tokenizer (if not supplied)      \r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "    \r\n",
        "    # initialize generated text\r\n",
        "    last_token =  tokenizer.texts_to_sequences([input_text])\r\n",
        "    trunc_text = input_text.upper() + ':\\n'\r\n",
        "    generated_text = []\r\n",
        "   \r\n",
        "    # text generation loop\r\n",
        "    initial_state = None\r\n",
        "    for _ in range(num_characters):\r\n",
        "\r\n",
        "        # prepare input for model\r\n",
        "        inputs = convert_to_input(last_token=last_token, \r\n",
        "                                  trunc_text=trunc_text,\r\n",
        "                                  max_len=max_len,\r\n",
        "                                  num_words=num_words)\r\n",
        "        \r\n",
        "        # pass forward final GRU layer state\r\n",
        "        GRU_layer = prediction_model.get_layer('GRU_OUTPUT')\r\n",
        "        GRU_layer.reset_states(initial_state)\r\n",
        "        \r\n",
        "        # run model and compute logits\r\n",
        "        output = prediction_model(inputs)\r\n",
        "        logits = output[:, -1, :]  # extract last character logits\r\n",
        "        logits = logits.numpy()\r\n",
        "       \r\n",
        "        # generate next character from logits distribution\r\n",
        "        # purturb probabilities (optional)\r\n",
        "        if precision_reduction != 0:\r\n",
        "            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\r\n",
        "            logits = logits * (1 + precision_reduction * fuzz_factor)\r\n",
        "\r\n",
        "        last_token = tf.random.categorical(logits=logits, num_samples=1)\r\n",
        "        last_token = last_token.numpy().tolist()\r\n",
        "        \r\n",
        "        # get input for next character prediction\r\n",
        "        input_text = tokenizer.sequences_to_texts(last_token)\r\n",
        "        input_text = input_text[0]\r\n",
        "\r\n",
        "        # record generated character\r\n",
        "        generated_text.append(input_text)\r\n",
        "\r\n",
        "        #  get GRU state for next character prediction\r\n",
        "        initial_state = GRU_layer.states[0].numpy()\r\n",
        "\r\n",
        "    # reset for next run\r\n",
        "    output_text = ''.join(generated_text)\r\n",
        "    \r\n",
        "    if print_result:\r\n",
        "        print(output_text)\r\n",
        "\r\n",
        "    return output_text"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rFetpOUqwQ"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tf4x1q8UpfD"
      },
      "source": [
        "# Store trained model separate from checkpoints\r\n",
        "def save_model(model, model_dir):\r\n",
        "\r\n",
        "    # save model\r\n",
        "    model.save(model_dir)\r\n",
        "\r\n",
        "    # get tokenizer\r\n",
        "    prediction_tokenizer = create_character_tokenizer()\r\n",
        "    \r\n",
        "    # save tokenizer\r\n",
        "    with open(model_dir + 'tokenizer.pickle', 'wb') as file:\r\n",
        "        pickle.dump(prediction_tokenizer, file, pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO27cbff77C4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJkLK0N8Bm-"
      },
      "source": [
        "Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDSYqGeR78qg",
        "outputId": "be084813-e70e-4739-d09c-076328433f57"
      },
      "source": [
        "X_data_list, Y_data_list = input_pipeline(DATA_FILES)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded saved pre-processed data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigiJJno8Hm3"
      },
      "source": [
        "Initialize Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Zd591P8Kvf",
        "outputId": "e00690d7-10f1-4910-b8ac-cc4452638b67"
      },
      "source": [
        "training_model = get_training_model(verbose=True)"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fea3bdfb170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fea3bdfb170> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fea3bdfb830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function recreate_function.<locals>.restored_function_body at 0x7fea3bdfb830> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "word_input (InputLayer)         [(32,)]              0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_tokenizer (KerasLayer)     (32, None, None)     0           word_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_input (InputLayer)         [(32, None)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_input_packer (KerasLayer)  {'input_word_ids': ( 0           bert_tokenizer[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "char_embedding (Embedding)      (32, None, 256)      25856       char_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "BERT_encoder (KerasLayer)       {'pooled_output': (3 13548801    bert_input_packer[0][0]          \n",
            "                                                                 bert_input_packer[0][1]          \n",
            "                                                                 bert_input_packer[0][2]          \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_1 (GRU)                (32, None, 256)      394752      char_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_GRU_1 (GRU)                (32, 5, 32)          27840       BERT_encoder[0][14]              \n",
            "__________________________________________________________________________________________________\n",
            "char_Dropout_1 (Dropout)        (32, None, 256)      0           char_GRU_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "word_Dropout_1 (Dropout)        (32, 5, 32)          0           word_GRU_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_1 (BatchNormali (32, None, 256)      1024        char_Dropout_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_Batch_Norm_1 (BatchNormali (32, 5, 32)          128         word_Dropout_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_final (GRU)            (32, None, 128)      148224      char_Batch_Norm_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "word_Dense_pre_final (Dense)    (32, 5, 5)           165         word_Batch_Norm_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "char_Dropout_final (Dropout)    (32, None, 128)      0           char_GRU_final[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_pooling_final (AveragePool (32, 1, 5)           0           word_Dense_pre_final[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_final (BatchNor (32, None, 128)      512         char_Dropout_final[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "word_Dense_final (Dense)        (32, 1, 128)         768         word_pooling_final[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "merged_layers (Average)         (32, None, 128)      0           char_Batch_Norm_final[0][0]      \n",
            "                                                                 word_Dense_final[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "GRU_OUTPUT (GRU)                (32, None, 256)      296448      merged_layers[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoding (Dense)                (32, None, 101)      25957       GRU_OUTPUT[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 14,470,475\n",
            "Trainable params: 920,842\n",
            "Non-trainable params: 13,549,633\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAjRSZ4WaDD"
      },
      "source": [
        "Load Latest Training Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuhcsgO3WZfK",
        "outputId": "3b39aa2b-a0cb-48fb-c582-bbc9446793ac"
      },
      "source": [
        "try:\r\n",
        "    # load from checkpoint\r\n",
        "    checkpoint, checkpoint_manager = \\\r\n",
        "        create_checkpoint_manager(model=training_model, \r\n",
        "                                    checkpoint_dir=CHECKPOINT_DIR)\r\n",
        "\r\n",
        "    checkpoint_manager.restore_or_initialize()\r\n",
        "    print('loaded checkpoint')\r\n",
        "\r\n",
        "except:\r\n",
        "    print('No matching checkpoints')"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KIYAQj8OZx"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zacay__l8M4T",
        "outputId": "3810ddfc-0486-4db2-eaa4-f9201908f723"
      },
      "source": [
        "num_epochs = 10\r\n",
        "\r\n",
        "training_model = train_model(training_model, X_data_list, Y_data_list,\r\n",
        "                             num_epochs=num_epochs, \r\n",
        "                             num_datasets_to_use=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "shuffled datasets\n",
            "dataset: 0\n",
            "253/306 [=======================>......] - ETA: 1:12 - loss: 2.2304 - sparse_categorical_accuracy: 0.4110 - sparse_categorical_crossentropy: 3.9703"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shv0hEXg8YQ9"
      },
      "source": [
        "Create Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy9kNuIz8XoD"
      },
      "source": [
        "prediction_model = get_prediction_model(trained_model=training_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikV04Jc_85RI"
      },
      "source": [
        "Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpDzr0x81-3"
      },
      "source": [
        "input_text = 'She '\r\n",
        "\r\n",
        "generator(input_text=input_text, \r\n",
        "          prediction_model=prediction_model, \r\n",
        "          precision_reduction=0, \r\n",
        "          num_characters=250, \r\n",
        "          tokenizer=None, \r\n",
        "          max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "          num_words=NUM_TRAILING_WORDS, \r\n",
        "          print_result=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFowEIJgUHN8"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQK0uZ4jUGbf"
      },
      "source": [
        "# training model\r\n",
        "save_model(training_model, model_dir=TRAINING_MODEL_DIR)\r\n",
        "\r\n",
        "# prediction model\r\n",
        "save_model(prediction_model, model_dir=PREDICTION_MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-75YmvdqmG"
      },
      "source": [
        "Anvil Web App Server Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS26o45XdoiI"
      },
      "source": [
        "if USE_ANVIL:\r\n",
        "\r\n",
        "    # get tokenizer\r\n",
        "    tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "    @anvil.server.callable\r\n",
        "    def generate_text(starting_text, precision_reduction,\r\n",
        "                      prediction_tokenizer=tokenizer, \r\n",
        "                      prediction_model=prediction_model,\r\n",
        "                      author='assorted'):\r\n",
        "\r\n",
        "        # set length of generated text\r\n",
        "        num_generation_steps = 350\r\n",
        "    \r\n",
        "        # format user input\r\n",
        "        starting_text = starting_text.upper() + ': '\r\n",
        "\r\n",
        "        prediction = generator(input_text=starting_text, \r\n",
        "                            prediction_model=prediction_model, \r\n",
        "                            precision_reduction=precision_reduction, \r\n",
        "                            num_characters=num_generation_steps, \r\n",
        "                            tokenizer=prediction_tokenizer, \r\n",
        "                            max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "                            num_words=NUM_TRAILING_WORDS, \r\n",
        "                            print_result=True)                             \r\n",
        "        \r\n",
        "        output = starting_text + '\\n'\r\n",
        "        split_on = ['?', '.', ',', ';', '!', ':']\r\n",
        "        splits = '([' + ''.join(split_on) + '])'\r\n",
        "        split_lines_prediction = re.split(splits, prediction)\r\n",
        "        \r\n",
        "        for line in split_lines_prediction:\r\n",
        "            line_update = line[0].upper()\r\n",
        "            try: \r\n",
        "                line_update += line[1:]\r\n",
        "            except:\r\n",
        "                pass\r\n",
        "            if line_update[-1] in split_on or line_update[-2:] == '\\n':\r\n",
        "                output= ''.join([output, line_update])\r\n",
        "            else:\r\n",
        "                output= '\\n'.join([output, line_update])\r\n",
        "        \r\n",
        "        return output + '... '\r\n",
        "\r\n",
        "    # start persistent connection to server\r\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}