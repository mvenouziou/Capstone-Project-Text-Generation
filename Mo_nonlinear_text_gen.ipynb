{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Mo_nonlinear_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpsRQ7irGqpQQQ3qUk3th3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Text-Generation/blob/main/Mo_nonlinear_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQyxxivFD7r"
      },
      "source": [
        "## Text Generation RNN\n",
        "\n",
        "This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. \n",
        "\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\n",
        "- See more at https://github.com/mvenouziou/Project-Text-Generation.\n",
        "\n",
        "- See credits /attributions below\n",
        "\n",
        "The code implements two different model architectures: \"linear\" and \"nonlinear.\"\n",
        "The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. \n",
        "\n",
        "---\n",
        "\n",
        "**What's New?**\n",
        "*(These items are original in the sense that I personally have not seen them at the original time of coding. Citations are below for content I have seen elsewhere.)*\n",
        "\n",
        "Model Architectures:\n",
        "\n",
        "- Experiments with: Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. \n",
        "\n",
        "- Experiments with: Tensorflow Probability layers to create a more interpretable probability distribution model. (Character-model only). The standard text generation algorithm outputs logits, which we view as a distribution from which to generate the next character. Here, we formalize this as outputing our model as a TF Probability Distribution, using probablistic weights in the Dense layer (instead of scalars) and trained via maximum likelihood. \n",
        "\n",
        "- Proper handling of GRU states for multiple stateful layers\n",
        "\n",
        "- Easily switch between model architectures through 'Paramaters' class object. Includes file management for organizing each architecture's checkpoints.\n",
        "\n",
        "\n",
        "Generation:\n",
        "\n",
        "- Add perturbations to learned probabilties in final generation function, to add extra variety to generated text.  (Included in addition to the 'temperature' control described in TF's documentation)\n",
        "\n",
        "Data Processing / Preparation:\n",
        "\n",
        "*These ideas are not original, but I have not seen it implemented in other text generation systems:*\n",
        "\n",
        "- Random crops and with random lengths and start locations. \n",
        "\n",
        "- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzU2QxoMFF7d"
      },
      "source": [
        "---\n",
        "**Credits / Citations / Attributions:**\n",
        "\n",
        "**Linear Model and Shared Code** \n",
        "\n",
        "Other than items noted in previous sections, this python code and linear model structure is based heavily on Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*\n",
        "\n",
        "\n",
        "**Nonlinear Model:**   \n",
        "\n",
        "This utilizes pretrained embeddings:\n",
        "-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" *https://tfhub.dev/google/collections/bert/1)*\n",
        "- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" *https://hub.tensorflow.google.cn/google/electra_small/2)*\n",
        "\n",
        "ELECTRA-Small++ has four times as many paramaters as the Small BERT embedding, producing better results, but at large computational cost.\n",
        "\n",
        "**Web App:** \n",
        "\n",
        "The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "- *'robert_frost_collection.csv'* is a Kaggle dataset available at https://www.kaggle.com/archanghosh/robert-frost-collection. Any other datasets used are public domain works available from Project Gutenberg https://www.gutenberg.org.\n",
        "\n",
        "---\n",
        "\n",
        "**About**\n",
        "\n",
        "Find me online at:\n",
        "- LinkedIn: https://www.linkedin.com/in/movenouziou/ \n",
        "- GitHub: https://github.com/mvenouziou\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "y1EW3w1GvDLx",
        "outputId": "d064295e-1b60-488f-d4d5-b2d55b42a0f4"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# TF text processing (also required for TF HUB word encoders)\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text  \n",
        "\n",
        "# TF TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "# file management\n",
        "import os\n",
        "import bz2\n",
        "import _pickle as cPickle\n",
        "\n",
        "\"\"\" ADDITIONAL IMPORTS:\n",
        "### Imported as needed when initializing 'Paramaters' class object:\n",
        "# TF pretrained models (for word encodings)\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# TF probability modules\n",
        "import tensorflow_probability as tfp  \n",
        "from tensorflow_probability import layers as tfpl\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "# Google Drive integration with Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Anvil Web App Server integration\n",
        "!pip install -q anvil-uplink\n",
        "import anvil.server\n",
        "\"\"\""
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" ADDITIONAL IMPORTS:\\n### Imported as needed when initializing 'Paramaters' class object:\\n# TF pretrained models (for word encodings)\\nimport tensorflow_hub as hub\\n\\n# TF probability modules\\nimport tensorflow_probability as tfp  \\nfrom tensorflow_probability import layers as tfpl\\nfrom tensorflow_probability import distributions as tfd\\n\\n# Google Drive integration with Google Colab\\nfrom google.colab import drive\\n\\n# Anvil Web App Server integration\\n!pip install -q anvil-uplink\\nimport anvil.server\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2682GuzqxyW"
      },
      "source": [
        "### Set Model Paramaters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8uz4beXIwcw"
      },
      "source": [
        "Define Paramaters class.\n",
        "\n",
        "*(Controls the model architecture and the file system used for loading data and checkpointing models.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b90MaNBijOfU"
      },
      "source": [
        "class Paramaters:\n",
        "    def __init__(self,  \n",
        "                 # integrations\n",
        "                use_gdrive, use_anvil,\n",
        "                 # model architecture\n",
        "                use_probability_layers,  # implements TensorFlow Probability\n",
        "                use_word_path,  # note: TFP layers not recommended with word-level model \n",
        "                use_electra, # use False for BERT embeddings (fewer params, word model only)\n",
        "                # datasets\n",
        "                author, data_files, \n",
        "                datasets_dir='https://raw.githubusercontent.com/mvenouziou/text_generator/main/',\n",
        "                # model params\n",
        "                num_trailing_words=5, padded_example_length=300, batch_size=32):\n",
        "        \n",
        "        # save param choices\n",
        "        # note: additional attributes are added below\n",
        "        self._use_gdrive = use_gdrive\n",
        "        self._use_anvil = use_anvil\n",
        "        self._author = author       \n",
        "        self._num_trailing_words = num_trailing_words\n",
        "        self._padded_example_length = padded_example_length\n",
        "        self._batch_size = batch_size\n",
        "        self._use_probability_layers = use_probability_layers\n",
        "        self._use_word_path = use_word_path\n",
        "        self._use_electra = use_electra\n",
        "        self._data_files = list(data_files)\n",
        "        self._datasets_dir = datasets_dir\n",
        "        self._embedding_dim = 32*8\n",
        "        self._char_rnn_units = 512\n",
        "        self._word_rnn_units = 64\n",
        "        self._merge_dim = 32*8\n",
        "\n",
        "        # Additional Imports\n",
        "        # # Tensorflow HUB for using pretrained embeddings (word models)\n",
        "        if self._use_word_path:\n",
        "            import tensorflow_hub as hub\n",
        "\n",
        "        # # Tensorflow Probability (probability distribution model architecture)\n",
        "        if self._use_probability_layers:\n",
        "            import tensorflow_probability as tfp  \n",
        "            from tensorflow_probability import layers as tfpl\n",
        "            from tensorflow_probability import distributions as tfd\n",
        "\n",
        "        # # Google Drive:\n",
        "        if self._use_gdrive:\n",
        "            self._gdrive_dir = '/content/gdrive/'\n",
        "            from google.colab import drive\n",
        "            drive.mount(self._gdrive_dir)\n",
        "        else:\n",
        "            self._gdrive_dir = ''\n",
        "\n",
        "        # # Anvil's web app server\n",
        "        if self._use_anvil:\n",
        "            !pip install -q anvil-uplink\n",
        "            import anvil.server\n",
        "            anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "            self.anvil = anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "\n",
        "        # Filepath Structure\n",
        "        # path name conventions due to model structure\n",
        "        if self._use_probability_layers :\n",
        "            self._author +=  '/probability/' \n",
        "        if self._use_word_path:\n",
        "            self._author += '_words_model/'\n",
        "        if self._use_electra:\n",
        "            self._author += 'electra/'\n",
        "\n",
        "        # models / checkpoints directories\n",
        "        # (Google Drive)\n",
        "        self._filepath = self._gdrive_dir + 'MyDrive/Colab_Notebooks/models/text_generation/' + self._author\n",
        "        self._checkpoint_dir = self._filepath + '/checkpoints/'\n",
        "\n",
        "        ###self._prediction_model_dir = self._filepath + '/prediction_model/'\n",
        "        self._training_model_dir = self._filepath + '/training_model/'\n",
        "        self._processed_data_dir = self._filepath + '/proc_data/'\n",
        "        self._tensorboard_dir = self._checkpoint_dir  + '/logs/'\n",
        "\n",
        "        # Create Tokenizer / Set Vocab Size\n",
        "        # character tokenizer\n",
        "        def create_character_tokenizer():\n",
        "        \n",
        "            char_tokens = string.printable\n",
        "            filters = '#$%&()*+-/<=>@[]^_`{|}~\\t'\n",
        "\n",
        "            # Initialize standard keras tokenizer\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                            num_words=None,  \n",
        "                            filters=filters,\n",
        "                            lower=False,  # conversion to lowercase letters\n",
        "                            char_level=True,\n",
        "                            oov_token=None,  # drop unknown characters\n",
        "                            )      \n",
        "            # fit tokenizer\n",
        "            tokenizer.fit_on_texts(char_tokens)\n",
        "            \n",
        "            return tokenizer\n",
        "\n",
        "        self._character_tokenizer = create_character_tokenizer()\n",
        "        self._vocab_size = len(self._character_tokenizer.word_index) + 1"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A5lFZkNEPVV"
      },
      "source": [
        "# Set Paramaters (global) object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf85QK99q2YH",
        "outputId": "a8d2d056-660b-4062-9aca-f82668268681"
      },
      "source": [
        "# paramater customizationss\n",
        "author='tests'\n",
        "data_files=['robert_frost_collection.csv']\n",
        "use_gdrive=True\n",
        "use_anvil=False\n",
        "use_probability_layers=True\n",
        "use_word_path=False\n",
        "use_electra=False\n",
        "\n",
        "# create paramaters object\n",
        "PARAMATERS = Paramaters(use_gdrive=use_gdrive, use_anvil=use_anvil, \n",
        "                        author=author, data_files=data_files,\n",
        "                        use_probability_layers=use_probability_layers,\n",
        "                        use_word_path=use_word_path, use_electra=use_electra)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39FgKokRXBe1"
      },
      "source": [
        "### Define Encoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl6b4S8zeFdi"
      },
      "source": [
        "Character-Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqQHC1D6Uwp"
      },
      "source": [
        "def make_padded_array(text_blocks, paramaters):\n",
        "    # Tokenizes and applies padding for uniform length\n",
        "\n",
        "    # load tokenizer if one is not supplied\n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "\n",
        "    # tokenize\n",
        "    token_blocks = tokenizer.texts_to_sequences(text_blocks)\n",
        "\n",
        "    # zero padding\n",
        "    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                        sequences=token_blocks,  # dataset\n",
        "                        maxlen=paramaters._padded_example_length, \n",
        "                        dtype='int32', \n",
        "                        padding='pre',\n",
        "                        truncating='pre', \n",
        "                        value=0.0\n",
        "                        )\n",
        "    \n",
        "    return padded_blocks"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqkR0FveIBt"
      },
      "source": [
        "Word-Level (BERT or Electra pre-trained embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC-8wxwtxDD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "7bc7f3ae-18d1-4671-81e1-32802e3d2e05"
      },
      "source": [
        "\"\"\"\n",
        "def get_word_encoder(paramaters):\n",
        "\n",
        "    # Word Embeddings\n",
        "    # Selects file locations for BERT or ELECTRA pretrained encoders\n",
        "    if paramaters._use_electra:\n",
        "        encoder_url = 'https://tfhub.dev/google/electra_small/2'\n",
        "    else:\n",
        "        encoder_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                            + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
        "    preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                        + 'bert_en_uncased_preprocess/3'\n",
        "\n",
        "               \n",
        "    # Get Encoder / Preprocessing layers\n",
        "    preprocessor = hub.load(preprocessor_url)\n",
        "\n",
        "    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize, name='bert_tokenizer')\n",
        "    \n",
        "    bert_packer = hub.KerasLayer(\n",
        "                    preprocessor.bert_pack_inputs,\n",
        "                    arguments=dict(seq_length=paramaters._num_trailing_words),\n",
        "                    name='bert_input_packer')\n",
        "\n",
        "    word_encoder = hub.KerasLayer(encoder_url, trainable=False, \n",
        "                                name='Word_encoder')\n",
        "\n",
        "    \n",
        "    return bert_tokenizer, bert_packer, word_encoder\n",
        "\"\"\""
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ndef get_word_encoder(paramaters):\\n\\n    # Word Embeddings\\n    # Selects file locations for BERT or ELECTRA pretrained encoders\\n    if paramaters._use_electra:\\n        encoder_url = 'https://tfhub.dev/google/electra_small/2'\\n    else:\\n        encoder_url = 'https://tfhub.dev/tensorflow/'                             + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\\n    preprocessor_url = 'https://tfhub.dev/tensorflow/'                         + 'bert_en_uncased_preprocess/3'\\n\\n               \\n    # Get Encoder / Preprocessing layers\\n    preprocessor = hub.load(preprocessor_url)\\n\\n    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize, name='bert_tokenizer')\\n    \\n    bert_packer = hub.KerasLayer(\\n                    preprocessor.bert_pack_inputs,\\n                    arguments=dict(seq_length=paramaters._num_trailing_words),\\n                    name='bert_input_packer')\\n\\n    word_encoder = hub.KerasLayer(encoder_url, trainable=False, \\n                                name='Word_encoder')\\n\\n    \\n    return bert_tokenizer, bert_packer, word_encoder\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlzYCN71orw"
      },
      "source": [
        "### Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqJnhind4a_"
      },
      "source": [
        "Load and Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzwvJaIXQ3q"
      },
      "source": [
        "# Function: loader for .csv files\n",
        "def prepare_csv(filename, paramaters, content_columns=['Title', 'Content']):\n",
        "    \n",
        "    \"\"\" Process CSV files. Text must be in column named 'Content', \n",
        "    (with optional 'Title' column allowed for titles).\"\"\"\n",
        "\n",
        "    # load data into DataFrame\n",
        "    dataframe = pd.read_csv(paramaters._datasets_dir + filename).dropna()\n",
        "    \n",
        "    # extract titles and content\n",
        "    # note: column headings must match those below\n",
        "    # This step is specific to the Robert Frost set\n",
        "    if 'Name ' in dataframe.columns:  \n",
        "        dataframe.rename(columns={'Name ':'Title'})\n",
        "    \n",
        "    # prepare titles\n",
        "    if 'Title' in dataframe.columns:  # add ':\\n'\n",
        "        dataframe['Title'] = dataframe['Title'].apply(lambda x: x + ':\\n')\n",
        "    else:  # no titles found\n",
        "        content_columns = ['Content']\n",
        "\n",
        "    # prepare content\n",
        "    #dataframe['Content'] = dataframe['Content'].apply(lambda x: x + '\\n')\n",
        "    #dataframe = dataframe[content_columns]\n",
        "\n",
        "    # shuffle entries (rows)\n",
        "    dataframe = dataframe.sample(frac=1)\n",
        "    \n",
        "    # merge desired text columns\n",
        "    dataframe['merge'] = dataframe[content_columns[0]]\n",
        "    for i in range(1, len(content_columns)):\n",
        "        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\n",
        "\n",
        "    # convert to list of strings\n",
        "    data_list = dataframe['merge'].tolist()\n",
        "    \n",
        "    return data_list   \n",
        "\n",
        "\n",
        "# Function: Load and standardize data files\n",
        "def load_parse(data_list):  \n",
        "\n",
        "    # remove paragraph / line marks and split up words (outputs bytestrings)\n",
        "    tokenizer = text.WhitespaceTokenizer()\n",
        "    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]\n",
        "\n",
        "    # convert data back to string format\n",
        "    num_entries = len(cleaned_list_byte)\n",
        "    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) \n",
        "                    for i in range(num_entries)]\n",
        "\n",
        "    return clean_list"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOSV4SWyyoS"
      },
      "source": [
        "def input_pipeline(paramaters, fresh_process=False):\n",
        "\n",
        "    # unpack param\n",
        "    saved_proc_dir = paramaters._processed_data_dir\n",
        "    filepath = paramaters._datasets_dir\n",
        "\n",
        "    # load previously processed data if possible\n",
        "    # (pbz2 compressed file format)\n",
        "    try:    \n",
        "        assert(fresh_process is False)  # otherwise create dataset from files\n",
        "\n",
        "        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\n",
        "            data_dict = cPickle.load(file)\n",
        "\n",
        "        clean_list = data_dict['clean_list']\n",
        "        print('loaded saved pre-processed data')\n",
        "\n",
        "    # process data if no saved data found\n",
        "    except:       \n",
        "\n",
        "        # load raw data files from disk\n",
        "        data_list = []\n",
        "        for filename in paramaters._data_files:\n",
        "            print(filename)\n",
        "            print(filepath + '/' + filename)\n",
        "\n",
        "            # select loader (csv or txt)\n",
        "            _, file_extension = os.path.splitext(filename)     \n",
        "\n",
        "            if file_extension == '.csv':   \n",
        "                data = prepare_csv(filename, paramaters=paramaters,\n",
        "                                   content_columns=['Name', 'Content'])\n",
        "            \n",
        "            else: # '.txt':\n",
        "                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:\n",
        "                    data = file.readlines()\n",
        "\n",
        "            # update list of extracted texts\n",
        "            data_list += data\n",
        "\n",
        "        print('PROGRESS: data_list created')\n",
        "        \n",
        "        # clean the data\n",
        "        clean_list = load_parse(data_list)\n",
        "        print('PROGRESS: clean_list created')\n",
        "        \n",
        "        # save data to disk (pbz2 compressed file format)\n",
        "        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\n",
        "            cPickle.dump({'clean_list': clean_list}, sfile)\n",
        "\n",
        "    return clean_list"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wSXBlz5_myo"
      },
      "source": [
        "Dynamic Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkuG9SJn_mQx"
      },
      "source": [
        "def random_text_blocks(full_examples, num_batches, paramaters):\n",
        "\n",
        "    \"\"\" creates random crops of example andn separates them into \n",
        "    input / target pairs for model training \"\"\"\n",
        "\n",
        "    max_length = paramaters._padded_example_length\n",
        "    num_examples = len(full_examples)\n",
        "    num_words = paramaters._num_trailing_words\n",
        "    num_sets = int(num_batches * paramaters._batch_size)\n",
        "\n",
        "    # count total characters\n",
        "    example_starts = [0]\n",
        "    for i in range(num_examples):\n",
        "        example_length = len(full_examples[i])\n",
        "        example_starts.append(example_starts[i] + example_length + 1)\n",
        "\n",
        "    total_chars = example_starts[-1]\n",
        "    example_starts = example_starts[:-1]\n",
        "\n",
        "    # create character blocks\n",
        "    char_blocks = []\n",
        "    word_blocks = []\n",
        "    \n",
        "    completed = False\n",
        "    while not completed:\n",
        "\n",
        "        # choose random starting locations\n",
        "        starting_points = tf.experimental.numpy.random.randint(\n",
        "                                low=0, high=total_chars-2, size=num_sets)\n",
        "\n",
        "        for start in starting_points:\n",
        "\n",
        "            # find containing example\n",
        "            temp = [i for i in range(num_examples) \n",
        "                    if start >= example_starts[i]]\n",
        "           \n",
        "            example_num = temp[-1]\n",
        "            this_example = full_examples[example_num]\n",
        "            this_ex_start = example_starts[example_num]\n",
        "\n",
        "            if example_num < num_examples - 1:\n",
        "                next_ex_start = example_starts[example_num + 1]\n",
        "            else:\n",
        "                next_ex_start = total_chars\n",
        "\n",
        "            # cap length to stay within containing example\n",
        "            length = min(next_ex_start - start, max_length)\n",
        "            cropped_text = this_example[(start - this_ex_start): \n",
        "                                        length + (start - this_ex_start)]\n",
        "\n",
        "            # enforce min character length\n",
        "            if len(cropped_text) < 5:\n",
        "                continue  # skip to next sample\n",
        "\n",
        "            # enforce use of full words\n",
        "            # add dummy start / end chars for easier word splitting\n",
        "            if cropped_text[0] == ' ':\n",
        "                cropped_text = '.' + cropped_text\n",
        "            elif start==this_ex_start:\n",
        "                cropped_text = '. ' + cropped_text\n",
        "\n",
        "            if cropped_text[-1] == ' ':\n",
        "                cropped_text = cropped_text + '.'\n",
        "            elif cropped_text[-1] in string.punctuation:\n",
        "                cropped_text = cropped_text + ' .'\n",
        "            \n",
        "            # split into words & drop incomplete first and last segments\n",
        "            cropped_text = cropped_text.split(' ')\n",
        "\n",
        "            # enforce min words length (2-3 words are dropped in later steps)\n",
        "            if len(cropped_text) < 3:\n",
        "                continue  # skip to next sample\n",
        "\n",
        "            cropped_text = cropped_text[1:-1]  # drop incomplete or dummy start/end\n",
        "\n",
        "            # get trailing words (last word dropped to avoid leaking target data)\n",
        "            trailing_words = cropped_text[-num_words - 1: -1]\n",
        "            \n",
        "            # convert back to string\n",
        "            cropped_text = ' '.join(cropped_text)\n",
        "            trailing_words = ' '.join(trailing_words)\n",
        "\n",
        "            # add cropped text\n",
        "            char_blocks.append(cropped_text)\n",
        "            word_blocks.append(trailing_words)\n",
        "       \n",
        "            if len(char_blocks) >= num_sets:\n",
        "                completed = True\n",
        "                break\n",
        "\n",
        "    return char_blocks, word_blocks"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyNV385EZeAP"
      },
      "source": [
        "def create_random_dataset(clean_list, num_batches, paramaters):\n",
        "\n",
        "    # select data samples by applying random crops\n",
        "    char_blocks, word_blocks  = \\\n",
        "        random_text_blocks(clean_list, num_batches, paramaters) \n",
        "    \n",
        "    # tokenize the char_blocks\n",
        "    char_blocks = make_padded_array(char_blocks, paramaters)\n",
        "\n",
        "    # split inputs / targets\n",
        "    char_input = char_blocks[:, :-1]\n",
        "    target = char_blocks[:, 1:]\n",
        "\n",
        "    # match datatypes and shapes\n",
        "    num_samples = char_blocks.shape[0]\n",
        "    word_input = np.array(word_blocks)\n",
        "\n",
        "    word_input = tf.constant(word_input, dtype=tf.string)\n",
        "    char_input = tf.constant(char_input, dtype=tf.int32)\n",
        "    target = tf.constant(target, dtype=tf.int32)\n",
        "    \n",
        "    # convert to dataset\n",
        "    ds = tf.data.Dataset.from_tensor_slices((\n",
        "        (char_input, word_input), target))\n",
        "    \n",
        "    ds = ds.batch(paramaters._batch_size)\\\n",
        "      .shuffle(5000)\\\n",
        "      .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
        "    \n",
        "    return ds\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpjX19aWvU6"
      },
      "source": [
        "### Define Models and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFPy3FIO59y7"
      },
      "source": [
        "Custom loss for (probability models)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLj9Q1VOfG6"
      },
      "source": [
        "#VOCAB_SIZE = len(create_character_tokenizer().word_index) + 1\n",
        "\n",
        "def neg_log_likely_logits(y_true, y_pred, depth):\n",
        "    \"\"\" loss function for probabalistic model \"\"\"\n",
        "\n",
        "    # encode labels as one-hot vectors\n",
        "    y_true_hot = tf.one_hot(y_true, depth=depth, axis=-1)\n",
        "\n",
        "    # return negative log likelihood\n",
        "    return -y_pred.log_prob(y_true_hot)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQC-Ahru83WN"
      },
      "source": [
        "def loss(model, y_pred, target):\n",
        "\n",
        "    # compute loss\n",
        "    if model.paramaters._use_probability_layers:\n",
        "        depth = model.paramaters._vocab_size\n",
        "        loss = neg_log_likely_logits(target, y_pred, depth)\n",
        "\n",
        "    else:\n",
        "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)(target, y_pred)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pr17EAJ8FPP"
      },
      "source": [
        "@tf.function\n",
        "def grad(model, inputs, states, target, stateful):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "\n",
        "        # compute predictions\n",
        "        y_pred, states = model(inputs, initial_states=states, stateful=stateful)\n",
        "        \n",
        "        # compute loss\n",
        "        loss_value = loss(model, y_pred, target)\n",
        "    \n",
        "    # get gradients\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    \n",
        "    return loss_value, grads, states"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6KD4XoYoQG"
      },
      "source": [
        "Checkpoint Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K09LDfJYnla"
      },
      "source": [
        "# checkpoint manager\n",
        "def create_checkpoint_manager(model):\n",
        "\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\n",
        "\n",
        "    checkpoint_manager = tf.train.CheckpointManager(\n",
        "                            checkpoint=checkpoint, \n",
        "                            directory=model.paramaters._checkpoint_dir, \n",
        "                            max_to_keep=4, \n",
        "                            keep_checkpoint_every_n_hours=None,\n",
        "                            checkpoint_name='ckpt', \n",
        "                            step_counter=None, \n",
        "                            checkpoint_interval=None,\n",
        "                            init_fn=None\n",
        "                            )\n",
        "    \n",
        "    model.checkpoint_manager = checkpoint_manager\n",
        "    model.checkpoint = checkpoint\n",
        "    \n",
        "    return None"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZbLYkrN2ILz"
      },
      "source": [
        "### Define Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGhSLvnK2Fl-"
      },
      "source": [
        "# Model Class\n",
        "class GenerationModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, paramaters, **kwargs):\n",
        "        super().__init__(self, **kwargs)\n",
        "       \n",
        "        # save attributes\n",
        "        self.batch_size = paramaters._batch_size\n",
        "        self.checkpoint_manager = None\n",
        "        self.checkpoint = None\n",
        "        self.paramaters = paramaters\n",
        "        self.gru_layer_names = ['char_GRU_1', 'char_GRU_2', \n",
        "                                'word_GRU_1', 'word_GRU_2', 'GRU_Output']\n",
        "\n",
        "        # imports - Keras\n",
        "        from keras.layers import Input, Embedding, Concatenate, \\\n",
        "                            Dense, GRU,Average, AveragePooling1D, \\\n",
        "                            Dropout, BatchNormalization, Lambda, Concatenate\n",
        "        \n",
        "        # imports Tensorflow Probability\n",
        "        if self.paramaters._use_probability_layers:\n",
        "            import tensorflow_probability as tfp  \n",
        "            from tensorflow_probability import layers as tfpl\n",
        "            from tensorflow_probability import distributions as tfd\n",
        "\n",
        "        # # Tensorflow HUB for using pretrained embeddings (word models)\n",
        "        if self.paramaters._use_word_path:\n",
        "            import tensorflow_hub as hub\n",
        "\n",
        "        # set model params\n",
        "        use_word_path = paramaters._use_word_path\n",
        "        use_probability_layers = paramaters._use_probability_layers\n",
        "        num_words = paramaters._num_trailing_words\n",
        "        vocab_size = paramaters._vocab_size\n",
        "        embedding_dim = paramaters._embedding_dim\n",
        "        char_rnn_units = paramaters._char_rnn_units\n",
        "        word_rnn_units = paramaters._word_rnn_units\n",
        "        merge_dim = paramaters._merge_dim\n",
        "\n",
        "        \n",
        "        # Encoder layers\n",
        "        if use_word_path:\n",
        "            self.bert_tokenizer, self.bert_packer, self.bert_encoder = \\\n",
        "                        self.get_word_encoder()\n",
        "                        \n",
        "        self.char_embedding = Embedding(input_dim=vocab_size, \n",
        "                                        output_dim=embedding_dim, \n",
        "                                        mask_zero=True,\n",
        "                                        name='char_embedding')\n",
        "        \n",
        "        # ## Character Path Layers\n",
        "        self.char_GRU_1 = GRU(units=char_rnn_units, return_state=True, \n",
        "                              return_sequences=True, name='char_GRU_1')\n",
        "        self.char_Batch_Norm_1 = BatchNormalization(name='char_Batch_Norm_1')\n",
        "        self.char_GRU_2 = GRU(units=char_rnn_units, return_state=True, \n",
        "                                  return_sequences=True, name='char_GRU_2')\n",
        "        self.char_Batch_Norm_2 = BatchNormalization(name='char_Batch_Norm_2')\n",
        "\n",
        "        # Word Encoding Path Layers\n",
        "        if use_word_path:\n",
        "            self.word_GRU_1 = \\\n",
        "                GRU(units=word_rnn_units, return_state=True, \n",
        "                    return_sequences=True, name='word_GRU_1',)\n",
        "            self.word_Batch_Norm_1 = BatchNormalization(name='word_Batch_Norm_1')\n",
        "            self.word_GRU_2 = \\\n",
        "                GRU(units=word_rnn_units, return_state=True, \n",
        "                    return_sequences=False, name='word_GRU_2',)\n",
        "            self.word_Batch_Norm_2 = BatchNormalization(name='word_Batch_Norm_2')\n",
        "\n",
        "        # Merge Layers\n",
        "        if use_word_path:\n",
        "            self.char_Dense_merge = \\\n",
        "                Dense(units=merge_dim, activation=None, name='char_Dense_merge')\n",
        "            self.word_Dense_merge = \\\n",
        "                Dense(units=merge_dim, activation=None, name='word_Dense_merge')\n",
        "            self.word_reshape = \\\n",
        "                Lambda(lambda x: tf.expand_dims(x, axis=1), name='word_reshape')\n",
        "            self.merged_layers = \\\n",
        "                Lambda(lambda x: tf.concat([x[0][:,1:,:], # drop first value so shape is preserved after concat\n",
        "                                            x[1]], axis=1), name='merged_layers')\n",
        "        \n",
        "        else:\n",
        "            self.rename = Lambda(lambda x: x, name='rename_variable')\n",
        "        \n",
        "        self.Batch_Norm_output = BatchNormalization(name='Batch_Norm_output')\n",
        "\n",
        "        # Character prediction (logits)\n",
        "        if use_probability_layers:\n",
        "            # Dense layer with probabalistic weights\n",
        "            self.dense_reparam = tfpl.DenseReparameterization(\n",
        "                    units=tfpl.OneHotCategorical.params_size(vocab_size),\n",
        "                    activation=None)                     \n",
        "            self.dist_outputs = tfpl.OneHotCategorical(\n",
        "                        vocab_size,\n",
        "                        convert_to_tensor_fn=tfd.OneHotCategorical.logits,\n",
        "                        name='Decoding')\n",
        "        else:\n",
        "            self.dense_outputs = \\\n",
        "                Dense(units=vocab_size, activation=None, name='Decoding')      \n",
        "\n",
        "    def call(self, inputs, initial_states=None, stateful=False, **kwargs):\n",
        "       \n",
        "        # set model params\n",
        "        use_word_path = self.paramaters._use_word_path\n",
        "        use_probability_layers = self.paramaters._use_probability_layers\n",
        "\n",
        "        # initialize states as TensorArray \n",
        "        # (This is similar to dictionary but works with @tf.function)\n",
        "        if stateful:\n",
        "            states = tf.TensorArray(tf.float32, size=len(self.gru_layer_names),\n",
        "                                        clear_after_read=True)\n",
        "            states.trainable = False\n",
        "\n",
        "            if initial_states is None:\n",
        "                indx_state = None\n",
        "\n",
        "            else:  # unpack values into 'states' array\n",
        "                states_list = tf.unstack(initial_states)\n",
        "                for indx in range(len(self.gru_layer_names)):\n",
        "                    states = states.write(indx, states_list[indx])         \n",
        "\n",
        "        else:\n",
        "            initial_states = None\n",
        "            indx_state = None\n",
        "\n",
        "        # inputs\n",
        "        input_1 = inputs[0]\n",
        "        input_2 = inputs[1]\n",
        "\n",
        "        # ## Character Path Layers\n",
        "        x1 = self.char_embedding(input_1)\n",
        "        \n",
        "        # Char GRU 1\n",
        "        if stateful:\n",
        "            indx = self.gru_layer_names.index('char_GRU_1')\n",
        "            if initial_states is not None:\n",
        "                indx_state = states.read(indx)\n",
        "            x1, new_indx_state = self.char_GRU_1(x1, initial_state=indx_state)\n",
        "            states = states.write(indx, new_indx_state)\n",
        "        else:\n",
        "            x1, _ = self.char_GRU_1(x1, initial_state=None)\n",
        "        x1 = self.char_Batch_Norm_1(x1)\n",
        "\n",
        "        # Char GRU 2\n",
        "        if stateful:\n",
        "            indx = self.gru_layer_names.index('char_GRU_2')\n",
        "            if initial_states is not None:\n",
        "                indx_state = states.read(indx)\n",
        "            x1, new_indx_state = self.char_GRU_2(x1, initial_state=indx_state)\n",
        "            states = states.write(indx, new_indx_state)\n",
        "        else:\n",
        "            x1, _ = self.char_GRU_2(x1, initial_state=None)\n",
        "        x1 = self.char_Batch_Norm_2(x1)\n",
        "\n",
        "        # Word Encoding Path Layers\n",
        "        if use_word_path:\n",
        "            # encoding\n",
        "            x2 = self.bert_tokenizer(input_2)  # tokenize\n",
        "            x2 = self.bert_packer([x2])  # pack inputs for encoder\n",
        "            x2 = self.bert_encoder(x2)['sequence_output'] # encoding\n",
        "            \n",
        "            # Word GRU 1\n",
        "            if stateful:\n",
        "                indx = self.gru_layer_names.index('word_GRU_1')\n",
        "                if initial_states is not None:\n",
        "                    indx_state = states.read(indx)\n",
        "                x2, new_indx_state = self.word_GRU_1(x2, initial_state=indx_state)\n",
        "                states = states.write(indx, new_indx_state)\n",
        "            else:\n",
        "                x2, _ = self.word_GRU_1(x2, initial_state=None)\n",
        "            x2 = self.word_Batch_Norm_1(x2)\n",
        "\n",
        "            # Word GRU 2\n",
        "            if stateful:\n",
        "                indx = self.gru_layer_names.index('word_GRU_2')\n",
        "                if initial_states is not None:\n",
        "                    indx_state = states.read(indx)\n",
        "                x2, new_indx_state = self.word_GRU_2(x2, initial_state=indx_state)\n",
        "                states = states.write(indx, new_indx_state)\n",
        "            else:\n",
        "                x2, _ = self.word_GRU_2(x2, initial_state=None)\n",
        "            x2 = self.word_Batch_Norm_2(x2)\n",
        "\n",
        "        # Merge Layers\n",
        "        if use_word_path:\n",
        "            x1 = self.char_Dense_merge(x1)\n",
        "            x2 = self.word_Dense_merge(x2)\n",
        "            x2 = self.word_reshape(x2)\n",
        "            x = self.merged_layers((x1, x2))\n",
        "            \n",
        "        else:  # update variable id to match next step\n",
        "            x = self.rename(x1)\n",
        "        \n",
        "        x = self.Batch_Norm_output(x)\n",
        "\n",
        "        # Character prediction (logits)\n",
        "        if use_probability_layers:\n",
        "            # Dense layer with probabalistic weights\n",
        "            x = self.dense_reparam(x)\n",
        "            y_pred = self.dist_outputs(x)\n",
        "        \n",
        "        else:\n",
        "            y_pred = self.dense_outputs(x)\n",
        "\n",
        "        if stateful:\n",
        "            new_states = states.stack()\n",
        "            states.close()\n",
        "        else:\n",
        "            new_states = tf.constant(1)  # dummy entry to avoid error in model.save()\n",
        "\n",
        "        return y_pred, new_states\n",
        "\n",
        "    \n",
        "    def get_word_encoder(self):\n",
        "        import tensorflow_hub as hub\n",
        "\n",
        "        # Word Embeddings\n",
        "        # Selects file locations for BERT or ELECTRA pretrained encoders\n",
        "        if self.paramaters._use_electra:\n",
        "            encoder_url = 'https://tfhub.dev/google/electra_small/2'\n",
        "        else:\n",
        "            encoder_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                                + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
        "        preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                            + 'bert_en_uncased_preprocess/3'\n",
        "\n",
        "                \n",
        "        # Get Encoder / Preprocessing layers\n",
        "        preprocessor = hub.load(preprocessor_url)\n",
        "\n",
        "        bert_tokenizer = hub.KerasLayer(preprocessor.tokenize, name='bert_tokenizer')\n",
        "        \n",
        "        bert_packer = hub.KerasLayer(\n",
        "                        preprocessor.bert_pack_inputs,\n",
        "                        arguments=dict(seq_length=self.paramaters._num_trailing_words),\n",
        "                        name='bert_input_packer')\n",
        "\n",
        "        word_encoder = hub.KerasLayer(encoder_url, trainable=False, \n",
        "                                    name='Word_encoder')\n",
        "\n",
        "        \n",
        "        return bert_tokenizer, bert_packer, word_encoder"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWkTKUaXZbm"
      },
      "source": [
        "Custom Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZWbiojXXpS"
      },
      "source": [
        "# Function: Train model\n",
        "def train_model(model, cleaned_data_list, num_epochs, \n",
        "                batches_per_epoch, learning_rate):\n",
        "\n",
        "    # get params\n",
        "    batch_size = model.batch_size\n",
        "    paramaters = model.paramaters\n",
        "    stateful = False\n",
        "    \n",
        "    # get optimizer\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    \n",
        "    # set checkpoint manager\n",
        "    if model.checkpoint is None or model.checkpoint_manager is None:\n",
        "        create_checkpoint_manager(model=model)\n",
        "\n",
        "    # initialize containers for metrics\n",
        "    train_loss_results = []\n",
        "    \n",
        "    \"\"\"\n",
        "    # set callbacks\n",
        "    # TensorBoard callback\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "                                log_dir=paramaters._tensorboard_dir, \n",
        "                                histogram_freq=1,\n",
        "                                )\n",
        "    \"\"\"\n",
        "    \n",
        "    # begin training loop\n",
        "    # prepare datasets\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        print(f'Epoch: {epoch}')\n",
        "\n",
        "        # initialize epoch metrics\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        # create dataset from randomized text crops\n",
        "        num_batches = max(batches_per_epoch, 1)\n",
        "        dataset = create_random_dataset(cleaned_data_list, num_batches, \n",
        "                                        paramaters)\n",
        "        \n",
        "        # train model\n",
        "        iteration = 0\n",
        "        for (inputs, targets) in dataset:\n",
        "\n",
        "            # get grads and loss\n",
        "            if stateful is False:  # ensure no states passed in to model\n",
        "                states = None\n",
        "            loss_value, grads, states = \\\n",
        "                grad(model, inputs, states, targets, stateful=stateful)\n",
        "            \n",
        "            # update weights\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            # update metrics\n",
        "            epoch_loss_avg(loss_value)\n",
        "\n",
        "            # save checkpoint\n",
        "            if iteration % 5 == 0:\n",
        "                model.checkpoint_manager.save()\n",
        "                \n",
        "                ave_loss = epoch_loss_avg.result().numpy()\n",
        "                print(f'  - loss: {np.format_float_positional(ave_loss, 4)}')\n",
        "            \n",
        "            iteration += 1\n",
        "\n",
        "        # End of Epoch\n",
        "        # save checkpoint\n",
        "        model.checkpoint_manager.save()\n",
        "\n",
        "        # record epoch metrics\n",
        "        ave_loss = epoch_loss_avg.result().numpy()\n",
        "        train_loss_results.append(ave_loss)\n",
        "        mean_loss = np.mean(train_loss_results)\n",
        "        \n",
        "        # report epoch summary\n",
        "        print(f'epoch ave loss: {np.format_float_positional(ave_loss, 4)}')\n",
        "        print(f'overall ave loss: {np.format_float_positional(mean_loss, 4)}')\n",
        "\n",
        "        # show sample result\n",
        "        if epoch % 3 == 0:\n",
        "            print('\\n')\n",
        "            generate_text(starting_text='Sample Output', \n",
        "                          prediction_model=model,\n",
        "                          precision_reduction=0,\n",
        "                          temperature=1,\n",
        "                          print_result=True,\n",
        "                          paramaters=model.paramaters,\n",
        "                          num_generation_steps=40)\n",
        "\n",
        "    # end of training    \n",
        "    # consolidate saved metrics\n",
        "    history = [train_loss_results]\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rFetpOUqwQ"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tf4x1q8UpfD"
      },
      "source": [
        "# Store trained model separate from checkpoints\n",
        "def save_model(model, paramaters):\n",
        "\n",
        "    model_dir = paramaters._training_model_dir\n",
        "    \n",
        "    # save model\n",
        "    tf.saved_model.save(model, model_dir)\n",
        "\n",
        "    return None"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMNy3XTW2Be"
      },
      "source": [
        "### Text Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuUcE7H4snz"
      },
      "source": [
        "def convert_to_input(last_token, text_string, paramaters):\n",
        "        \n",
        "    # words\n",
        "    if paramaters._use_word_path:\n",
        "        num_words = paramaters._num_trailing_words\n",
        "\n",
        "        words_input = text_string.split(' ')  # separate words \n",
        "        words_input = words_input[-num_words-1: -1]  # get trailing words\n",
        "        words_input = tf.constant(' '.join(words_input))  # convert to tensor\n",
        "        \n",
        "    else:\n",
        "        words_input=tf.constant('_ ')\n",
        "    words_input = tf.reshape(words_input, shape=(1,-1))\n",
        "\n",
        "    # pad token sequence\n",
        "    inputs_char = tf.constant(last_token)\n",
        "    inputs_char = tf.reshape(inputs_char, shape=(1,-1))\n",
        "\n",
        "    # dummy output for batch\n",
        "\n",
        "    # create separate input / target pairs for each block\n",
        "    ds = tf.data.Dataset.from_tensor_slices((inputs_char, words_input, ()))\n",
        "    \n",
        "    # set batch shape for model input\n",
        "    ds = ds.batch(1)\n",
        "\n",
        "    return ds"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2atvRiGv-z"
      },
      "source": [
        "def generator(input_text, prediction_model, precision_reduction, temperature,\n",
        "              num_characters, print_result, paramaters):\n",
        "\n",
        "    # get tokenizer (if not supplied)      \n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "    \n",
        "    # initialize generated text\n",
        "    last_token =  tokenizer.texts_to_sequences([input_text])\n",
        "    output_text = input_text.upper() + '\\n'\n",
        "    generated_text = list(output_text)\n",
        "    \n",
        "    # set initial GRU state values\n",
        "    initial_states = None\n",
        "\n",
        "    # text generation loop\n",
        "    for _ in range(num_characters):\n",
        "        \n",
        "        # prepare input for model\n",
        "        inputs = convert_to_input(last_token=last_token, \n",
        "                                  text_string=output_text,\n",
        "                                  paramaters=paramaters)\n",
        "                   \n",
        "        # run model and get logits of last character prediction\n",
        "        for input in inputs.take(1):\n",
        "            logits, initial_states = \\\n",
        "                prediction_model(input, initial_states=initial_states, \n",
        "                                 stateful=True)\n",
        "        \n",
        "        # extract last character's logits\n",
        "        logits = logits[0, -1, :].numpy() \n",
        "        \n",
        "        # 'temperature' control to distort probabilities\n",
        "        if temperature != 1:\n",
        "            logits = logits / temperature\n",
        "\n",
        "        # perturb probabilities before selecting token\n",
        "        if precision_reduction != 0:   \n",
        "            \n",
        "            # add noise to perturb logits\n",
        "            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n",
        "            logits = logits * (1 + precision_reduction * fuzz_factor)\n",
        "\n",
        "        # Choose a token using the logits probability distribution\n",
        "        last_token = tf.random.categorical(logits=[logits], num_samples=1)       \n",
        "        last_token = last_token.numpy().tolist()\n",
        "\n",
        "        # correct for any invalid character token choices\n",
        "        if last_token==[[0]]:  \n",
        "            last_token==[[1]]\n",
        "    \n",
        "        # get text value of predicted character\n",
        "        input_text = tokenizer.sequences_to_texts(last_token)\n",
        "        input_text = input_text[0]\n",
        "\n",
        "        # record generated character\n",
        "        generated_text.append(input_text)\n",
        "        output_text = ''.join(generated_text)\n",
        "        \n",
        "    if print_result:\n",
        "        print(output_text)\n",
        "    \n",
        "    return output_text"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiLYUGJ4uvk"
      },
      "source": [
        "Final generation function for end user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCvZ7x8D4rz1"
      },
      "source": [
        "def generate_text(starting_text, \n",
        "                  prediction_model,\n",
        "                  print_result,\n",
        "                  paramaters=PARAMATERS,\n",
        "                  precision_reduction=0.,\n",
        "                  temperature=1,\n",
        "                  num_generation_steps=150): # set length of generated text\n",
        "\n",
        "    # format user input\n",
        "    starting_text = starting_text.upper() + ': '\n",
        "\n",
        "    # get generated text\n",
        "    # note: very rarely this produces a line indexing error. \n",
        "    # This while loop reruns prediction if needed\n",
        "\n",
        "    prediction = generator(input_text=starting_text, \n",
        "                                prediction_model=prediction_model, \n",
        "                                precision_reduction=precision_reduction, \n",
        "                                temperature=temperature,\n",
        "                                num_characters=num_generation_steps, \n",
        "                                print_result=print_result,\n",
        "                                paramaters=paramaters)\n",
        "                                    \n",
        "   \n",
        "    # define formatting rules\n",
        "    split_on = ['?', '.', ',', ';', '!', ':']\n",
        "    splits = '([' + ''.join(split_on) + '])'\n",
        "    split_lines_prediction = re.split(splits, prediction)\n",
        "\n",
        "    # format output\n",
        "    output = ''\n",
        "    for line in split_lines_prediction:\n",
        "\n",
        "        # capitalize first word of each line   \n",
        "        if len(line) >= 1:\n",
        "            line_update = line[0].upper()  \n",
        "\n",
        "            # add capitalized letter to remainder of line\n",
        "            if len(line) >= 2:\n",
        "                line_update += line[1:]\n",
        "        else:\n",
        "            line_update = ''\n",
        "                \n",
        "        # update output text\n",
        "        if (len(line_update) >= 1 and line_update[-1] in split_on) \\\n",
        "          or (len(line_update) >= 2 and line_update[-2:] == '\\n'):\n",
        "                output = ''.join([output, line_update])\n",
        "        else:\n",
        "            output = '\\n'.join([output, line_update])\n",
        "\n",
        "    \n",
        "    return output + '... '"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO27cbff77C4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJkLK0N8Bm-"
      },
      "source": [
        "Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDSYqGeR78qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a3e7ef-d18d-4beb-d8bc-24e1ee04911f"
      },
      "source": [
        "require_fresh_process = False\n",
        "\n",
        "try:\n",
        "    # check if prepared datasets already in memory\n",
        "    assert(require_fresh_process is False)\n",
        "    assert(len(X_data_list) > 0)\n",
        "    print('dataset already loaded')\n",
        "\n",
        "except:\n",
        "    print('Preparing dataset')\n",
        "    cleaned_data_list = input_pipeline(paramaters=PARAMATERS, \n",
        "                                              fresh_process=require_fresh_process)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset\n",
            "loaded saved pre-processed data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigiJJno8Hm3"
      },
      "source": [
        "Initialize Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Zd591P8Kvf"
      },
      "source": [
        "model = GenerationModel(paramaters=PARAMATERS)\n",
        "create_checkpoint_manager(model)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAjRSZ4WaDD"
      },
      "source": [
        "Load Latest Training Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhcsgO3WZfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dae5fef-ce5a-47a3-8857-f665f91df169"
      },
      "source": [
        "load_checkpoint=True\n",
        "\n",
        "# run an element through model to buil it\n",
        "# and match checkpointed model\n",
        "temp_ds  = create_random_dataset(cleaned_data_list, 1, PARAMATERS)\n",
        "for inp in temp_ds:\n",
        "    model(inp[0], inp[1])\n",
        "\n",
        "try:\n",
        "    assert(load_checkpoint is True)\n",
        "\n",
        "    # load checkpoint\n",
        "    model.checkpoint_manager.restore_or_initialize()\n",
        "    print('loaded checkpoint')\n",
        "\n",
        "except: \n",
        "    print('No matching checkpoints')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded checkpoint\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KIYAQj8OZx"
      },
      "source": [
        "Train Model\n",
        "\n",
        "*Caution: overfitting can be a major problem where the model can eventually memorize and return complete segments from the source material. 'Precision reduction' and 'temperature' controls are included in the generation function to partially compensate by altering learned probabilities*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacay__l8M4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3c8a4a3-7717-4ab4-fdba-7eb511b16101"
      },
      "source": [
        "train_model_now = True\n",
        "\n",
        "learning_rate = 0.01\n",
        "num_epochs = 30\n",
        "batches_per_epoch = 30\n",
        "\n",
        "# train model\n",
        "if train_model_now:\n",
        "    model, history = train_model(model, cleaned_data_list,\n",
        "                                num_epochs=num_epochs, \n",
        "                                learning_rate=learning_rate,\n",
        "                                batches_per_epoch=batches_per_epoch)  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFowEIJgUHN8"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQK0uZ4jUGbf"
      },
      "source": [
        "save_model_now = True\n",
        "\n",
        "if save_model_now:\n",
        "    save_model(model, paramaters=model.paramaters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikV04Jc_85RI"
      },
      "source": [
        "## Test Output: Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpDzr0x81-3"
      },
      "source": [
        "starting_text = 'The road less'#AI is becoming accessible'\n",
        "precision_reduction = 0 \n",
        "temperature=1,\n",
        "\n",
        "gen = generate_text(starting_text=starting_text, \n",
        "                    prediction_model=model,\n",
        "                    precision_reduction=precision_reduction,\n",
        "                    temperature=temperature,\n",
        "                    print_result=True,\n",
        "                    paramaters=PARAMATERS,\n",
        "                    num_generation_steps=250)\n",
        "\n",
        "print(gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13TslMTcCo_"
      },
      "source": [
        "## Web App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-75YmvdqmG"
      },
      "source": [
        "Anvil Web App Server Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS26o45XdoiI"
      },
      "source": [
        "if PARAMATERS._use_anvil:\n",
        "\n",
        "    # enable connection\n",
        "    self.anvil \n",
        "\n",
        "    # get tokenizer\n",
        "    tokenizer = create_character_tokenizer()\n",
        "\n",
        "    @anvil.server.callable\n",
        "    def anvil_callable(starting_text, \n",
        "                       precision_reduction=0,\n",
        "                       temperature=1,\n",
        "                        paramaters=PARAMATERS,\n",
        "                        prediction_tokenizer=PARAMATERS._character_tokenizer, ####### not used -- remove\n",
        "                        prediction_model=prediction_model,\n",
        "                        print_result=True,\n",
        "                        author='assorted',\n",
        "                        num_generation_steps=150):\n",
        "\n",
        "        return generate_text(starting_text=starting_text, \n",
        "                             precision_reduction=precision_reduction,\n",
        "                             temperature=temperature,\n",
        "                             prediction_model,\n",
        "                             print_result=print_result,\n",
        "                             paramaters=paramaters,\n",
        "                             num_generation_steps=num_generation_steps)\n",
        "\n",
        "    # start persistent connection to server\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}