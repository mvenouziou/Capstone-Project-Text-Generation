# -*- coding: utf-8 -*-
"""Copy of Mo_nonlinear_text_gen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkLbr2-8KuG4TTy2PCbgoLTWcv5qzR2C

## Text Generation RNN

This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. 

- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)
- See more at https://github.com/mvenouziou/Project-Text-Generation.

- See credits /attributions below

The code implements two different model architectures: "linear" and "nonlinear."
The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. 

---

**What's New?**
*(These items are original in the sense that I personally have not seen them at the original time of coding. Citations are below for content I have seen elsewhere.)*

Model Architectures:

- Experiments with: Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. 

- Experiments with: Tensorflow Probability layers to create a more interpretable probability distribution model. (Character-model only). The standard text generation algorithm outputs logits, which we view as a distribution from which to generate the next character. Here, we formalize this as outputing our model as a TF Probability Distribution, using probablistic weights in the Dense layer (instead of scalars) and trained via maximum likelihood. 

- Proper handling of GRU states for multiple stateful layers

- Easily switch between model architectures through 'Paramaters' class object. Includes file management for organizing each architecture's checkpoints.


Generation:

- Add perturbations to learned probabilties in final generation function, to add extra variety to generated text.  (Included in addition to the 'temperature' control described in TF's documentation)

Data Processing / Preparation:

*These ideas are not original, but I have not seen it implemented in other text generation systems:*

- Random crops and with random lengths and start locations. 

- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.)

---
**Credits / Citations / Attributions:**

**Linear Model and Shared Code** 

Other than items noted in previous sections, this python code and linear model structure is based heavily on Imperial College London's Coursera course, "Customising your models with Tensorflow 2" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*


**Nonlinear Model:**   

This utilizes pretrained embeddings:
-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper "Well-Read Students Learn Better: On the Importance of Pre-training Compact Models." *https://tfhub.dev/google/collections/bert/1)*
- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators." *https://hub.tensorflow.google.cn/google/electra_small/2)*

ELECTRA-Small++ has four times as many paramaters as the Small BERT embedding, producing better results, but at large computational cost.

**Web App:** 

The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).

**Datasets:**

- *'robert_frost_collection.csv'* is a Kaggle dataset available at https://www.kaggle.com/archanghosh/robert-frost-collection. Any other datasets used are public domain works available from Project Gutenberg https://www.gutenberg.org.

---

**About**

Find me online at:
- LinkedIn: https://www.linkedin.com/in/movenouziou/ 
- GitHub: https://github.com/mvenouziou

---
"""

# Commented out IPython magic to ensure Python compatibility.
#### PACKAGE IMPORTS ####

# TF Model design
import tensorflow as tf
from tensorflow import keras

# TF text processing (also required for TF HUB word encoders)
!pip install -q tensorflow-text
import tensorflow_text as text  

# TF TensorBoard notebook extension
# %load_ext tensorboard
import datetime, os

# data management
import numpy as np
import pandas as pd
import string
import random
import re

# file management
import os
import bz2
import _pickle as cPickle

""" ADDITIONAL IMPORTS:
### Imported as needed when initializing 'Paramaters' class object:
# TF pretrained models (for word encodings)
import tensorflow_hub as hub

# TF probability modules
import tensorflow_probability as tfp  
from tensorflow_probability import layers as tfpl
from tensorflow_probability import distributions as tfd

# Google Drive integration with Google Colab)
from google.colab import drive

# Anvil Web App Server integration
!pip install -q anvil-uplink
import anvil.server
"""

"""### Set Model Paramaters"""

# paramater customizationss
author='tests'
data_files=['robert_frost_collection.csv']
use_gdrive=True
use_anvil=False
use_probability_layers=True
use_word_path=False
use_electra=False

# create paramaters object
PARAMATERS = Paramaters(use_gdrive=use_gdrive, use_anvil=use_anvil, 
                        author=author, data_files=data_files,
                        use_probability_layers=use_probability_layers,
                        use_word_path=use_word_path, use_electra=use_electra)

"""### Input Pipeline
Data prep prior to training loop

Load and Clean Datasets
"""

def make_padded_array(text_blocks, paramaters):
    # Tokenizes and applies padding for uniform length

    # load tokenizer if one is not supplied
    tokenizer = paramaters._character_tokenizer

    # tokenize
    token_blocks = tokenizer.texts_to_sequences(text_blocks)

    # zero padding
    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(
                        sequences=token_blocks,  # dataset
                        maxlen=paramaters._padded_example_length, 
                        dtype='int32', 
                        padding='pre',
                        truncating='pre', 
                        value=0.0
                        )
    
    return padded_blocks

# Function: loader for .csv files
def prepare_csv(filename, paramaters, content_columns=['Title', 'Content']):
    
    """ Process CSV files. Text must be in column named 'Content', 
    (with optional 'Title' column allowed for titles)."""

    # load data into DataFrame
    dataframe = pd.read_csv(paramaters._datasets_dir + filename).dropna()
    
    # extract titles and content
    # note: column headings must match those below
    # This step is specific to the Robert Frost set
    if 'Name ' in dataframe.columns:  
        dataframe.rename(columns={'Name ':'Title'})
    
    # prepare titles
    if 'Title' in dataframe.columns:  # add ':\n'
        dataframe['Title'] = dataframe['Title'].apply(lambda x: x + ':\n')
    else:  # no titles found
        content_columns = ['Content']

    # prepare content
    #dataframe['Content'] = dataframe['Content'].apply(lambda x: x + '\n')
    #dataframe = dataframe[content_columns]

    # shuffle entries (rows)
    dataframe = dataframe.sample(frac=1)
    
    # merge desired text columns
    dataframe['merge'] = dataframe[content_columns[0]]
    for i in range(1, len(content_columns)):
        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]

    # convert to list of strings
    data_list = dataframe['merge'].tolist()
    
    return data_list   


# Function: Load and standardize data files
def load_parse(data_list):  

    # remove paragraph / line marks and split up words (outputs bytestrings)
    tokenizer = text.WhitespaceTokenizer()
    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]

    # convert data back to string format
    num_entries = len(cleaned_list_byte)
    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) 
                    for i in range(num_entries)]

    return clean_list

def input_pipeline(paramaters, fresh_process=False):

    # unpack param
    saved_proc_dir = paramaters._processed_data_dir
    filepath = paramaters._datasets_dir

    # load previously processed data if possible
    # (pbz2 compressed file format)
    try:    
        assert(fresh_process is False)  # otherwise create dataset from files

        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:
            data_dict = cPickle.load(file)

        clean_list = data_dict['clean_list']
        print('loaded saved pre-processed data')

    # process data if no saved data found
    except:       

        # load raw data files from disk
        data_list = []
        for filename in paramaters._data_files:
            print(filename)
            print(filepath + '/' + filename)

            # select loader (csv or txt)
            _, file_extension = os.path.splitext(filename)     

            if file_extension == '.csv':   
                data = prepare_csv(filename, paramaters=paramaters,
                                   content_columns=['Name', 'Content'])
            
            else: # '.txt':
                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:
                    data = file.readlines()

            # update list of extracted texts
            data_list += data

        print('PROGRESS: data_list created')
        
        # clean the data
        clean_list = load_parse(data_list)
        print('PROGRESS: clean_list created')
        
        # save data to disk (pbz2 compressed file format)
        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:
            cPickle.dump({'clean_list': clean_list}, sfile)

    return clean_list

"""# Dynamic Dataset Creation 
Used within Training Loop
"""

def random_text_blocks(full_examples, num_batches, paramaters):

    """ creates random crops of example andn separates them into 
    input / target pairs for model training """

    max_length = paramaters._padded_example_length
    num_examples = len(full_examples)
    num_words = paramaters._num_trailing_words
    num_sets = int(num_batches * paramaters._batch_size)

    # count total characters
    example_starts = [0]
    for i in range(num_examples):
        example_length = len(full_examples[i])
        example_starts.append(example_starts[i] + example_length + 1)

    total_chars = example_starts[-1]
    example_starts = example_starts[:-1]

    # create character blocks
    char_blocks = []
    word_blocks = []
    
    completed = False
    while not completed:

        # choose random starting locations
        starting_points = tf.experimental.numpy.random.randint(
                                low=0, high=total_chars-2, size=num_sets)

        for start in starting_points:

            # find containing example
            temp = [i for i in range(num_examples) 
                    if start >= example_starts[i]]
           
            example_num = temp[-1]
            this_example = full_examples[example_num]
            this_ex_start = example_starts[example_num]

            if example_num < num_examples - 1:
                next_ex_start = example_starts[example_num + 1]
            else:
                next_ex_start = total_chars

            # cap length to stay within containing example
            length = min(next_ex_start - start, max_length)
            cropped_text = this_example[(start - this_ex_start): 
                                        length + (start - this_ex_start)]

            # enforce min character length
            if len(cropped_text) < 5:
                continue  # skip to next sample

            # enforce use of full words
            # add dummy start / end chars for easier word splitting
            if cropped_text[0] == ' ':
                cropped_text = '.' + cropped_text
            elif start==this_ex_start:
                cropped_text = '. ' + cropped_text

            if cropped_text[-1] == ' ':
                cropped_text = cropped_text + '.'
            elif cropped_text[-1] in string.punctuation:
                cropped_text = cropped_text + ' .'
            
            # split into words & drop incomplete first and last segments
            cropped_text = cropped_text.split(' ')

            # enforce min words length (2-3 words are dropped in later steps)
            if len(cropped_text) < 3:
                continue  # skip to next sample

            cropped_text = cropped_text[1:-1]  # drop incomplete or dummy start/end

            # get trailing words (last word dropped to avoid leaking target data)
            trailing_words = cropped_text[-num_words - 1: -1]
            
            # convert back to string
            cropped_text = ' '.join(cropped_text)
            trailing_words = ' '.join(trailing_words)

            # add cropped text
            char_blocks.append(cropped_text)
            word_blocks.append(trailing_words)
       
            if len(char_blocks) >= num_sets:
                completed = True
                break

    return char_blocks, word_blocks

def create_random_dataset(clean_list, num_batches, paramaters):

    # select data samples by applying random crops
    char_blocks, word_blocks  = \
        random_text_blocks(clean_list, num_batches, paramaters) 
    
    # tokenize the char_blocks
    char_blocks = make_padded_array(char_blocks, paramaters)

    # split inputs / targets
    char_input = char_blocks[:, :-1]
    target = char_blocks[:, 1:]

    # match datatypes and shapes
    num_samples = char_blocks.shape[0]
    word_input = np.array(word_blocks)

    word_input = tf.constant(word_input, dtype=tf.string)
    char_input = tf.constant(char_input, dtype=tf.int32)
    target = tf.constant(target, dtype=tf.int32)
    
    # convert to dataset
    ds = tf.data.Dataset.from_tensor_slices((
        (char_input, word_input), target))
    
    ds = ds.batch(paramaters._batch_size)\
      .shuffle(5000)\
      .prefetch(tf.data.experimental.AUTOTUNE)\
    
    return ds

"""### Define Models and Training Loop

Custom loss for (probability models)
"""

#VOCAB_SIZE = len(create_character_tokenizer().word_index) + 1

def neg_log_likely_logits(y_true, y_pred, depth):
    """ loss function for probabalistic model """

    # encode labels as one-hot vectors
    y_true_hot = tf.one_hot(y_true, depth=depth, axis=-1)

    # return negative log likelihood
    return -y_pred.log_prob(y_true_hot)

def loss(model, y_pred, target):

    # compute loss
    if model.paramaters._use_probability_layers:
        depth = model.paramaters._vocab_size
        loss = neg_log_likely_logits(target, y_pred, depth)

    else:
        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)(target, y_pred)

    return loss

"""Griadents"""

@tf.function
def grad(model, inputs, states, target, stateful):

    with tf.GradientTape() as tape:
        tape.watch(model.trainable_variables)

        # compute predictions
        y_pred, states = model(inputs, initial_states=states, stateful=stateful)
        
        # compute loss
        loss_value = loss(model, y_pred, target)
    
    # get gradients
    grads = tape.gradient(loss_value, model.trainable_variables)
    
    return loss_value, grads, states

"""Checkpoint Manager"""

# checkpoint manager
def create_checkpoint_manager(model):

    checkpoint = tf.train.Checkpoint(model=model)

    checkpoint_manager = tf.train.CheckpointManager(
                            checkpoint=checkpoint, 
                            directory=model.paramaters._checkpoint_dir, 
                            max_to_keep=4, 
                            keep_checkpoint_every_n_hours=None,
                            checkpoint_name='ckpt', 
                            step_counter=None, 
                            checkpoint_interval=None,
                            init_fn=None
                            )
    
    model.checkpoint_manager = checkpoint_manager
    model.checkpoint = checkpoint
    
    return None

"""## Training Loop"""

# Function: Train model
def train_model(model, cleaned_data_list, num_epochs, 
                batches_per_epoch, learning_rate):

    # get params
    batch_size = model.batch_size
    paramaters = model.paramaters
    stateful = False
    
    # get optimizer
    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)
    
    # set checkpoint manager
    if model.checkpoint is None or model.checkpoint_manager is None:
        create_checkpoint_manager(model=model)

    # initialize containers for metrics
    train_loss_results = []
    
    """
    # set callbacks
    # TensorBoard callback
    tensorboard_callback = tf.keras.callbacks.TensorBoard(
                                log_dir=paramaters._tensorboard_dir, 
                                histogram_freq=1,
                                )
    """
    
    # begin training loop
    # prepare datasets

    for epoch in range(num_epochs):

        print(f'Epoch: {epoch}')

        # initialize epoch metrics
        epoch_loss_avg = tf.keras.metrics.Mean()

        # create dataset from randomized text crops
        num_batches = max(batches_per_epoch, 1)
        dataset = create_random_dataset(cleaned_data_list, num_batches, 
                                        paramaters)
        
        # train model
        iteration = 0
        for (inputs, targets) in dataset:

            # get grads and loss
            if stateful is False:  # ensure no states passed in to model
                states = None
            loss_value, grads, states = \
                grad(model, inputs, states, targets, stateful=stateful)
            
            # update weights
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            # update metrics
            epoch_loss_avg(loss_value)

            # save checkpoint
            if iteration % 5 == 0:
                model.checkpoint_manager.save()
                
                ave_loss = epoch_loss_avg.result().numpy()
                print(f'  - loss: {np.format_float_positional(ave_loss, 4)}')
            
            iteration += 1

        # End of Epoch
        # save checkpoint
        model.checkpoint_manager.save()

        # record epoch metrics
        ave_loss = epoch_loss_avg.result().numpy()
        train_loss_results.append(ave_loss)
        mean_loss = np.mean(train_loss_results)
        
        # report epoch summary
        print(f'epoch ave loss: {np.format_float_positional(ave_loss, 4)}')
        print(f'overall ave loss: {np.format_float_positional(mean_loss, 4)}')

        # show sample result
        if epoch % 3 == 0:
            print('\n')
            generate_text(starting_text='Sample Output', 
                          prediction_model=model,
                          precision_reduction=0,
                          temperature=1,
                          print_result=True,
                          paramaters=model.paramaters,
                          num_generation_steps=40)

    # end of training    
    # consolidate saved metrics
    history = [train_loss_results]

    return model, history

"""Saving Models"""

# Store trained model separate from checkpoints
def save_model(model, paramaters):

    model_dir = paramaters._training_model_dir
    
    # save model
    tf.saved_model.save(model, model_dir)

    return None

"""### Text Generation Functions

"""

def convert_to_input(last_token, text_string, paramaters):
        
    # words
    if paramaters._use_word_path:
        num_words = paramaters._num_trailing_words

        words_input = text_string.split(' ')  # separate words 
        words_input = words_input[-num_words-1: -1]  # get trailing words
        words_input = tf.constant(' '.join(words_input))  # convert to tensor
        
    else:
        words_input=tf.constant('_ ')
    words_input = tf.reshape(words_input, shape=(1,-1))

    # pad token sequence
    inputs_char = tf.constant(last_token)
    inputs_char = tf.reshape(inputs_char, shape=(1,-1))

    # dummy output for batch

    # create separate input / target pairs for each block
    ds = tf.data.Dataset.from_tensor_slices((inputs_char, words_input, ()))
    
    # set batch shape for model input
    ds = ds.batch(1)

    return ds

def generator(input_text, prediction_model, precision_reduction, temperature,
              num_characters, print_result, paramaters):

    # get tokenizer (if not supplied)      
    tokenizer = paramaters._character_tokenizer
    
    # initialize generated text
    last_token =  tokenizer.texts_to_sequences([input_text])
    output_text = input_text.upper() + '\n'
    generated_text = list(output_text)
    
    # set initial GRU state values
    initial_states = None

    # text generation loop
    for _ in range(num_characters):
        
        # prepare input for model
        inputs = convert_to_input(last_token=last_token, 
                                  text_string=output_text,
                                  paramaters=paramaters)
                   
        # run model and get logits of last character prediction
        for input in inputs.take(1):
            logits, initial_states = \
                prediction_model(input, initial_states=initial_states, 
                                 stateful=True)
        
        # extract last character's logits
        logits = logits[0, -1, :].numpy() 
        
        # 'temperature' control to distort probabilities
        if temperature != 1:
            logits = logits / temperature

        # perturb probabilities before selecting token
        if precision_reduction != 0:   
            
            # add noise to perturb logits
            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)
            logits = logits * (1 + precision_reduction * fuzz_factor)

        # Choose a token using the logits probability distribution
        last_token = tf.random.categorical(logits=[logits], num_samples=1)       
        last_token = last_token.numpy().tolist()

        # correct for any invalid character token choices
        if last_token==[[0]]:  
            last_token==[[1]]
    
        # get text value of predicted character
        input_text = tokenizer.sequences_to_texts(last_token)
        input_text = input_text[0]

        # record generated character
        generated_text.append(input_text)
        output_text = ''.join(generated_text)
        
    if print_result:
        print(output_text)
    
    return output_text

"""Final generation function for end user"""

def generate_text(starting_text, 
                  prediction_model,
                  print_result,
                  paramaters=PARAMATERS,
                  precision_reduction=0.,
                  temperature=1,
                  num_generation_steps=150): # set length of generated text

    # format user input
    starting_text = starting_text.upper() + ': '

    # get generated text
    # note: very rarely this produces a line indexing error. 
    # This while loop reruns prediction if needed

    prediction = generator(input_text=starting_text, 
                                prediction_model=prediction_model, 
                                precision_reduction=precision_reduction, 
                                temperature=temperature,
                                num_characters=num_generation_steps, 
                                print_result=print_result,
                                paramaters=paramaters)
                                    
   
    # define formatting rules
    split_on = ['?', '.', ',', ';', '!', ':']
    splits = '([' + ''.join(split_on) + '])'
    split_lines_prediction = re.split(splits, prediction)

    # format output
    output = ''
    for line in split_lines_prediction:

        # capitalize first word of each line   
        if len(line) >= 1:
            line_update = line[0].upper()  

            # add capitalized letter to remainder of line
            if len(line) >= 2:
                line_update += line[1:]
        else:
            line_update = ''
                
        # update output text
        if (len(line_update) >= 1 and line_update[-1] in split_on) \
          or (len(line_update) >= 2 and line_update[-2:] == '\n'):
                output = ''.join([output, line_update])
        else:
            output = '\n'.join([output, line_update])

    
    return output + '... '

"""# Implementation

Load and Process Data
"""

require_fresh_process = False

try:
    # check if prepared datasets already in memory
    assert(require_fresh_process is False)
    assert(len(X_data_list) > 0)
    print('dataset already loaded')

except:
    print('Preparing dataset')
    cleaned_data_list = input_pipeline(paramaters=PARAMATERS, 
                                              fresh_process=require_fresh_process)

"""Initialize Training Model"""

model = GenerationModel(paramaters=PARAMATERS)
create_checkpoint_manager(model)

"""Load Latest Training Checkpoint"""

load_checkpoint=True

# run an element through model to buil it
# and match checkpointed model
temp_ds  = create_random_dataset(cleaned_data_list, 1, PARAMATERS)
for inp in temp_ds:
    model(inp[0], inp[1])

try:
    assert(load_checkpoint is True)

    # load checkpoint
    model.checkpoint_manager.restore_or_initialize()
    print('loaded checkpoint')

except: 
    print('No matching checkpoints')

"""Train Model

*Caution: overfitting can be a major problem where the model can eventually memorize and return complete segments from the source material. 'Precision reduction' and 'temperature' controls are included in the generation function to partially compensate by altering learned probabilities*
"""

train_model_now = True

learning_rate = 0.001
num_epochs = 60
batches_per_epoch = 60

# train model
if train_model_now:
    model, history = train_model(model, cleaned_data_list,
                                num_epochs=num_epochs, 
                                learning_rate=learning_rate,
                                batches_per_epoch=batches_per_epoch)

"""Save Models"""

save_model_now = True

if save_model_now:
    save_model(model, paramaters=model.paramaters)

"""## Test Output: Generate Text"""

starting_text = 'The road less'#AI is becoming accessible'
precision_reduction = 0 
temperature=1,

gen = generate_text(starting_text=starting_text, 
                    prediction_model=model,
                    precision_reduction=precision_reduction,
                    temperature=temperature,
                    print_result=True,
                    paramaters=PARAMATERS,
                    num_generation_steps=250)

print(gen)

"""## Web App

Anvil Web App Server Integration
"""

if PARAMATERS._use_anvil:

    # enable connection
    !pip install -q anvil-uplink
    import anvil.server
    anvil.server.connect(self.anvil_code)

    # get tokenizer
    tokenizer = create_character_tokenizer()

    @anvil.server.callable
    def anvil_callable(starting_text, 
                       precision_reduction=0,
                       temperature=1,
                        paramaters=PARAMATERS,
                        prediction_tokenizer=PARAMATERS._character_tokenizer, ####### not used -- remove
                        prediction_model=prediction_model,
                        print_result=True,
                        author='assorted',
                        num_generation_steps=150):

        return generate_text(starting_text=starting_text, 
                             precision_reduction=precision_reduction,
                             temperature=temperature,
                             prediction_model=prediction_model,
                             print_result=print_result,
                             paramaters=paramaters,
                             num_generation_steps=num_generation_steps)

    # start persistent connection to server
    anvil.server.wait_forever()
