{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Generator_Anvil Web App.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "tensor-flow-2-2",
      "graded_item_id": "4eYSM",
      "launcher_item_id": "HEV6h"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Capstone-Project-Text-Generation/blob/main/Text_Generator_Anvil_Web_App.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFRRArflRfR1"
      },
      "source": [
        "## Text Generation Models\r\n",
        "\r\n",
        "This file loads trained models for character-level text generation, then runs the prediction model on Anvil's servers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3wpZrx7RfR-"
      },
      "source": [
        "Load imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j_QtDXtRfR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3818e7-15d6-4bdd-c028-07ea8e6d961d"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "# install Anvil's web app link\n",
        "!pip install -q anvil-uplink\n",
        "import anvil.server\n",
        "anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "\n",
        "# ML design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text\n",
        "\n",
        "# data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# other\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "# load google drive:\n",
        "from google.colab import drive\n",
        "gdrive_dir = '/content/gdrive/'\n",
        "drive.mount(gdrive_dir)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25h  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default environment (dev)\" as SERVER\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 6.4MB/s \n",
            "\u001b[?25hMounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkHeu2U8keZ6"
      },
      "source": [
        "Set File directories and hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fRagWNBR0-E"
      },
      "source": [
        "# set hyperparameters\n",
        "batch_size=32\n",
        "\n",
        "# Set save/load directory info\n",
        "filepath = {}\n",
        "tokenizer = {}\n",
        "prediction_model = {}\n",
        "\n",
        "# set model options\n",
        "model_names = ['buy_local', 'Shakespeare', 'Robert_Frost']\n",
        "\n",
        "# set file paths\n",
        "models_folder = 'MyDrive/Colab_Notebooks/models/'\n",
        "\n",
        "for model in model_names:\n",
        "    filepath[model] = gdrive_dir + models_folder + model + '/'"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHPbk--Y-1J4"
      },
      "source": [
        "Load saved models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uPexjrJCm1h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "9ba4c227-899f-4451-a844-ec4fe08cafda"
      },
      "source": [
        "for model in model_names:\r\n",
        "\r\n",
        "    # set directory\r\n",
        "    directory = filepath[model]\r\n",
        "\r\n",
        "    # load tokenizer\r\n",
        "    with open(directory + 'prediction_model/tokenizer.pickle', 'rb') as handle:\r\n",
        "        tokenizer[model] = pickle.load(handle)\r\n",
        "\r\n",
        "    # load prediction model\r\n",
        "    prediction_model[model] = \\\r\n",
        "        tf.keras.models.load_model(directory + 'prediction_model/')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-1367723804be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'prediction_model/tokenizer.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/MyDrive/Colab_Notebooks/models/buy_local/prediction_model/tokenizer.pickle'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2msBeLSoRfT7"
      },
      "source": [
        "##### Functions generating text from models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aHg6y44RfUJ"
      },
      "source": [
        "# Function: Outputs weighted predictions for next token \n",
        "# (as logits / log-odds ration)\n",
        "def get_logits(model, token_sequence, initial_state=None):\n",
        "    \"\"\"\n",
        "    Paramater:\n",
        "    model - our prediction model set with batch size = 1\n",
        "    \"\"\"\n",
        "    \n",
        "    # carry forward previous state from GRU layer\n",
        "    GRU_layer = model.get_layer('GRU_1')\n",
        "    GRU_layer.reset_states(initial_state)\n",
        "\n",
        "    # Get the model's next token prediction (as logits)\n",
        "    input = tf.constant(token_sequence)\n",
        "    final_pred = model(input)[:, -1, :]\n",
        "        \n",
        "    return final_pred.numpy()    \n",
        "\n",
        "\n",
        "# Function: selects a value from logits prediction\n",
        "def sample_token(logits, precision_reduction=0):   \n",
        "\n",
        "    # choose a value from logits distribution\n",
        "    # fuzz_factor: adds some imprecision to results\n",
        "    fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\n",
        "\n",
        "    sample = tf.random.categorical(\n",
        "                        logits=logits * (1 + precision_reduction * fuzz_factor), \n",
        "                        num_samples=1, \n",
        "                        )\n",
        "\n",
        "    # convert to integer\n",
        "    next_token = sample[0,0].numpy()\n",
        "\n",
        "    return next_token\n",
        "\n",
        "\"\"\" old version\n",
        "def sample_token(logits):   \n",
        "\n",
        "    # choose a value from logits distribution\n",
        "    sample = tf.random.categorical(\n",
        "                        logits=logits, \n",
        "                        num_samples=1, \n",
        "                        )\n",
        "    \n",
        "    # convert to integer\n",
        "    next_token = sample[0,0].numpy()\n",
        "\n",
        "    return next_token\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYl_VlGatdzu"
      },
      "source": [
        "# Final Prediction Function #######\r\n",
        "\r\n",
        "# Use the model to generate a token sequence\r\n",
        "def make_prediction(init_string, num_generation_steps, precision_reduction=0,\r\n",
        "                    model_name='Shakespeare', print_result=False):\r\n",
        "    \r\n",
        "\r\n",
        "    our_model = prediction_model[model_name]\r\n",
        "    our_tokenizer = tokenizer[model_name]\r\n",
        "\r\n",
        "    GRU_layer = our_model.get_layer('GRU_1')\r\n",
        "\r\n",
        "    batch_size=1\r\n",
        "    \r\n",
        "    token_sequence = our_tokenizer.texts_to_sequences([init_string])\r\n",
        "    initial_state = None\r\n",
        "    input_sequence = token_sequence\r\n",
        "    init_len = len(input_sequence[0])\r\n",
        "\r\n",
        "    for i in range(num_generation_steps):\r\n",
        "        logits = get_logits(our_model, input_sequence, initial_state=initial_state)\r\n",
        "        sampled_token = sample_token(logits, precision_reduction)\r\n",
        "        token_sequence[0].append(sampled_token)\r\n",
        "        input_sequence = [[sampled_token]]  # use only last letter because previous model state is carried forward\r\n",
        "        initial_state = GRU_layer.states[0].numpy()\r\n",
        "\r\n",
        "    predicted_text = our_tokenizer.sequences_to_texts(token_sequence)[0][::2]\r\n",
        "\r\n",
        "    \r\n",
        "    \r\n",
        "    if print_result:\r\n",
        "        print(predicted_text)\r\n",
        "\r\n",
        "    return predicted_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdVAz52omlzz"
      },
      "source": [
        "temp_String = 'gjj, kgk. f'\r\n",
        "\r\n",
        "split_text = re.split('[?.,]', temp_String)\r\n",
        "\r\n",
        "print(split_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A0FOxOTRfUc"
      },
      "source": [
        "Sample text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgXNTiaYRfUd"
      },
      "source": [
        "########### set this to false to prevent training from overqriti\n",
        "\n",
        "\n",
        "init_string = 'EMMY:'  # starting point for prediction\n",
        "num_generation_steps = 300  # max number characters to produce\n",
        "\n",
        "make_prediction(init_string, num_generation_steps, \n",
        "                precision_reduction=5, \n",
        "                model_name='Shakespeare', print_result=True)\n",
        "\n",
        "\"\"\" old version\n",
        "make_prediction(init_string, num_generation_steps=num_generation_steps, \n",
        "                print_result=True)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxoTRH3d-KHg"
      },
      "source": [
        "## Run Program for Up Anvil Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69ZM4jE-sP8Z",
        "outputId": "1f887710-cecc-4a1f-ba2f-194ade5c7ab1"
      },
      "source": [
        "prediction = 'fdkjdfj, fsdjk.'\r\n",
        "\r\n",
        "split_lines_prediction = re.split('([?.,])', prediction)\r\n",
        "\r\n",
        "print(split_lines_prediction)\r\n",
        "\r\n",
        "output = ''\r\n",
        "i=0\r\n",
        "for line in split_lines_prediction:\r\n",
        "    if i%2==0:\r\n",
        "        output= ''.join([output, line])\r\n",
        "    else:\r\n",
        "        output= '\\n'.join([output, line])\r\n",
        "\r\n",
        "output= '... '.join([output, line])\r\n",
        "\r\n",
        "print(output)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fdkjdfj', ',', ' fsdjk', '.', '']\n",
            "fdkjdfj, fsdjk.... \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTXyxnDg6Qae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "aee514cc-553a-4250-ea2a-bbfce84c5526"
      },
      "source": [
        "@anvil.server.callable\r\n",
        "\r\n",
        "def generate_text(starting_text, precision_reduction=0, author='assorted'):\r\n",
        "\r\n",
        "    num_generation_steps = 250  # max number characters to produce\r\n",
        "  \r\n",
        "    prediction = make_prediction(init_string=starting_text, \r\n",
        "                               num_generation_steps=num_generation_steps, \r\n",
        "                               precision_reduction=precision_reduction, \r\n",
        "                               model_name=author, \r\n",
        "                               print_result=True)\r\n",
        "  \r\n",
        "    split_lines_prediction = re.split('([?.,;!:])', prediction)\r\n",
        "\r\n",
        "\r\n",
        "    output = ''\r\n",
        "    i = 0\r\n",
        "    for line in split_lines_prediction:\r\n",
        "        if i%2==1:\r\n",
        "            output= ''.join([output, line])\r\n",
        "        else:\r\n",
        "            output= '\\n'.join([output, line])\r\n",
        "            \r\n",
        "        i += 1\r\n",
        "\r\n",
        "    output= '... '.join([output, line])\r\n",
        "  \r\n",
        "    return output\r\n",
        "\r\n",
        "anvil.server.wait_forever()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" old version\\n@anvil.server.callable\\ndef generate_text(starting_text, author):\\n\\n  num_generation_steps = 350  # max number characters to produce\\n  \\n  prediction = make_prediction(starting_text, \\n                               num_generation_steps=num_generation_steps, \\n                               model_name=author,\\n                               print_result=False)\\n  \\n  \\n  return prediction + '... '\\n\\n\\nanvil.server.wait_forever()\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "3p7YOrnVrGiW",
        "outputId": "9902c408-0691-440e-ae92-ea8699eb142b"
      },
      "source": [
        "generate_text(starting_text='test, this is . how', precision_reduction=0, author='assorted')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-282ee9d3e64e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstarting_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test, this is . how'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_reduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'assorted'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-d6bd6df862ec>\u001b[0m in \u001b[0;36mgenerate_text\u001b[0;34m(starting_text, precision_reduction, author)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mnum_generation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m350\u001b[0m  \u001b[0;31m# max number characters to produce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   prediction = make_prediction(init_string=starting_text, \n\u001b[0m\u001b[1;32m      7\u001b[0m                                \u001b[0mnum_generation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_generation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                \u001b[0mprecision_reduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision_reduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'make_prediction' is not defined"
          ]
        }
      ]
    }
  ]
}