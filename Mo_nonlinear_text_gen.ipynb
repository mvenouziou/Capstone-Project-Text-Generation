{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_nonlinear_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPWAzU+FRjjpKGv06VDr9c7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Text-Generation/blob/main/Mo_nonlinear_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQyxxivFD7r"
      },
      "source": [
        "## Text Generation RNN\n",
        "\n",
        "This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. \n",
        "\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\n",
        "- See more at https://github.com/mvenouziou/Project-Text-Generation.\n",
        "\n",
        "- See credits /attributions below\n",
        "\n",
        "The code implements two different model architectures: \"linear\" and \"nonlinear.\"\n",
        "The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. \n",
        "\n",
        "---\n",
        "\n",
        "**What's New?**\n",
        "*(These items are original in the sense that I personally have not seen them at the original time of coding. Citations are below for content I have seen elsewhere.)*\n",
        "\n",
        "Model Architectures:\n",
        "\n",
        "- Experiments with: Nonlinear model architecture uses parallel RNN's with both word-level embeddings and character-level embeddings. \n",
        "\n",
        "- Experiments with: Tensorflow Probability layers to create a more interpretable probability distribution model. (Character-model only). The standard text generation algorithm outputs logits, which we view as a distribution from which to generate the next character. Here, we formalize this as outputing our model as a TF Probability Distribution, using probablistic weights in the Dense layer (instead of scalars) and trained via maximum likelihood. \n",
        "\n",
        "- Proper handling of GRU states for multiple stateful layers\n",
        "\n",
        "- Easily switch between model architectures through 'Paramaters' class object. Includes file management for organizing each architecture's checkpoints.\n",
        "\n",
        "\n",
        "Data Processing / Preparation:\n",
        "\n",
        "*These ideas are not all new, but I have not seen them implemented in text generation systems:*\n",
        "\n",
        "- Random crops and with random lengths and start locations. \n",
        "\n",
        "- Standard (character level) generation separates inputs and targets by one letter (generating one char at a time). This is adjusted for word-embeddings to avoid leaking target data, and multiple characters are generated at a time.\n",
        "\n",
        "- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) \n",
        "\n",
        "\n",
        "Generation:\n",
        "\n",
        "- Add perturbations to learned probabilties in final generation function, to add extra variety to generated text.  (Included in addition to the 'temperature' control described in TF's documentation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzU2QxoMFF7d"
      },
      "source": [
        "---\n",
        "**Credits / Citations / Attributions:**\n",
        "\n",
        "**Linear Model and Shared Code** \n",
        "\n",
        "Other than items noted in previous sections, this python code and linear model structure is based heavily on Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*\n",
        "\n",
        "\n",
        "**Nonlinear Model:**   \n",
        "\n",
        "This utilizes pretrained embeddings:\n",
        "-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" *https://tfhub.dev/google/collections/bert/1)*\n",
        "- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" *https://hub.tensorflow.google.cn/google/electra_small/2)*\n",
        "\n",
        "ELECTRA-Small++ has four times as many paramaters as the Small BERT embedding, producing better results, but at large computational cost.\n",
        "\n",
        "**Web App:** \n",
        "\n",
        "The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\n",
        "\n",
        "**Datasets:**\n",
        "\n",
        "- *'robert_frost_collection.csv'* is a Kaggle dataset available at https://www.kaggle.com/archanghosh/robert-frost-collection. Any other datasets used are public domain works available from Project Gutenberg https://www.gutenberg.org.\n",
        "\n",
        "---\n",
        "\n",
        "**About**\n",
        "\n",
        "Find me online at:\n",
        "- LinkedIn: https://www.linkedin.com/in/movenouziou/ \n",
        "- GitHub: https://github.com/mvenouziou\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "y1EW3w1GvDLx",
        "outputId": "a1a1fa33-65f1-455e-aed2-d272b74daa83"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# TF text processing (also required for TF HUB word encoders)\n",
        "!pip install -q tensorflow-text\n",
        "import tensorflow_text as text  \n",
        "\n",
        "# TF TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import random\n",
        "import re\n",
        "\n",
        "# file management\n",
        "import os\n",
        "import bz2\n",
        "import _pickle as cPickle\n",
        "\n",
        "\"\"\" ADDITIONAL IMPORTS:\n",
        "### Imported as needed within 'Paramaters' and GenerationModel classes:\n",
        "# these are left out to avoid incompatabilities with the cloud server and some \n",
        "# of the TF modules\n",
        "\n",
        "# TF pretrained models (for word encodings)\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "# TF probability modules\n",
        "import tensorflow_probability as tfp  \n",
        "from tensorflow_probability import layers as tfpl\n",
        "from tensorflow_probability import distributions as tfd\n",
        "\n",
        "# Google Drive integration with Google Colab)\n",
        "from google.colab import drive\n",
        "\n",
        "# Anvil Web App Server integration\n",
        "!pip install -q anvil-uplink\n",
        "import anvil.server\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" ADDITIONAL IMPORTS:\\n### Imported as needed within 'Paramaters' and GenerationModel classes:\\n# these are left out to avoid incompatabilities with the cloud server and some \\n# of the TF modules\\n\\n# TF pretrained models (for word encodings)\\nimport tensorflow_hub as hub\\n\\n# TF probability modules\\nimport tensorflow_probability as tfp  \\nfrom tensorflow_probability import layers as tfpl\\nfrom tensorflow_probability import distributions as tfd\\n\\n# Google Drive integration with Google Colab)\\nfrom google.colab import drive\\n\\n# Anvil Web App Server integration\\n!pip install -q anvil-uplink\\nimport anvil.server\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A5lFZkNEPVV"
      },
      "source": [
        "# Set Paramaters (global)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf85QK99q2YH"
      },
      "source": [
        "# paramater customizationss\n",
        "AUTHOR = 'tests'\n",
        "DATA_FILES = ['robert_frost_collection.csv']\n",
        "USE_GDRIVE = True  # required for save / load / checkpoint operations\n",
        "USE_ANVIL = False\n",
        "USE_PROBABILITY_LAYERS = True\n",
        "USE_WORD_PATH = False\n",
        "USE_ELECTRA = False"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2682GuzqxyW"
      },
      "source": [
        "### Set Model Paramaters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8uz4beXIwcw"
      },
      "source": [
        "Define Paramaters class.\n",
        "\n",
        "*(Controls the model architecture and the file system used for loading data and checkpointing models.)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b90MaNBijOfU"
      },
      "source": [
        "class Paramaters:\n",
        "    def __init__(self,  \n",
        "                 # integrations\n",
        "                use_gdrive, use_anvil,\n",
        "                 \n",
        "                 # model architecture\n",
        "                use_probability_layers,  # implements TensorFlow Probability\n",
        "                use_word_path,  # note: TFP layers not recommended with word-level model \n",
        "                use_electra, # use False for BERT embeddings (fewer params, word model only)\n",
        "                \n",
        "                # datasets\n",
        "                author, data_files, \n",
        "                datasets_dir='https://raw.githubusercontent.com/mvenouziou/text_generator/main/',\n",
        "                \n",
        "                # model hyper params\n",
        "                num_trailing_words=5, padded_example_length=300, batch_size=32):\n",
        "        \n",
        "        # save param choices\n",
        "        # note: additional attributes are added below\n",
        "        self._use_gdrive = use_gdrive\n",
        "        self._use_anvil = use_anvil\n",
        "        self._author = author       \n",
        "        self._num_chars_per_step = 1  # characters generated each model call\n",
        "        self._padded_example_length = padded_example_length\n",
        "        self._num_trailing_words = num_trailing_words\n",
        "        self._batch_size = batch_size\n",
        "        self._use_probability_layers = use_probability_layers\n",
        "        self._use_word_path = use_word_path\n",
        "        self._use_electra = use_electra\n",
        "        self._data_files = list(data_files)\n",
        "        self._datasets_dir = datasets_dir\n",
        "        self._embedding_dim = 32*8\n",
        "        self._char_rnn_units = 320\n",
        "        self._word_rnn_units = 64\n",
        "        self._merge_dim = 32*8\n",
        "\n",
        "        # Additional Imports\n",
        "        # # Tensorflow HUB for using pretrained embeddings (word models)\n",
        "        if self._use_word_path:\n",
        "            import tensorflow_hub as hub\n",
        "\n",
        "        # # Tensorflow Probability (probability distribution model architecture)\n",
        "        if self._use_probability_layers:\n",
        "            import tensorflow_probability as tfp  \n",
        "            from tensorflow_probability import layers as tfpl\n",
        "            from tensorflow_probability import distributions as tfd\n",
        "\n",
        "        # # Google Drive:\n",
        "        if self._use_gdrive:\n",
        "            self._gdrive_dir = '/content/gdrive/'\n",
        "            from google.colab import drive\n",
        "            drive.mount(self._gdrive_dir)\n",
        "        else:\n",
        "            self._gdrive_dir = ''\n",
        "\n",
        "        # # Anvil's web app server\n",
        "        if self._use_anvil:\n",
        "            !pip install -q anvil-uplink\n",
        "            import anvil.server\n",
        "            anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "            self.anvil = anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')\n",
        "\n",
        "        # Filepath Structure\n",
        "        # path name conventions due to model structure\n",
        "        if self._use_probability_layers :\n",
        "            self._author +=  '/probability/' \n",
        "        if self._use_word_path:\n",
        "            self._author += '_words_model/'\n",
        "        if self._use_electra:\n",
        "            self._author += 'electra/'\n",
        "\n",
        "        # models / checkpoints directories\n",
        "        # (Google Drive)\n",
        "        self._filepath = self._gdrive_dir + 'MyDrive/Colab_Notebooks/models/text_generation/' + self._author\n",
        "        self._checkpoint_dir = self._filepath + '/model_checkpoints/'\n",
        "\n",
        "        ###self._prediction_model_dir = self._filepath + '/prediction_model/'\n",
        "        self._training_model_dir = self._filepath + '/training_model/'\n",
        "        self._processed_data_dir = self._filepath + '/proc_data/'\n",
        "        self._tensorboard_dir = self._checkpoint_dir  + '/logs/'\n",
        "\n",
        "        # Create Tokenizer / Set Vocab Size\n",
        "        # character tokenizer\n",
        "        def create_character_tokenizer():\n",
        "        \n",
        "            char_tokens = string.printable\n",
        "            filters = '#$%&()*+-/<=>@[]^_`{|}~\\t'\n",
        "\n",
        "            # Initialize standard keras tokenizer\n",
        "            tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                            num_words=None,  \n",
        "                            filters=filters,\n",
        "                            lower=False,  # conversion to lowercase letters\n",
        "                            char_level=True,\n",
        "                            oov_token=None,  # drop unknown characters\n",
        "                            )      \n",
        "            # fit tokenizer\n",
        "            tokenizer.fit_on_texts(char_tokens)\n",
        "            \n",
        "            return tokenizer\n",
        "\n",
        "        self._character_tokenizer = create_character_tokenizer()\n",
        "        self._vocab_size = len(self._character_tokenizer.word_index) + 1\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZbLYkrN2ILz"
      },
      "source": [
        "### Define Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGhSLvnK2Fl-"
      },
      "source": [
        "# Model Class\n",
        "class GenerationModel(tf.keras.Model):\n",
        "\n",
        "    def __init__(self, paramaters, **kwargs):\n",
        "        super().__init__(self, **kwargs)\n",
        "       \n",
        "        # save attributes\n",
        "        self.paramaters = paramaters\n",
        "        self.gru_layer_names_char = ['char_GRU_1', 'char_GRU_2']\n",
        "        self.gru_layer_names_words = ['word_GRU_1', 'word_GRU_2']\n",
        "\n",
        "        # imports - Keras\n",
        "        from keras.layers import Input, Embedding, Concatenate, \\\n",
        "                            Dense, GRU,Average, AveragePooling1D, \\\n",
        "                            Dropout, BatchNormalization, Lambda, Concatenate\n",
        "        \n",
        "        # imports - Tensorflow Probability\n",
        "        if self.paramaters._use_probability_layers:\n",
        "            import tensorflow_probability as tfp  \n",
        "            from tensorflow_probability import layers as tfpl\n",
        "            from tensorflow_probability import distributions as tfd\n",
        "\n",
        "        # imports - Tensorflow HUB (pretrained embeddings)\n",
        "        if self.paramaters._use_word_path:\n",
        "            import tensorflow_hub as hub\n",
        "\n",
        "        # Set checkpoint manager\n",
        "        self.checkpoint, self.checkpoint_manager = self.create_checkpoint_manager()\n",
        "\n",
        "        # params for model architecture \n",
        "        use_word_path = paramaters._use_word_path\n",
        "        use_probability_layers = paramaters._use_probability_layers\n",
        "        vocab_size = paramaters._vocab_size\n",
        "        embedding_dim = paramaters._embedding_dim\n",
        "        char_rnn_units = paramaters._char_rnn_units\n",
        "        word_rnn_units = paramaters._word_rnn_units\n",
        "        merge_dim = paramaters._merge_dim\n",
        "        char_input_length = paramaters._padded_example_length\n",
        "        \n",
        "        # initialize placeholder TensorArrays (when not stateful)\n",
        "        # ## for stateful path\n",
        "        self.states_char_array = self.initialize_tensorarray(\n",
        "                                        size=len(self.gru_layer_names_char),\n",
        "                                        init_with_zero_units=char_rnn_units)\n",
        "        self.states_words_array = self.initialize_tensorarray(\n",
        "                                        size=len(self.gru_layer_names_words),\n",
        "                                        init_with_zero_units=word_rnn_units)\n",
        "        # ## placeholder value for non-stateful path\n",
        "        self.init_states_zeros_placeholder = self.create_zeros_placeholder()\n",
        "\n",
        "        # Model Layers\n",
        "        # ## Encoders\n",
        "        if use_word_path:\n",
        "            self.bert_tokenizer, self.bert_packer, self.bert_encoder = \\\n",
        "                        self.get_word_encoder()\n",
        "                        \n",
        "        self.char_embedding = Embedding(input_dim=vocab_size, \n",
        "                                        output_dim=embedding_dim, \n",
        "                                        mask_zero=True,\n",
        "                                        name='char_embedding',\n",
        "                                        input_length=char_input_length)\n",
        "        \n",
        "        # ## Character Path       \n",
        "        self.char_GRU_1 = GRU(units=char_rnn_units, return_state=True, \n",
        "                              return_sequences=True, name='char_GRU_1')\n",
        "        self.char_Batch_Norm_1 = BatchNormalization(name='char_Batch_Norm_1')\n",
        "        self.char_GRU_2 = GRU(units=char_rnn_units, return_state=True, \n",
        "                              return_sequences=True, name='char_GRU_2')\n",
        "        self.char_Batch_Norm_2 = BatchNormalization(name='char_Batch_Norm_2')\n",
        "\n",
        "        # ## Word Path\n",
        "        if use_word_path:\n",
        "            self.word_GRU_1 = GRU(units=word_rnn_units, return_state=True, \n",
        "                                  return_sequences=True, name='word_GRU_1',)\n",
        "            self.word_Batch_Norm_1 = BatchNormalization(name='word_Batch_Norm_1')\n",
        "            self.word_GRU_2 = GRU(units=word_rnn_units, return_state=True, \n",
        "                                  return_sequences=False, name='word_GRU_2',)\n",
        "            self.word_Batch_Norm_2 = BatchNormalization(name='word_Batch_Norm_2')\n",
        "\n",
        "        # ## Merge Char and Words paths\n",
        "        if use_word_path:\n",
        "            self.char_Dense_merge = Dense(units=merge_dim, activation=None, \n",
        "                                          name='char_Dense_merge')\n",
        "            self.word_Dense_merge = Dense(units=merge_dim, activation=None, \n",
        "                                          name='word_Dense_merge')\n",
        "            self.word_reshape = Lambda(lambda x: tf.expand_dims(x, axis=1), \n",
        "                                       name='word_reshape')\n",
        "            self.merged_layers = Lambda(lambda x: tf.concat([x[0][:, 1:, :], # drop first value to preserve shape after concat\n",
        "                                        x[1]], axis=1), name='merged_layers')     \n",
        "        else:\n",
        "            self.rename = Lambda(lambda x: x, name='rename_variable')       \n",
        "        self.Batch_Norm_output = BatchNormalization(name='Batch_Norm_output')\n",
        "        \n",
        "        # ## Output logits\n",
        "        if use_probability_layers:\n",
        "\n",
        "            self.dense_reparam = tfpl.DenseReparameterization(activation=None,\n",
        "                    units=tfpl.IndependentBernoulli.params_size(vocab_size))                     \n",
        "            self.decoding = tfpl.IndependentBernoulli(event_shape=[vocab_size],\n",
        "                convert_to_tensor_fn=tfd.Bernoulli.logits, name='Decoding')\n",
        "\n",
        "        else:\n",
        "            self.dense_outputs = Dense(units=vocab_size, activation=None, \n",
        "                                       name='Decoding')      \n",
        "                \n",
        "    def call(self, inputs, initial_states=None, stateful=False, **kwargs):\n",
        "       \n",
        "        # set model params\n",
        "        use_word_path = self.paramaters._use_word_path\n",
        "        use_probability_layers = self.paramaters._use_probability_layers\n",
        "        indx_state = None\n",
        "        \n",
        "        if stateful:\n",
        "\n",
        "            # initialize states containers (TensorArray)\n",
        "            states_char = self.states_char_array\n",
        "\n",
        "            if use_word_path:\n",
        "                states_words = self.states_words_array\n",
        "\n",
        "            # unpack states info if it was passed in\n",
        "            if initial_states is not None:\n",
        "\n",
        "                # characters array\n",
        "                states_list_char = tf.unstack(initial_states[0])\n",
        "                for indx in range(len(self.gru_layer_names_char)):\n",
        "                    states_char = states_char.write(indx, states_list_char[indx])     \n",
        "                \n",
        "                # words array\n",
        "                if use_word_path:\n",
        "                    states_list_words = tf.unstack(initial_states[1])\n",
        "                    for indx in range(len(self.gru_layer_names_char)):         \n",
        "                        states_words = states_words.write(indx, states_list_words[indx])                            \n",
        "        \n",
        "        # Begin Model Path\n",
        "        input_1 = inputs[0]\n",
        "        input_2 = inputs[1]\n",
        "\n",
        "        # ## Character Path Layers\n",
        "        x1 = self.char_embedding(input_1)\n",
        "        \n",
        "        # Char GRU 1\n",
        "        if stateful:\n",
        "            indx = self.gru_layer_names_char.index('char_GRU_1')\n",
        "            if initial_states is not None:\n",
        "                indx_state = states_char.read(indx)\n",
        "\n",
        "            x1, new_indx_state = self.char_GRU_1(x1, initial_state=indx_state)\n",
        "            states_char = states_char.write(indx, new_indx_state)\n",
        "        \n",
        "        else:\n",
        "            x1, _ = self.char_GRU_1(x1, initial_state=None)\n",
        "\n",
        "        x1 = self.char_Batch_Norm_1(x1)\n",
        "\n",
        "        # Char GRU 2\n",
        "        if stateful:\n",
        "            indx = self.gru_layer_names_char.index('char_GRU_2')\n",
        "            if initial_states is not None:\n",
        "                indx_state = states_char.read(indx)\n",
        "            x1, new_indx_state = self.char_GRU_2(x1, initial_state=indx_state)\n",
        "            states_char = states_char.write(indx, new_indx_state)\n",
        "        else:\n",
        "            x1, _ = self.char_GRU_2(x1, initial_state=None)\n",
        "        \n",
        "        x1 = self.char_Batch_Norm_2(x1)\n",
        "\n",
        "        # Word Encoding Path Layers\n",
        "        if use_word_path:\n",
        "            # encoding\n",
        "            x2 = self.bert_tokenizer(input_2)  # tokenize\n",
        "            x2 = self.bert_packer([x2])  # pack inputs for encoder\n",
        "            x2 = self.bert_encoder(x2)['sequence_output'] # encoding\n",
        "            \n",
        "            # Word GRU 1\n",
        "            if stateful:\n",
        "                indx = self.gru_layer_names_words.index('word_GRU_1')\n",
        "                if initial_states is not None:\n",
        "                    indx_state = states_words.read(indx)\n",
        "                x2, new_indx_state = self.word_GRU_1(x2, initial_state=indx_state)\n",
        "                states_words = states_words.write(indx, new_indx_state)\n",
        "            else:\n",
        "                x2, _ = self.word_GRU_1(x2, initial_state=None)\n",
        "            x2 = self.word_Batch_Norm_1(x2)\n",
        "\n",
        "            # Word GRU 2\n",
        "            if stateful:\n",
        "                indx = self.gru_layer_names_words.index('word_GRU_2')\n",
        "                if initial_states is not None:\n",
        "                    indx_state = states_words.read(indx)\n",
        "                x2, new_indx_state = self.word_GRU_2(x2, initial_state=indx_state)\n",
        "                states_words = states_words.write(indx, new_indx_state)\n",
        "            else:\n",
        "                x2, _ = self.word_GRU_2(x2, initial_state=None)\n",
        "            x2 = self.word_Batch_Norm_2(x2)\n",
        "\n",
        "        # Merge Layers\n",
        "        if use_word_path:\n",
        "            x1 = self.char_Dense_merge(x1)\n",
        "            x2 = self.word_Dense_merge(x2)\n",
        "            x2 = self.word_reshape(x2)\n",
        "            x = self.merged_layers((x1, x2))\n",
        "            \n",
        "        else:  # update variable id to match next step\n",
        "            x = self.rename(x1)\n",
        "                \n",
        "        x = self.Batch_Norm_output(x)\n",
        "\n",
        "        # Character prediction (logits)\n",
        "        if use_probability_layers:  # weights are prob dstributions\n",
        "            x = self.dense_reparam(x)\n",
        "            y_pred = self.decoding(x)     \n",
        "\n",
        "        else:\n",
        "            y_pred = self.dense_outputs(x)\n",
        "\n",
        "        # handle statefulness\n",
        "        if stateful:\n",
        "            new_char_states = states_char.stack()\n",
        "            #states_char.close()\n",
        "\n",
        "            if use_word_path:\n",
        "                new_word_states = states_words.stack()\n",
        "                #states_words.close()\n",
        "            else:\n",
        "                new_word_states = self.init_states_zeros_placeholder[1]\n",
        "            new_states = tf.tuple([new_char_states, new_word_states])\n",
        "\n",
        "        else:\n",
        "            new_states = self.init_states_zeros_placeholder\n",
        "\n",
        "        return y_pred, new_states\n",
        "\n",
        "\n",
        "    # Checkpoint Manager\n",
        "    def create_checkpoint_manager(self):\n",
        "        \"\"\" manages automatic saving during model training \"\"\"\n",
        "\n",
        "        checkpoint = tf.train.Checkpoint(model=self)\n",
        "        checkpoint_manager = tf.train.CheckpointManager(\n",
        "                                checkpoint=checkpoint, \n",
        "                                directory=self.paramaters._checkpoint_dir, \n",
        "                                max_to_keep=4, \n",
        "                                keep_checkpoint_every_n_hours=None,\n",
        "                                checkpoint_name='ckpt', \n",
        "                                step_counter=None, \n",
        "                                checkpoint_interval=None,\n",
        "                                init_fn=None)\n",
        "\n",
        "        return checkpoint, checkpoint_manager\n",
        "    \n",
        "\n",
        "    def get_word_encoder(self):\n",
        "        \"\"\" loads pretrained word embedding models \"\"\"\n",
        "\n",
        "        import tensorflow_hub as hub\n",
        "\n",
        "        # Word Embeddings\n",
        "        # Selects file locations for BERT or ELECTRA pretrained encoders\n",
        "        if self.paramaters._use_electra:\n",
        "            encoder_url = 'https://tfhub.dev/google/electra_small/2'\n",
        "        else:\n",
        "            encoder_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                                + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\n",
        "        preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\n",
        "                            + 'bert_en_uncased_preprocess/3'\n",
        "                \n",
        "        # Get Encoder / Preprocessing layers\n",
        "        preprocessor = hub.load(preprocessor_url)\n",
        "\n",
        "        bert_tokenizer = hub.KerasLayer(preprocessor.tokenize, name='bert_tokenizer')\n",
        "        \n",
        "        bert_packer = hub.KerasLayer(\n",
        "                        preprocessor.bert_pack_inputs,\n",
        "                        name='bert_input_packer')\n",
        "\n",
        "        word_encoder = hub.KerasLayer(encoder_url, trainable=False, \n",
        "                                    name='Word_encoder')\n",
        "\n",
        "        \n",
        "        return bert_tokenizer, bert_packer, word_encoder\n",
        "\n",
        "    \n",
        "    # Tensor Arrays\n",
        "    def initialize_tensorarray(self, size, init_with_zero_units=None):\n",
        "        \"\"\" note: We use Tensor Arrays to hold GRU state values \"\"\"\n",
        "        \n",
        "        array = tf.TensorArray(tf.float32, size=size, clear_after_read=True)    \n",
        "        array.trainable=False      \n",
        "\n",
        "        if init_with_zero_units is not None:        \n",
        "            zeros = tf.zeros((1, init_with_zero_units))           \n",
        "            for indx in range(size):         \n",
        "                array = array.write(indx, zeros)\n",
        "\n",
        "        return array \n",
        "\n",
        "    def create_zeros_placeholder(self):\n",
        "        \"\"\" Creates placeholders for when model is set to stateful=False.\n",
        "        Output is same shape as saved model states \"\"\"\n",
        "\n",
        "        char_rnn_units = self.paramaters._char_rnn_units\n",
        "        word_rnn_units = self.paramaters._word_rnn_units\n",
        "\n",
        "        states_char_array_zeros = self.initialize_tensorarray(\n",
        "                                            size=len(self.gru_layer_names_char),\n",
        "                                            init_with_zero_units=char_rnn_units)\n",
        "        states_words_array_zeros = self.initialize_tensorarray(\n",
        "                                            size=len(self.gru_layer_names_words),\n",
        "                                            init_with_zero_units=word_rnn_units)\n",
        "\n",
        "        # ## convert to init state placeholder\n",
        "        char_states = states_char_array_zeros.stack()\n",
        "        word_states = states_words_array_zeros.stack()\n",
        "        char_states.trainable = False       \n",
        "        word_states.trainable = False  \n",
        "\n",
        "        # ## close out zeros TensorArrays (for non-stateful path)\n",
        "        states_char_array_zeros.close()\n",
        "        states_words_array_zeros.close()\n",
        "\n",
        "        return tf.tuple([char_states, word_states])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlzYCN71orw"
      },
      "source": [
        "## Input Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqJnhind4a_"
      },
      "source": [
        "Load and Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzwvJaIXQ3q"
      },
      "source": [
        "# Function: loader for .csv files\n",
        "def prepare_csv(filename, paramaters, content_columns=['Title', 'Content']):\n",
        "    \n",
        "    \"\"\" Process CSV files. Text must be in column named 'Content', \n",
        "    (with optional 'Title' column allowed for titles).\"\"\"\n",
        "\n",
        "    # load data into DataFrame\n",
        "    dataframe = pd.read_csv(paramaters._datasets_dir + filename).dropna()\n",
        "    \n",
        "    # extract titles and content\n",
        "    # note: column headings must match those below\n",
        "    # This step is specific to the Robert Frost set\n",
        "    if 'Name ' in dataframe.columns:  \n",
        "        dataframe.rename(columns={'Name ':'Title'})\n",
        "    \n",
        "    # prepare titles\n",
        "    if 'Title' in dataframe.columns:  # add ':\\n'\n",
        "        dataframe['Title'] = dataframe['Title'].apply(lambda x: x + ':\\n')\n",
        "    else:  # no titles found\n",
        "        content_columns = ['Content']\n",
        "\n",
        "    # prepare content\n",
        "    #dataframe['Content'] = dataframe['Content'].apply(lambda x: x + '\\n')\n",
        "    #dataframe = dataframe[content_columns]\n",
        "\n",
        "    # shuffle entries (rows)\n",
        "    dataframe = dataframe.sample(frac=1)\n",
        "    \n",
        "    # merge desired text columns\n",
        "    dataframe['merge'] = dataframe[content_columns[0]]\n",
        "    for i in range(1, len(content_columns)):\n",
        "        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\n",
        "\n",
        "    # convert to list of strings\n",
        "    data_list = dataframe['merge'].tolist()\n",
        "    \n",
        "    return data_list   \n",
        "\n",
        "\n",
        "# Function: Load and standardize data files\n",
        "def load_parse(data_list):  \n",
        "    \"\"\"\n",
        "    Placeholder function for any other data prep steps\n",
        "    \"\"\"\n",
        "    return clean_list"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOSV4SWyyoS"
      },
      "source": [
        "def input_pipeline(paramaters, fresh_process=False):\n",
        "\n",
        "    # unpack param\n",
        "    saved_proc_dir = paramaters._processed_data_dir\n",
        "    filepath = paramaters._datasets_dir\n",
        "\n",
        "    # load previously processed data if possible\n",
        "    # (pbz2 compressed file format)\n",
        "    try:    \n",
        "        assert(fresh_process is False)  # otherwise create dataset from files\n",
        "\n",
        "        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\n",
        "            data_dict = cPickle.load(file)\n",
        "\n",
        "        clean_list = data_dict['clean_list']\n",
        "        print('loaded saved pre-processed data')\n",
        "\n",
        "    # process data if no saved data found\n",
        "    except:       \n",
        "\n",
        "        # load raw data files from disk\n",
        "        data_list = []\n",
        "        for filename in paramaters._data_files:\n",
        "            print(filename)\n",
        "            print(filepath + '/' + filename)\n",
        "\n",
        "            # select loader (csv or txt)\n",
        "            _, file_extension = os.path.splitext(filename)     \n",
        "\n",
        "            if file_extension == '.csv':   \n",
        "                data = prepare_csv(filename, paramaters=paramaters,\n",
        "                                   content_columns=['Name', 'Content'])\n",
        "            \n",
        "            else: # '.txt':\n",
        "                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:\n",
        "                    data = file.readlines()\n",
        "\n",
        "            # update list of extracted texts\n",
        "            data_list += data\n",
        "\n",
        "        print('PROGRESS: data_list created')\n",
        "        \n",
        "        # clean the data\n",
        "        clean_list = load_parse(data_list)\n",
        "        print('PROGRESS: clean_list created')\n",
        "        \n",
        "        # save data to disk (pbz2 compressed file format)\n",
        "        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\n",
        "            cPickle.dump({'clean_list': clean_list}, sfile)\n",
        "\n",
        "    return clean_list"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wSXBlz5_myo"
      },
      "source": [
        "Dynamic Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkuG9SJn_mQx"
      },
      "source": [
        "def random_text_blocks(full_examples, num_batches, paramaters,\n",
        "                       for_training=True):\n",
        "\n",
        "    \"\"\" \n",
        "    When 'for_training' is True, this creates random crops \n",
        "    of examples and creates input / target pairs for model training. \n",
        "    \n",
        "    Otherwise this prepares a single text for input into model\n",
        "    \"\"\"\n",
        "\n",
        "    # skip on inference\n",
        "    if not for_training:\n",
        "        char_blocks = full_examples\n",
        "        return char_blocks\n",
        "\n",
        "    num_chars_per_step = paramaters._num_chars_per_step\n",
        "    max_length = paramaters._padded_example_length + num_chars_per_step\n",
        "    num_examples = len(full_examples)\n",
        "    num_sets = int(num_batches * paramaters._batch_size)\n",
        "    num_words = paramaters._num_trailing_words\n",
        "    use_word_path = paramaters._use_word_path\n",
        "\n",
        "\n",
        "    # count total characters\n",
        "    example_starts = [0]\n",
        "    for i in range(num_examples):\n",
        "        example_length = len(full_examples[i])\n",
        "        example_starts.append(example_starts[i] + example_length + 1)\n",
        "\n",
        "    total_chars = example_starts[-1]\n",
        "    example_starts = example_starts[:-1]\n",
        "\n",
        "    # create input / target blocks\n",
        "    char_blocks = []\n",
        "    word_blocks = []\n",
        "    target_blocks = []\n",
        "    \n",
        "    if use_word_path is False:\n",
        "        min_example_length = 10 + num_chars_per_step\n",
        "    else:\n",
        "        # note: word path leaks a lot of info on its overlap\n",
        "        # with chars input / target. Need to keep num words small \n",
        "        # compared to example length to compensate\n",
        "        min_example_length = 10*num_words + num_chars_per_step\n",
        "    \n",
        "    \n",
        "    completed = False\n",
        "    while not completed:\n",
        "\n",
        "        # choose random starting locations\n",
        "        starting_points = tf.experimental.numpy.random.randint(\n",
        "                                low=0, high=total_chars-min_example_length, \n",
        "                                size=num_sets)\n",
        "\n",
        "        for start in starting_points:\n",
        "\n",
        "            # find containing example\n",
        "            temp = [i for i in range(num_examples) \n",
        "                    if start >= example_starts[i]]\n",
        "           \n",
        "            example_num = temp[-1]\n",
        "            this_example = full_examples[example_num]\n",
        "            this_ex_start = example_starts[example_num]\n",
        "\n",
        "            if example_num < num_examples - 1:\n",
        "                next_ex_start = example_starts[example_num + 1]\n",
        "            else:\n",
        "                next_ex_start = total_chars\n",
        "\n",
        "            # cap length to stay within containing example\n",
        "            # and is not cropped when converted to padded array\n",
        "            length = min(next_ex_start - start, max_length)\n",
        "            \n",
        "            # enforce min character length\n",
        "            if length < min_example_length:\n",
        "                continue  # skip to next sample\n",
        "            \n",
        "            # crop sample\n",
        "            cropped_text = this_example[(start - this_ex_start): \n",
        "                                        length + (start - this_ex_start)]\n",
        "\n",
        "            # require first 'word' to be complete\n",
        "            if start != this_ex_start:\n",
        "                skip_to_next = False\n",
        "                while cropped_text[0] != ' ':\n",
        "                    cropped_text = cropped_text[1:]\n",
        "                    \n",
        "                    if len(cropped_text)==0:  \n",
        "                        # ran out fo characters, skip example\n",
        "                        skip_to_next = True\n",
        "                        break\n",
        "                if skip_to_next:\n",
        "                    continue\n",
        "                cropped_text = cropped_text[1:]         \n",
        "\n",
        "            # update input / target blocs\n",
        "            char_blocks.append(cropped_text)\n",
        "             \n",
        "            if len(char_blocks) >= num_sets:\n",
        "                completed = True\n",
        "                break\n",
        "\n",
        "    return char_blocks"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqQHC1D6Uwp"
      },
      "source": [
        "# Character-Level Encoder\n",
        "\n",
        "def make_padded_array(text_blocks, paramaters, padding, for_training):\n",
        "    # Tokenizes and applies padding for uniform length\n",
        "\n",
        "    # load tokenizer if one is not supplied\n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "\n",
        "    # tokenize\n",
        "    token_blocks = tokenizer.texts_to_sequences(text_blocks)\n",
        "\n",
        "    # zero padding\n",
        "    # (this will later be split into input / target)\n",
        "    if for_training:\n",
        "        maxlen = paramaters._padded_example_length + paramaters._num_chars_per_step   \n",
        "\n",
        "        padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                            sequences=token_blocks,  # dataset\n",
        "                            maxlen=maxlen, \n",
        "                            dtype='int32', \n",
        "                            padding=padding,\n",
        "                            truncating=padding,\n",
        "                            value=0.0\n",
        "                            )\n",
        "    else: \n",
        "        padded_blocks = np.array(token_blocks)\n",
        "    \n",
        "    return padded_blocks"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyNV385EZeAP"
      },
      "source": [
        "def create_random_dataset(clean_list, num_batches, paramaters,\n",
        "                          for_training=False):\n",
        "\n",
        "    # get params\n",
        "    num_chars_per_step = paramaters._num_chars_per_step\n",
        "    num_words = paramaters._num_trailing_words\n",
        "\n",
        "    # select data samples by applying random crops\n",
        "    char_blocks = random_text_blocks(clean_list, num_batches, \n",
        "                                     paramaters, for_training) \n",
        "    \n",
        "    # convert characters to array\n",
        "    # and split into inputs / targets\n",
        "    char_array = make_padded_array(char_blocks, paramaters, \n",
        "                                       padding='pre', \n",
        "                                       for_training=for_training)\n",
        "\n",
        "    if for_training:\n",
        "        char_input = char_array[:, : -num_chars_per_step]\n",
        "        target = char_array[:, num_chars_per_step: ]\n",
        "    else:\n",
        "        char_input = char_array\n",
        "        target = char_input\n",
        "    \n",
        "    # create words inputs\n",
        "    if paramaters._use_word_path:\n",
        "        def split_words(text):\n",
        "            text = text[: -num_chars_per_step]\n",
        "            words = text.split()[:-1]  # drop (possibly partial) last word\n",
        "            words = words[: num_words]\n",
        "            return ' '.join(words)\n",
        "\n",
        "        word_input = np.array([split_words(text) for text in char_blocks])\n",
        "    else:\n",
        "        word_input = tf.constant(' ', shape=char_array.shape[0])\n",
        "\n",
        "    # update to TF dtypes\n",
        "    char_input = tf.constant(char_input, dtype=tf.int32, name='char_input')\n",
        "    target = tf.constant(target, dtype=tf.int32, name='target')\n",
        "    word_input = tf.constant(word_input, dtype=tf.string, name='word_input')\n",
        "\n",
        "    # batch and shuffle for training\n",
        "    if for_training:    \n",
        "        ds = tf.data.Dataset.from_tensor_slices((\n",
        "                    (char_input, word_input), target))\n",
        "        ds = ds.batch(paramaters._batch_size)\\\n",
        "        .shuffle(5000)\\\n",
        "        .prefetch(tf.data.experimental.AUTOTUNE)\\\n",
        "    \n",
        "    else:\n",
        "        ds = (char_input, word_input)\n",
        "    \n",
        "    return ds\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpjX19aWvU6"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFPy3FIO59y7"
      },
      "source": [
        "Loss Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQLj9Q1VOfG6"
      },
      "source": [
        "#VOCAB_SIZE = len(create_character_tokenizer().word_index) + 1\n",
        "\n",
        "def neg_log_likely_logits(y_true, y_pred, depth):\n",
        "    \"\"\" loss function for probabalistic model \"\"\"\n",
        "\n",
        "    # encode labels as one-hot vectors\n",
        "    y_true_hot = tf.one_hot(y_true, depth=depth, axis=-1)\n",
        "\n",
        "    # return negative log likelihood\n",
        "    return -y_pred.log_prob(y_true_hot)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQC-Ahru83WN"
      },
      "source": [
        "def loss(model, y_pred, target):\n",
        "\n",
        "    # compute loss\n",
        "    if model.paramaters._use_probability_layers:\n",
        "        depth = model.paramaters._vocab_size\n",
        "        loss = neg_log_likely_logits(target, y_pred, depth)\n",
        "\n",
        "    else:\n",
        "        loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)(target, y_pred)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiL9QFvpXyBk"
      },
      "source": [
        "Gradient Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Pr17EAJ8FPP"
      },
      "source": [
        "@tf.function(experimental_relax_shapes=True)\n",
        "def grad(model, inputs, states, target, stateful):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(model.trainable_variables)\n",
        "\n",
        "        # compute predictions\n",
        "        y_pred, states = model(inputs, initial_states=states, stateful=stateful)\n",
        "        \n",
        "        # compute loss\n",
        "        loss_value = loss(model, y_pred, target)\n",
        "    \n",
        "    # get gradients\n",
        "    grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "    \n",
        "    return loss_value, grads, states"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWkTKUaXZbm"
      },
      "source": [
        "Custom Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZWbiojXXpS"
      },
      "source": [
        "# Function: Train model\n",
        "def train_model(model, cleaned_data_list, num_epochs, \n",
        "                batches_per_epoch, learning_rate):\n",
        "\n",
        "    # get params\n",
        "    paramaters = model.paramaters\n",
        "    stateful = False\n",
        "    \n",
        "    # get optimizer\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    \n",
        "    # set checkpoint manager\n",
        "    if model.checkpoint is None or model.checkpoint_manager is None:\n",
        "        create_checkpoint_manager(model=model)\n",
        "\n",
        "    # initialize containers for metrics\n",
        "    train_loss_results = []\n",
        "    \n",
        "    \"\"\"\n",
        "    # set callbacks\n",
        "    # TensorBoard callback\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "                                log_dir=paramaters._tensorboard_dir, \n",
        "                                histogram_freq=1,\n",
        "                                )\n",
        "    \"\"\"\n",
        "    \n",
        "    # begin training loop\n",
        "    # prepare datasets\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        tf.print(f'Epoch: {epoch}')\n",
        "\n",
        "        # initialize epoch metrics\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "\n",
        "        # create dataset from randomized text crops\n",
        "        num_batches = max(batches_per_epoch, 1)\n",
        "        dataset = create_random_dataset(cleaned_data_list, \n",
        "                                        num_batches, \n",
        "                                        paramaters,\n",
        "                                        for_training=True)\n",
        "        \n",
        "        # train model\n",
        "        iteration = 0\n",
        "        for (inputs, targets) in dataset:\n",
        "\n",
        "            # get grads and loss\n",
        "            if stateful is False:  # ensure no states passed in to model\n",
        "                states = None\n",
        "            loss_value, grads, states = \\\n",
        "                grad(model, inputs, states, targets, stateful=stateful)\n",
        "            \n",
        "            # update weights\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            # update metrics\n",
        "            epoch_loss_avg(loss_value)\n",
        "            \n",
        "            # report results\n",
        "            if iteration % 5 == 0:\n",
        "                ave_loss = epoch_loss_avg.result().numpy()\n",
        "                tf.print(f'  - loss: {np.format_float_positional(ave_loss, 4)}')\n",
        "            \n",
        "            iteration += 1\n",
        "\n",
        "        # End of Epoch\n",
        "        # save checkpoint\n",
        "        model.checkpoint_manager.save()\n",
        "\n",
        "        # record epoch metrics\n",
        "        ave_loss = epoch_loss_avg.result().numpy()\n",
        "        train_loss_results.append(ave_loss)\n",
        "        mean_loss = np.mean(train_loss_results)\n",
        "        \n",
        "        # report epoch summary\n",
        "        tf.print(f'epoch ave loss: {np.format_float_positional(ave_loss, 4)}')\n",
        "        tf.print(f'overall ave loss: {np.format_float_positional(mean_loss, 4)}')\n",
        "\n",
        "        # show sample result\n",
        "        if epoch % 3 == 0:\n",
        "            num_generation_steps = 20\n",
        "            sample_input = 'The road less travelled is'\n",
        "            \n",
        "            tf.print('\\nSample input:', sample_input)\n",
        "            tf.print('Output:', end='')\n",
        "            generate_text(input_text=sample_input, \n",
        "                          prediction_model=model,\n",
        "                          precision_reduction=0,\n",
        "                          temperature=1,\n",
        "                          num_generation_steps=num_generation_steps,\n",
        "                          print_result=True)\n",
        "\n",
        "    # end of training    \n",
        "    # save checkpoint\n",
        "    model.checkpoint_manager.save()\n",
        "\n",
        "    # consolidate saved metrics\n",
        "    history = [train_loss_results]\n",
        "\n",
        "    return model, history"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rFetpOUqwQ"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tf4x1q8UpfD"
      },
      "source": [
        "# Store trained model separate from checkpoints\n",
        "def save_model(model, paramaters):\n",
        "\n",
        "    model_dir = paramaters._training_model_dir\n",
        "    \n",
        "    # save model\n",
        "    tf.saved_model.save(model, model_dir)\n",
        "\n",
        "    return None"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMNy3XTW2Be"
      },
      "source": [
        "### Text Generation Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuUcE7H4snz"
      },
      "source": [
        "def convert_to_input(generated_text, paramaters):    \n",
        "\n",
        "    if generated_text == '':\n",
        "        generated_text = ' '\n",
        "        \n",
        "    generated_text = [generated_text]\n",
        "\n",
        "    input = create_random_dataset(generated_text, \n",
        "                                     num_batches=1, \n",
        "                                     paramaters=paramaters,\n",
        "                                     for_training=False)\n",
        "    return input"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2atvRiGv-z"
      },
      "source": [
        "def generator(input_text, prediction_model, precision_reduction, temperature,\n",
        "              num_steps, print_result):\n",
        "\n",
        "    # get tokenizer and params\n",
        "    paramaters = prediction_model.paramaters\n",
        "    tokenizer = paramaters._character_tokenizer\n",
        "    num_characters_per_step = paramaters._num_chars_per_step\n",
        "    \n",
        "    # enforce dtypes\n",
        "    temperature = tf.constant(temperature, tf.float32)\n",
        "    precision_reduction = tf.constant(precision_reduction, tf.float32)\n",
        "    \n",
        "    # initialize generated text\n",
        "    generated_text = input_text\n",
        "    \n",
        "    # set initial GRU state values\n",
        "    initial_states = None\n",
        "\n",
        "    # text generation loop\n",
        "    for _ in range(num_steps):\n",
        "        \n",
        "        # tf.print('input_text:', input_text)\n",
        "        # prepare input for model\n",
        "        input = convert_to_input(input_text, paramaters)\n",
        "    \n",
        "        # run model and get logits of last character prediction\n",
        "        logits, initial_states = prediction_model(input, \n",
        "                                                  initial_states, \n",
        "                                                  stateful=True)\n",
        "\n",
        "        # extract newly generated character logits\n",
        "        logits = logits[0, -num_characters_per_step:, :]\n",
        "\n",
        "        # 'temperature' control to distort probabilities\n",
        "        if temperature != 1.:  \n",
        "            logits = logits / temperature\n",
        "        \n",
        "        # 'precision reduction' (random perturbations to probs)\n",
        "        if precision_reduction != 0.:            \n",
        "            fuzz_factor = tf.random.normal(shape=logits.shape, \n",
        "                                           mean=1, \n",
        "                                           stddev=.2)\n",
        "            logits = logits * (1 + precision_reduction * fuzz_factor)\n",
        "        \n",
        "        # Choose a token using the logits probability distribution\n",
        "        new_tokens = tf.random.categorical(logits=logits, num_samples=1) \n",
        "        #print('new_tokens:', new_tokens)\n",
        "\n",
        "        # get newly predicted characters\n",
        "        # (becomes input for next iteration)\n",
        "        new_tokens = new_tokens.numpy().tolist()\n",
        "        input_text = tokenizer.sequences_to_texts(new_tokens)\n",
        "        #print('input_text:', input_text)\n",
        "\n",
        "        # updated generated text\n",
        "        input_text = ''.join(input_text)\n",
        "        generated_text = generated_text + input_text\n",
        "        \n",
        "    if print_result:\n",
        "        tf.print(generated_text)\n",
        "    \n",
        "    return generated_text"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iiLYUGJ4uvk"
      },
      "source": [
        "Final generation function for end user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCvZ7x8D4rz1"
      },
      "source": [
        "def generate_text(input_text, \n",
        "                  prediction_model,\n",
        "                  print_result,\n",
        "                  precision_reduction=0.,\n",
        "                  temperature=1,\n",
        "                  num_generation_steps=150): # set length of generated text\n",
        "\n",
        "    # format user input\n",
        "    if input_text == '':\n",
        "        input_text = ' '\n",
        "    starting_text = input_text[0].upper()\n",
        "    \n",
        "    if len(input_text) >= 2:\n",
        "        starting_text += input_text[1:]\n",
        "    paramaters = prediction_model.paramaters\n",
        "\n",
        "    # get generated text\n",
        "    prediction = generator(input_text=starting_text, \n",
        "                            prediction_model=prediction_model, \n",
        "                            precision_reduction=precision_reduction, \n",
        "                            temperature=temperature,\n",
        "                            num_steps=num_generation_steps, \n",
        "                            print_result=print_result)                                   \n",
        "   \n",
        "    # define formatting rules\n",
        "    split_on = ['?', '.', ',', ';', '!', ':']\n",
        "    splits = '([' + ''.join(split_on) + '])'\n",
        "    split_lines_prediction = re.split(splits, prediction)\n",
        "\n",
        "    # format output\n",
        "    output = ''\n",
        "    for line in split_lines_prediction:\n",
        "\n",
        "        # capitalize first word of each line   \n",
        "        if len(line) >= 1:\n",
        "            line_update = line[0].upper()  \n",
        "\n",
        "            # add capitalized letter to remainder of line\n",
        "            if len(line) >= 2:\n",
        "                line_update += line[1:]\n",
        "        else:\n",
        "            line_update = ''\n",
        "                \n",
        "        # update output text\n",
        "        if (len(line_update) >= 1 and line_update[-1] in split_on) \\\n",
        "          or (len(line_update) >= 2 and line_update[-2:] == '\\n'):\n",
        "                output = ''.join([output, line_update])\n",
        "        else:\n",
        "            output = '\\n'.join([output, line_update])\n",
        "\n",
        "    \n",
        "    return output + '... '"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO27cbff77C4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJq0R9gcV1Mc"
      },
      "source": [
        "Set paramaters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR8k8bziVqzw",
        "outputId": "6288a03b-f201-4717-c4e9-cdedfa9ae797"
      },
      "source": [
        "# create paramaters object\n",
        "paramaters = Paramaters(use_gdrive=USE_GDRIVE, use_anvil=USE_ANVIL, \n",
        "                        author=AUTHOR, data_files=DATA_FILES,\n",
        "                        use_probability_layers=USE_PROBABILITY_LAYERS,\n",
        "                        use_word_path=USE_WORD_PATH, use_electra=USE_ELECTRA)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJkLK0N8Bm-"
      },
      "source": [
        "Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDSYqGeR78qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b5d5110-07a8-40bf-b119-4f30f219f62b"
      },
      "source": [
        "require_fresh_process = False\n",
        "\n",
        "try:\n",
        "    # check if prepared datasets already in memory\n",
        "    assert(require_fresh_process is False)\n",
        "    assert(len(X_data_list) > 0)\n",
        "    print('dataset already loaded')\n",
        "\n",
        "except:\n",
        "    print('Preparing dataset')\n",
        "    cleaned_data_list = input_pipeline(paramaters=paramaters, \n",
        "                                       fresh_process=require_fresh_process)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing dataset\n",
            "loaded saved pre-processed data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigiJJno8Hm3"
      },
      "source": [
        "Initialize Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Zd591P8Kvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05b12584-0e96-47b8-b50d-a66dbf752ba9"
      },
      "source": [
        "model = GenerationModel(paramaters=paramaters)\n",
        "\n",
        "# build model by running an element through it\n",
        "input = convert_to_input(' ', paramaters=model.paramaters)\n",
        "temp = model(input, initial_states=None, stateful=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:2281: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAjRSZ4WaDD"
      },
      "source": [
        "Load Latest Training Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhcsgO3WZfK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d53d4ec4-b806-40ed-aa6c-2efb5112de59"
      },
      "source": [
        "load_checkpoint=True\n",
        "try:\n",
        "    assert(load_checkpoint is True)\n",
        "\n",
        "    # load checkpoint\n",
        "    model.checkpoint_manager.restore_or_initialize()\n",
        "    print('loaded checkpoint')\n",
        "\n",
        "except: \n",
        "    print('No matching checkpoints')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KIYAQj8OZx"
      },
      "source": [
        "Train Model\n",
        "\n",
        "*Training loop dynamically generates training data from the text using random crops of random lengths to improve predictions and reduce overfitting.  Sample outputs are shown preiodically to observe model improvements.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zacay__l8M4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb6cc15-3869-45e6-c627-20121141fb96"
      },
      "source": [
        "train_model_now = True\n",
        "\n",
        "learning_rate = 0.02\n",
        "num_epochs = 100\n",
        "batches_per_epoch = 60\n",
        "\n",
        "# note:\n",
        "# aim for loss < .10 on char only\n",
        "# aim for loss < .001 with word path\n",
        "\n",
        "# train model\n",
        "if train_model_now:\n",
        "    model, history = train_model(model, cleaned_data_list,\n",
        "                                num_epochs=num_epochs, \n",
        "                                learning_rate=learning_rate,\n",
        "                                batches_per_epoch=batches_per_epoch)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFowEIJgUHN8"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQK0uZ4jUGbf"
      },
      "source": [
        "save_model_now = True\n",
        "\n",
        "if save_model_now:\n",
        "    save_model(model, paramaters=model.paramaters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikV04Jc_85RI"
      },
      "source": [
        "## Test Output: Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpDzr0x81-3"
      },
      "source": [
        "starting_text = ''#AI is becoming accessible'\n",
        "precision_reduction = 0 \n",
        "temperature=1,\n",
        "\n",
        "# NOTE: if using word path, recommeded using  >= paramaters._num_trailing_words \n",
        "# words in starting text\n",
        "\n",
        "gen = generate_text(starting_text, \n",
        "                    prediction_model=model,\n",
        "                    precision_reduction=precision_reduction,\n",
        "                    temperature=temperature,\n",
        "                    print_result=True,\n",
        "                    num_generation_steps=300)\n",
        "\n",
        "print(gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L13TslMTcCo_"
      },
      "source": [
        "## Web App"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-75YmvdqmG"
      },
      "source": [
        "Anvil Web App Server Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS26o45XdoiI"
      },
      "source": [
        "if model.paramaters._use_anvil:\n",
        "\n",
        "    # enable connection\n",
        "    !pip install -q anvil-uplink\n",
        "    import anvil.server\n",
        "    anvil.server.connect(self.anvil_code)\n",
        "\n",
        "\n",
        "    @anvil.server.callable\n",
        "    def anvil_callable(starting_text, \n",
        "                       precision_reduction=0,\n",
        "                       temperature=1,\n",
        "                       paramaters=PARAMATERS, ####### not used -- remove\n",
        "                       prediction_tokenizer=PARAMATERS._character_tokenizer, ####### not used -- remove\n",
        "                       prediction_model=model,\n",
        "                       print_result=True,\n",
        "                       author='assorted',\n",
        "                       num_generation_steps=150):\n",
        "        \n",
        "        paramaters = prediction_model.paramaters\n",
        "\n",
        "        if paramaters._use_word_path:\n",
        "            num_generation_steps = max(1, num_generation_steps // len(starting_text))\n",
        "\n",
        "        return generate_text(starting_text=starting_text, \n",
        "                             precision_reduction=precision_reduction,\n",
        "                             temperature=temperature,\n",
        "                             prediction_model=prediction_model,\n",
        "                             print_result=print_result,\n",
        "                             num_generation_steps=num_generation_steps)\n",
        "\n",
        "    # start persistent connection to server\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}