{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mo_nonlinear_text_gen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPc3arbFPRmkNsLJOTL+5cb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Text-Generation/blob/main/Mo_nonlinear_text_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uQyxxivFD7r"
      },
      "source": [
        "## Text Generation RNN\r\n",
        "\r\n",
        "This program constructs a character-level sequence model to generate text according to a character distribution learned from the dataset. \r\n",
        "\r\n",
        "- Try my web app implementation at www.communicatemission.com/ml-projects#text_generation. (Currently, only the standard model is implemented in the app)\r\n",
        "- See more at https://github.com/mvenouziou/Project-Text-Generation.\r\n",
        "\r\n",
        "- See credits /attributions below\r\n",
        "\r\n",
        "The code implements two different model architectures: \"linear\" and \"nonlinear.\"\r\n",
        "The linear model uses character-level embeddings to form the model. The nonlinear model adds a parallel word level embedding network, which is merged with the character embedding model. \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**What's New?**\r\n",
        "*(Surely others have created similar models, however I personally have not seen them at the time of coding this. Citations are below for content I have seen elsewhere.)*\r\n",
        "\r\n",
        "- Option to implement either the standard linear model architecture (see credits below) or nonlinear architectures.\r\n",
        "\r\n",
        "- Nonlinear model architecture uses parallel RNN's for word-level embeddings and character-level embeddings. \r\n",
        "\r\n",
        "- Manage RNN statefulness for independent data sources. The linear model (credited below) codes' approach to statefulness imposes a dependence relation between samples / batches. This model implements the ability to treat independent works (individual poems, books, authors, etc.) as truly independent samples by resetting RNN states and shuffling independent data sources.\r\n",
        "\r\n",
        "- Load and prepare data from multiple CSV and text files. Each rows from a CSV and each complete TXT file are treated as independent data sources. (CSV data prep accepts titles and content.) \r\n",
        "\r\n",
        "- Parameters to perturb learned probabilties in final generation function, to add extra variety to generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzU2QxoMFF7d"
      },
      "source": [
        "---\r\n",
        "**Credits / Citations / Attributions:**\r\n",
        "\r\n",
        "**Linear Model and Shared Code** \r\n",
        "\r\n",
        "Other than items noted in previous sections, this python code and linear model structure is based heavily on Imperial College London's Coursera course, \"Customising your models with Tensorflow 2\" *(https://www.coursera.org/learn/customising-models-tensorflow2)* and the Tensorflow RNN text generation documentation *(https://www.tensorflow.org/tutorials/text/text_generation?hl=en).*\r\n",
        "\r\n",
        "\r\n",
        "**Nonlinear Model:**   \r\n",
        "\r\n",
        "This utilizes the pretrained embeddings:\r\n",
        "-  Small BERT word embeddings from Tensorflow Hub, (*credited to Iulia Turc, Ming-Wei Chang, Kenton Lee, Kristina Toutanova's paper \"Well-Read Students Learn Better: On the Importance of Pre-training Compact Models.\" *https://tfhub.dev/google/collections/bert/1)*\r\n",
        "- ELECTRA-Small++ from Tensorflow Hub, (*credited to Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning's paper \"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\" *https://hub.tensorflow.google.cn/google/electra_small/2)*\r\n",
        "\r\n",
        "**Web App:** \r\n",
        "\r\n",
        "The web app is built on the Anvil platform and (at the time of this writing) is hosted on Google Cloud server (CPU).\r\n",
        "\r\n",
        "**Datasets:**\r\n",
        "\r\n",
        "- *'robert_frost_collection.csv'* is a Kaggle dataset available at https://www.kaggle.com/archanghosh/robert-frost-collection. Any other datasets used are public domain works available from Project Gutenberg https://www.gutenberg.org.\r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**About**\r\n",
        "\r\n",
        "Find me online at:\r\n",
        "- LinkedIn: https://www.linkedin.com/in/movenouziou/ \r\n",
        "- GitHub: https://github.com/mvenouziou\r\n",
        "\r\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1EW3w1GvDLx"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\r\n",
        "# ML design\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "!pip install -q tensorflow-text\r\n",
        "import tensorflow_text as text  # text processing / required for BERT encoder\r\n",
        "import tensorflow_hub as hub  # for BERT encoder\r\n",
        "\r\n",
        "# TensorBoard notebook extension\r\n",
        "%load_ext tensorboard\r\n",
        "import datetime, os\r\n",
        "\r\n",
        "# data handling\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import string\r\n",
        "import random\r\n",
        "import re\r\n",
        "\r\n",
        "# file management\r\n",
        "import os\r\n",
        "import bz2\r\n",
        "import pickle\r\n",
        "import _pickle as cPickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjhCdyMaX1Wh",
        "outputId": "fb4b6376-6e6b-438c-ec94-f69a34a6be7a"
      },
      "source": [
        "# 3rd party integrations\r\n",
        "\r\n",
        "# Mount Google Drive:\r\n",
        "USE_GDRIVE = True\r\n",
        "if USE_GDRIVE:\r\n",
        "    GDRIVE_DIR = '/content/gdrive/'\r\n",
        "    from google.colab import drive\r\n",
        "    drive.mount(GDRIVE_DIR)\r\n",
        "\r\n",
        "# Anvil's web app server\r\n",
        "USE_ANVIL = False  \r\n",
        "!pip install -q anvil-uplink\r\n",
        "import anvil.server\r\n",
        "\r\n",
        "if USE_ANVIL:\r\n",
        "    anvil.server.connect('53NFXI7IX7IE233XQTVJDXUM-PUGRV2WON2LETWBG')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzpBe-DQv4Nr"
      },
      "source": [
        "# GLOBAL PARAMATERS\r\n",
        "\r\n",
        "NUM_TRAILING_WORDS = 5  # for word model path\r\n",
        "PADDED_EXAMPLE_LENGTH = 500  # for character model path\r\n",
        "BATCH_SIZE = 32\r\n",
        "USE_ELECTRA = True"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3iL0mYZcgEL"
      },
      "source": [
        "# set author / file paths\r\n",
        "AUTHOR = 'tests'\r\n",
        "\r\n",
        "# model structure\r\n",
        "USE_WORD_PATH = True\r\n",
        "if USE_WORD_PATH:\r\n",
        "    AUTHOR += '_words_model/'\r\n",
        "if USE_ELECTRA:\r\n",
        "    AUTHOR += 'electra/'\r\n",
        "else: PATH_EXTENSION = ''\r\n",
        "\r\n",
        "# saving models/ checkpoints\r\n",
        "# (Google Drive)\r\n",
        "FILEPATH = GDRIVE_DIR + 'MyDrive/Colab_Notebooks/models/text_generation/' + AUTHOR\r\n",
        "CHECKPOINT_DIR = FILEPATH + '/checkpoints/'\r\n",
        "PREDICTION_MODEL_DIR = FILEPATH + '/prediction_model/'\r\n",
        "TRAINING_MODEL_DIR = FILEPATH + '/training_model/'\r\n",
        "PROCESSED_DATA_DIR = FILEPATH + '/proc_data/'\r\n",
        "TENSORBOARD_DIR = CHECKPOINT_DIR + '/logs/'\r\n",
        "#os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\r\n",
        "\r\n",
        "# online dataset repository\r\n",
        "DATASETS_DIR = 'https://raw.githubusercontent.com/mvenouziou/text_generator/main/'\r\n",
        "DATA_FILES = ['robert_frost_collection.csv']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39FgKokRXBe1"
      },
      "source": [
        "### Define Encoders / Tokenizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jl6b4S8zeFdi"
      },
      "source": [
        "Character-Level"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zN13GwsjyB9z"
      },
      "source": [
        "def create_character_tokenizer():\r\n",
        "    \"\"\"\r\n",
        "    This function takes a list of strings as its argument. It should create \r\n",
        "    and return a Tokenizer according to the above specifications. \r\n",
        "    \"\"\"\r\n",
        "    \r\n",
        "    char_tokens = string.printable\r\n",
        "    filters = '#$%&()*+-/<=>@[]^_`{|}~\\t'\r\n",
        "\r\n",
        "    # Initialize standard keras tokenizer\r\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\r\n",
        "                    num_words=None,  \r\n",
        "                    filters=filters,\r\n",
        "                    lower=False,  # conversion to lowercase letters\r\n",
        "                    char_level=True,\r\n",
        "                    oov_token=None,  # drop unknown characters\r\n",
        "                    )\r\n",
        "    \r\n",
        "    # fit tokenizer\r\n",
        "    tokenizer.fit_on_texts(char_tokens)\r\n",
        "\r\n",
        "    return tokenizer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsqQHC1D6Uwp"
      },
      "source": [
        "def make_padded_array(text_blocks, tokenizer=None, max_len=PADDED_EXAMPLE_LENGTH):\r\n",
        "    # Tokenizes and applies padding for uniform length\r\n",
        "\r\n",
        "    # load tokenizer if one is not supplied\r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "    # tokenize\r\n",
        "    token_blocks = tokenizer.texts_to_sequences(text_blocks)\r\n",
        "\r\n",
        "    # zero padding\r\n",
        "    padded_blocks = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "                        sequences=token_blocks,  # dataset\r\n",
        "                        maxlen=max_len, \r\n",
        "                        dtype='int32', \r\n",
        "                        padding='pre',\r\n",
        "                        truncating='pre', \r\n",
        "                        value=0.0\r\n",
        "                        )\r\n",
        "    \r\n",
        "    return padded_blocks"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umqkR0FveIBt"
      },
      "source": [
        "Word-Level (BERT or Electra pre-trained embedding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAC-8wxwtxDD"
      },
      "source": [
        "def get_bert_encoder(seq_length=NUM_TRAILING_WORDS, use_electra=USE_ELECTRA):\r\n",
        "\r\n",
        "    # Word Embeddings path (bert encoder)\r\n",
        "    if use_electra:\r\n",
        "        encoder_url = 'https://tfhub.dev/google/electra_small/2'\r\n",
        "    else:\r\n",
        "        encoder_url = 'https://tfhub.dev/tensorflow/' \\\r\n",
        "                            + 'small_bert/bert_en_uncased_L-2_H-128_A-2/1'\r\n",
        "    preprocessor_url = 'https://tfhub.dev/tensorflow/' \\\r\n",
        "                        + 'bert_en_uncased_preprocess/3'\r\n",
        "                \r\n",
        "    # preprocessing layer\r\n",
        "    # get BERT components\r\n",
        "    preprocessor = hub.load(preprocessor_url)\r\n",
        "    bert_tokenizer = hub.KerasLayer(preprocessor.tokenize,\r\n",
        "                                    name='bert_tokenizer')\r\n",
        "    bert_packer = hub.KerasLayer(preprocessor.bert_pack_inputs,\r\n",
        "                                 arguments=dict(seq_length=seq_length),\r\n",
        "                                 name='bert_input_packer')\r\n",
        "    bert_encoder = hub.KerasLayer(encoder_url, trainable=False, \r\n",
        "                             name='BERT_encoder')\r\n",
        "    \r\n",
        "    return bert_tokenizer, bert_packer, bert_encoder"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8EYMa9-XKJY"
      },
      "source": [
        "### Define Data Pre-processors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccqJnhind4a_"
      },
      "source": [
        "Load and Clean Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uzwvJaIXQ3q"
      },
      "source": [
        "# Function: loader for .csv files\r\n",
        "def prepare_csv(filename, datasets_dir=DATASETS_DIR, \r\n",
        "                content_columns=['Name', 'Content'], shuffle_rows=True):\r\n",
        "    \r\n",
        "    # load data into DataFrame\r\n",
        "    dataframe = pd.read_csv(datasets_dir + filename).dropna()\r\n",
        "    \r\n",
        "    # extract titles and content\r\n",
        "    # note: column headings must match those below\r\n",
        "    if 'Name ' in dataframe.columns:  # required for the Robert Frost set\r\n",
        "        dataframe.rename(columns={'Name ':'Name'})\r\n",
        "    \r\n",
        "    # prepare titles\r\n",
        "    try: \r\n",
        "        dataframe['Name'] = dataframe['Name'].apply(\r\n",
        "                            lambda x: x.upper() + ':\\n')\r\n",
        "    except:\r\n",
        "        # no titles found\r\n",
        "        content_columns = ['Content']\r\n",
        "\r\n",
        "    # prepare content\r\n",
        "    dataframe['Content'] = dataframe['Content'].apply(\r\n",
        "                    lambda x: x + '\\n')\r\n",
        "\r\n",
        "    # restrict dataset\r\n",
        "    dataframe = dataframe[content_columns]\r\n",
        "\r\n",
        "    # shuffle entries (rows)\r\n",
        "    if shuffle_rows:\r\n",
        "        dataframe = dataframe.sample(frac=1)\r\n",
        "    \r\n",
        "    # data cleanup\r\n",
        "    dataframe = dataframe[content_columns]\r\n",
        "    \r\n",
        "    # merge desired text columns\r\n",
        "    dataframe['merge'] = dataframe[content_columns[0]]\r\n",
        "    for i in range(1, len(content_columns)):\r\n",
        "        dataframe['merge'] = dataframe['merge'] + dataframe[content_columns[i]]\r\n",
        "\r\n",
        "    # convert to list of strings\r\n",
        "    data_list = dataframe['merge'].tolist()\r\n",
        "    \r\n",
        "    return data_list   \r\n",
        "\r\n",
        "\r\n",
        "# Function: Load and standardize data files\r\n",
        "def load_parse(data_list, display_samples=True):  \r\n",
        "\r\n",
        "    # remove paragraph / line marks and split up words  \r\n",
        "    tokenizer = text.WhitespaceTokenizer()\r\n",
        "\r\n",
        "    # tokenize data (outputs bytestrings)\r\n",
        "    cleaned_list_byte = [tokenizer.tokenize(data).numpy() for data in data_list]\r\n",
        "\r\n",
        "    # convert data back to string format\r\n",
        "    num_entries = len(cleaned_list_byte)\r\n",
        "\r\n",
        "    clean_list = [' '.join(map(lambda x: x.decode(), cleaned_list_byte[i])) \r\n",
        "                    for i in range(num_entries)]\r\n",
        "\r\n",
        "    # Display some text samples\r\n",
        "    if display_samples:\r\n",
        "        num_samples = 5\r\n",
        "        inx = np.random.choice(len(clean_list), num_samples, replace=False)\r\n",
        "        for example in np.array(clean_list)[inx]:\r\n",
        "            print(example)\r\n",
        "            print()\r\n",
        "\r\n",
        "        print('len(text_chunks):', len(clean_list))\r\n",
        "\r\n",
        "    return clean_list"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1HAd-j7J9eq"
      },
      "source": [
        "def create_input_target_blocks(full_examples, tokenizer=None,\r\n",
        "                               max_len=PADDED_EXAMPLE_LENGTH,\r\n",
        "                               num_words=NUM_TRAILING_WORDS):\r\n",
        "    # converts text into sliding n-grams of words and characters\r\n",
        "    # returning input / target sets\r\n",
        "\r\n",
        "    # helper function to create word-level inputs\r\n",
        "    def update_word_char_lists(text, chars_list, words_list):\r\n",
        "        words_input = text.split(' ')  # separate words\r\n",
        "        words_input = words_input[-num_words-1: -1]  # get trailing words\r\n",
        "\r\n",
        "        # convert words to string (tensor)\r\n",
        "        words_input = ' '.join(words_input)\r\n",
        "\r\n",
        "        # add values to lists\r\n",
        "        chars_list.append(text)\r\n",
        "        words_list.append([words_input])\r\n",
        "        \r\n",
        "        return None\r\n",
        "\r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "    blocks = []\r\n",
        "    for example in full_examples:      \r\n",
        "\r\n",
        "        char_block = []\r\n",
        "        word_block = []\r\n",
        "        example_length = len(example)\r\n",
        "\r\n",
        "        # small blocks at start (will be zero-padded later)\r\n",
        "        leading_characters = 1  # min chars to seed predictions\r\n",
        "        for i in range(leading_characters, example_length - max_len - 1):\r\n",
        "            text = example[: i]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "\r\n",
        "        # full length blocks\r\n",
        "        for i in range(example_length - max_len - 1):\r\n",
        "            # create n-gram\r\n",
        "            text = example[i: max_len + i]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "\r\n",
        "        # small blocks at end (will be zero-padded later)\r\n",
        "        for i in range(example_length - max_len - 1, example_length-1):\r\n",
        "            text = example[i: ]\r\n",
        "            update_word_char_lists(text, char_block, word_block)\r\n",
        "    \r\n",
        "        # tokenize and add pre-padding\r\n",
        "        char_block = make_padded_array(char_block, tokenizer, max_len=max_len)\r\n",
        "\r\n",
        "        # separate into inputs and targets\r\n",
        "        inputs_char = char_block[:, :-1]\r\n",
        "        targets_char = char_block[:, 1:]\r\n",
        "\r\n",
        "        # update blocks\r\n",
        "        word_block = np.array(word_block)\r\n",
        "        blocks.append((inputs_char, word_block, targets_char))\r\n",
        "\r\n",
        "    return blocks"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb9aQIGZu6te"
      },
      "source": [
        "# Function: data prep to create stateful RNN batches\r\n",
        "# note: This will be applied separately on each example text, \r\n",
        "# so that RNN can reset internal state / distinguish between unrelated passages\r\n",
        "# note: This code is taken directly from Imperial College London's \r\n",
        "# Coursera course cited above\r\n",
        "\r\n",
        "def preprocess_stateful(char_input, word_input, target, batch_size=BATCH_SIZE):\r\n",
        "\r\n",
        "    # Prepare input and output arrays for training the stateful RNN\r\n",
        "    num_examples = char_input.shape[0]\r\n",
        "\r\n",
        "    # adjust for batch size to divide evenly into sample size\r\n",
        "    num_processed_examples = num_examples - (num_examples % batch_size)\r\n",
        "    input_cropped = char_input[:num_processed_examples]\r\n",
        "    target_cropped = target[:num_processed_examples]\r\n",
        "\r\n",
        "    # separate out samples so rows of data match up across epochs\r\n",
        "    # 'steps' measures how to space them out\r\n",
        "    steps = num_processed_examples // batch_size  \r\n",
        "\r\n",
        "    # define reordering\r\n",
        "    inx = np.empty((0,), dtype=np.int32)  # initialize empty array object\r\n",
        "    \r\n",
        "    for i in range(steps):\r\n",
        "        inx = np.concatenate((inx, i + np.arange(0, num_processed_examples, \r\n",
        "                                                    steps)))\r\n",
        "\r\n",
        "    # reorder the data\r\n",
        "    input_char_stateful = input_cropped[inx]\r\n",
        "    input_word_stateful = word_input[inx]\r\n",
        "    target_seq_stateful = target_cropped[inx]\r\n",
        "\r\n",
        "    return input_char_stateful, input_word_stateful, target_seq_stateful"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYlzYCN71orw"
      },
      "source": [
        "Input Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuOSV4SWyyoS"
      },
      "source": [
        "def input_pipeline(data_files=DATA_FILES, verbose=True, batch_size=BATCH_SIZE, \r\n",
        "                   max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS,\r\n",
        "                   datasets_dir=DATASETS_DIR, saved_proc_dir=PROCESSED_DATA_DIR):\r\n",
        "\r\n",
        "    # load previously processed data (pbz2 compressed file format)\r\n",
        "    try:    \r\n",
        "        with bz2.open(saved_proc_dir + 'datafiles.pbz2', 'rb') as file:\r\n",
        "            data_dict = cPickle.load(file)\r\n",
        "\r\n",
        "        X_data_list = data_dict['X_data_list']\r\n",
        "        Y_data_list = data_dict['Y_data_list']\r\n",
        "\r\n",
        "        print('loaded saved pre-processed data')\r\n",
        "\r\n",
        "    except:       \r\n",
        "\r\n",
        "        # load data file\r\n",
        "        data_list = []\r\n",
        "        for filename in data_files:\r\n",
        "\r\n",
        "            # check file extension and select loader (csv or txt)\r\n",
        "            _, file_extension = os.path.splitext(filename)     \r\n",
        "\r\n",
        "            if file_extension == '.csv':   \r\n",
        "                data = prepare_csv(filename, \r\n",
        "                                datasets_dir=datasets_dir, \r\n",
        "                                content_columns=['Name', 'Content'], \r\n",
        "                                shuffle_rows=True)\r\n",
        "                \r\n",
        "            else: # file_extension == '.txt':\r\n",
        "                with open(filepath + '/' + filename, 'r', encoding='utf-8') as file:\r\n",
        "                    data = file.readlines()\r\n",
        "\r\n",
        "            # add extracted list of texts to data list\r\n",
        "            data_list += data\r\n",
        "\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: data_list created')\r\n",
        "        \r\n",
        "        # clean data\r\n",
        "        clean_list = load_parse(data_list, display_samples=False)\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: clean_list created')\r\n",
        "        \r\n",
        "        # preprocess data\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "        blocks = create_input_target_blocks(full_examples=clean_list, \r\n",
        "                                            tokenizer=tokenizer,\r\n",
        "                                            max_len=max_len,\r\n",
        "                                            num_words=num_words)\r\n",
        "        if verbose:\r\n",
        "            print('PROGRESS: blocks created')\r\n",
        "        \r\n",
        "        # create separate input / target pairs for each block\r\n",
        "        X_data_list = []\r\n",
        "        Y_data_list = []\r\n",
        "\r\n",
        "        i=0\r\n",
        "        for block in blocks:\r\n",
        "            if i % 10 == 0:\r\n",
        "                print(f'PROGRESS: processing block {i} of {len(blocks)}')\r\n",
        "\r\n",
        "            char_input = block[0] \r\n",
        "            word_input = block[1] \r\n",
        "            target = block[2]\r\n",
        "\r\n",
        "            input_char_stateful, input_word_stateful, target_seq_stateful = \\\r\n",
        "                                    preprocess_stateful(char_input=char_input, \r\n",
        "                                                        word_input=word_input, \r\n",
        "                                                        target=target, \r\n",
        "                                                        batch_size=batch_size)\r\n",
        "\r\n",
        "            # group for model input\r\n",
        "            X = [input_char_stateful, input_word_stateful]\r\n",
        "            Y = target_seq_stateful\r\n",
        "\r\n",
        "            X_data_list.append(X)\r\n",
        "            Y_data_list.append(Y)\r\n",
        "\r\n",
        "            # advance index\r\n",
        "            i += 1\r\n",
        "\r\n",
        "        # save file (pbz2 compressed file format)\r\n",
        "        with bz2.BZ2File(saved_proc_dir + 'datafiles.pbz2', 'wb') as sfile:\r\n",
        "            cPickle.dump({'X_data_list': X_data_list, \r\n",
        "                          'Y_data_list': Y_data_list}, sfile)\r\n",
        "\r\n",
        "    return X_data_list, Y_data_list"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOpjX19aWvU6"
      },
      "source": [
        "### Define Models and Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtYOFDwndkK4"
      },
      "source": [
        "Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFYArtLttwEF"
      },
      "source": [
        "# Function: Model Definition\r\n",
        "def get_training_model(use_word_path=USE_WORD_PATH,\r\n",
        "                       verbose=True,\r\n",
        "                       batch_size=BATCH_SIZE, \r\n",
        "                       padded_examples=PADDED_EXAMPLE_LENGTH,\r\n",
        "                       num_words=NUM_TRAILING_WORDS,\r\n",
        "                       use_electra=USE_ELECTRA):\r\n",
        "    \r\n",
        "    \"\"\" Defines and compiles our stateful RNN model. \r\n",
        "    Note: batch size is required argument for stateful RNN. \"\"\"\r\n",
        "    \r\n",
        "    from keras.layers import Input, Embedding, Concatenate, Dense, GRU,\\\r\n",
        "                             Average, AveragePooling1D, Dropout, \\\r\n",
        "                             BatchNormalization, Lambda\r\n",
        "\r\n",
        "    # parameters\r\n",
        "    vocab_size = len(create_character_tokenizer().word_index) + 1\r\n",
        "    embedding_dim = 128\r\n",
        "    merge_dim = 128\r\n",
        "\r\n",
        "    # pre-trained encoder\r\n",
        "    bert_tokenizer, bert_packer, bert_encoder = \\\r\n",
        "            get_bert_encoder(use_electra=use_electra)\r\n",
        "    \r\n",
        "    # Build model\r\n",
        "    # define input shapes\r\n",
        "    input_1 = Input(shape=(None, ), #(padded_examples-1, ), \r\n",
        "                    batch_size=batch_size,\r\n",
        "                    dtype=tf.int32, \r\n",
        "                    name='char_input')\r\n",
        "    \r\n",
        "    input_2 = Input(shape=(), \r\n",
        "                    batch_size=batch_size,\r\n",
        "                    dtype=tf.string, \r\n",
        "                    name='word_input')\r\n",
        "\r\n",
        "    # travel individual paths\r\n",
        "    # Character Level Path\r\n",
        "    # ## Char: Embedding\r\n",
        "    x1 = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \r\n",
        "                   mask_zero=True, batch_input_shape=(batch_size, None),\r\n",
        "                   name='char_embedding',)(input_1)\r\n",
        "\r\n",
        "    # ## Char: GRU 1\r\n",
        "    x1 = GRU(units=embedding_dim, stateful=True, \r\n",
        "             return_sequences=True, name='char_GRU_1',)(x1)\r\n",
        "    x1 = Dropout(rate=.10, name='char_Dropout_1')(x1)\r\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_1')(x1)\r\n",
        "    \r\n",
        "    # ## Char: GRU Final --  must use output_dim = merge_dim!\r\n",
        "    x1 = GRU(units=merge_dim, stateful=True, \r\n",
        "             return_sequences=True, name='char_GRU_final',)(x1)\r\n",
        "    x1 = Dropout(rate=.10, name='char_Dropout_final')(x1)\r\n",
        "    x1 = BatchNormalization(name='char_Batch_Norm_final')(x1)\r\n",
        "\r\n",
        "    # Word Encoding Path\r\n",
        "    if use_word_path:\r\n",
        "        \r\n",
        "        x2 = bert_tokenizer(input_2)  # tokenize\r\n",
        "        x2 = bert_packer([x2])  # pack inputs for encoder\r\n",
        "        x2 = bert_encoder(x2)['sequence_output'] # encoding\r\n",
        "\r\n",
        "        # ## Word: GRU 1\r\n",
        "        x2 = GRU(units=32, stateful=True, \r\n",
        "                 return_sequences=True, name='word_GRU_1',)(x2)\r\n",
        "        x2 = Dropout(rate=.10, name='word_Dropout_1')(x2)\r\n",
        "        x2 = BatchNormalization(name='word_Batch_Norm_1')(x2)\r\n",
        "\r\n",
        "        # ## Word: Required conversion to valid merge output dim = merge_dim!\r\n",
        "        x2 = Dense(units=num_words, activation=None, \r\n",
        "                   name='word_Dense_pre_final')(x2)\r\n",
        "        x2 = AveragePooling1D(pool_size=5, padding='same', \r\n",
        "                              name='word_pooling_final')(x2)\r\n",
        "        x2 = Dense(units=merge_dim, activation=None, \r\n",
        "                   name='word_Dense_final')(x2)\r\n",
        "\r\n",
        "        # Merge Paths\r\n",
        "        x = Average(name='merged_layers')([x1, x2])\r\n",
        "\r\n",
        "    else:  # update variable id to match next step\r\n",
        "        x = Lambda(lambda x: x, name='rename_variable')(x1)  \r\n",
        "    \r\n",
        "    # Final GRU layer\r\n",
        "    x = GRU(units=embedding_dim, stateful=True, \r\n",
        "            return_sequences=True, name='GRU_OUTPUT')(x)          \r\n",
        "\r\n",
        "    # Character prediction (logits)\r\n",
        "    outputs = Dense(units=vocab_size, activation=None, \r\n",
        "                    name='Decoding')(x)       \r\n",
        "    \r\n",
        "    # create model\r\n",
        "    model = keras.Model(inputs=[input_1, input_2], outputs=outputs)\r\n",
        "\r\n",
        "    if verbose:\r\n",
        "        print(model.summary())\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5YFw5SJut1K"
      },
      "source": [
        "Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7pwjfSAusua"
      },
      "source": [
        "def get_prediction_model(trained_model=None, use_word_path=USE_WORD_PATH,\r\n",
        "                         padded_examples=PADDED_EXAMPLE_LENGTH, verbose=False):\r\n",
        "    \"\"\" enforces batch size = 1, only returns last character prediction\r\n",
        "     and loads any saved weights \"\"\"\r\n",
        "\r\n",
        "    # set paramaters\r\n",
        "    batch_size=1\r\n",
        "\r\n",
        "    # create model\r\n",
        "    prediction_model = get_training_model(batch_size=batch_size, \r\n",
        "                                          use_word_path=use_word_path,\r\n",
        "                                          padded_examples=padded_examples,\r\n",
        "                                          verbose=verbose)\r\n",
        "\r\n",
        "\r\n",
        "    # load weights from pre-trained model\r\n",
        "    if trained_model is not None:        \r\n",
        "        trained_weights = trained_model.get_weights()\r\n",
        "        prediction_model.set_weights(trained_weights)\r\n",
        "\r\n",
        "    return prediction_model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEC8q6pYYBue"
      },
      "source": [
        "Compiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loz4fJP1YA78"
      },
      "source": [
        "def compile_model(model, learning_rate):\r\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adamax(\r\n",
        "                                learning_rate=learning_rate),\r\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "                                                    from_logits=True),\r\n",
        "                  metrics=['sparse_categorical_accuracy', \r\n",
        "                        'sparse_categorical_crossentropy'],\r\n",
        "                 )\r\n",
        "    \r\n",
        "    return model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW6KD4XoYoQG"
      },
      "source": [
        "Checkpoint Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K09LDfJYnla"
      },
      "source": [
        "# checkpoint manager\r\n",
        "def create_checkpoint_manager(model, checkpoint_dir=CHECKPOINT_DIR):\r\n",
        "\r\n",
        "    checkpoint = tf.train.Checkpoint(model=model)\r\n",
        "\r\n",
        "    checkpoint_manager = tf.train.CheckpointManager(\r\n",
        "                            checkpoint=checkpoint, \r\n",
        "                            directory=checkpoint_dir, \r\n",
        "                            max_to_keep=4, \r\n",
        "                            keep_checkpoint_every_n_hours=None,\r\n",
        "                            checkpoint_name='ckpt', \r\n",
        "                            step_counter=None, \r\n",
        "                            checkpoint_interval=None,\r\n",
        "                            init_fn=None\r\n",
        "                            )\r\n",
        "    \r\n",
        "    return checkpoint, checkpoint_manager"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWkTKUaXZbm"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYZWbiojXXpS"
      },
      "source": [
        "# Function: Train model\r\n",
        "def train_model(model, X_data_list, Y_data_list,\r\n",
        "                num_epochs=1, \r\n",
        "                num_datasets_to_use=None,\r\n",
        "                checkpoint=None, \r\n",
        "                checkpoint_manager=None,\r\n",
        "                learning_rate=0.001,\r\n",
        "                batch_size=BATCH_SIZE, \r\n",
        "                filepath=FILEPATH, \r\n",
        "                checkpoint_dir=CHECKPOINT_DIR,\r\n",
        "                logdir=TENSORBOARD_DIR):\r\n",
        "\r\n",
        "    # compile model\r\n",
        "    model = compile_model(model, learning_rate=learning_rate)\r\n",
        "\r\n",
        "    # set checkpoint manager\r\n",
        "    if checkpoint is None or checkpoint_manager is None:\r\n",
        "        checkpoint, checkpoint_manager = \\\r\n",
        "                    create_checkpoint_manager(model=model, \r\n",
        "                                              checkpoint_dir=checkpoint_dir)\r\n",
        "                    \r\n",
        "    # set callbacks\r\n",
        "    # TensorBoard callback\r\n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\r\n",
        "                                log_dir=logdir, histogram_freq=1,\r\n",
        "                                )\r\n",
        "    \r\n",
        "    # organize training data\r\n",
        "    num_blocks = len(X_data_list)\r\n",
        "    train_datasets_list = list(zip(X_data_list, Y_data_list)) \r\n",
        "\r\n",
        "    if num_datasets_to_use is None:  # if not specified, use all datasets\r\n",
        "        num_datasets_to_use = len(X_data_list)      \r\n",
        "    \r\n",
        "    # begin training loop\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "\r\n",
        "        print(f'Epoch: {epoch}')\r\n",
        "\r\n",
        "        # shuffle dataset order\r\n",
        "        random.shuffle(train_datasets_list)\r\n",
        "        print('shuffled datasets')\r\n",
        "\r\n",
        "        for i in range(num_datasets_to_use):\r\n",
        "            print(f'dataset: {i}')\r\n",
        "\r\n",
        "            # select dataset\r\n",
        "            data = train_datasets_list[i]\r\n",
        "            X = data[0]\r\n",
        "            Y = data[1]\r\n",
        "\r\n",
        "            # train model\r\n",
        "            history = model.fit(x=X, y=Y,\r\n",
        "                                shuffle=False,\r\n",
        "                                epochs=1,\r\n",
        "                                verbose=1,\r\n",
        "                                #callbacks=[tensorboard_callback],\r\n",
        "                                )\r\n",
        "            \r\n",
        "            # reset RNN hidden states\r\n",
        "            model.reset_states()\r\n",
        "\r\n",
        "            # save checkpoint\r\n",
        "            checkpoint_manager.save()\r\n",
        "\r\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvMNy3XTW2Be"
      },
      "source": [
        "### Define Implementation Functions\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuUcE7H4snz"
      },
      "source": [
        "def convert_to_input(last_token, trunc_text, \r\n",
        "                     prepare_words=USE_WORD_PATH,\r\n",
        "                     max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "                     num_words=NUM_TRAILING_WORDS):\r\n",
        "    \r\n",
        "    # words\r\n",
        "    if prepare_words:\r\n",
        "        words_input = trunc_text.split(' ')  # separate words\r\n",
        "        words_input = words_input[-num_words-1:-1]  # get trailing words\r\n",
        "        words_input = tf.constant(' '.join(trunc_text))  # convert to tensor\r\n",
        "    else:\r\n",
        "        words_input=tf.constant(' ')\r\n",
        "\r\n",
        "    # pad token sequence\r\n",
        "    inputs_char=tf.constant(last_token)\r\n",
        "    \"\"\"\r\n",
        "    length = max_len - 1\r\n",
        "    inputs_char = tf.keras.preprocessing.sequence.pad_sequences(\r\n",
        "                        sequences=inputs_char,  # dataset\r\n",
        "                        maxlen=length, \r\n",
        "                        dtype='int32', \r\n",
        "                        padding='pre',\r\n",
        "                        truncating='pre', \r\n",
        "                        value=0.0\r\n",
        "                        )\r\n",
        "    \"\"\"\r\n",
        "    # create separate input / target pairs for each block\r\n",
        "    X = [inputs_char, words_input]\r\n",
        "\r\n",
        "    return X"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq2atvRiGv-z"
      },
      "source": [
        "def generator(input_text, prediction_model, precision_reduction=0, \r\n",
        "              num_characters=250, tokenizer=None, \r\n",
        "              max_len=PADDED_EXAMPLE_LENGTH, num_words=NUM_TRAILING_WORDS, \r\n",
        "              print_result=True):\r\n",
        "\r\n",
        "    # get tokenizer (if not supplied)      \r\n",
        "    if tokenizer is None:\r\n",
        "        tokenizer = create_character_tokenizer()\r\n",
        "    \r\n",
        "    # initialize generated text\r\n",
        "    last_token =  tokenizer.texts_to_sequences([input_text])\r\n",
        "    trunc_text = input_text.upper() + ':\\n'\r\n",
        "    generated_text = []\r\n",
        "   \r\n",
        "    # text generation loop\r\n",
        "    initial_state = None\r\n",
        "    for _ in range(num_characters):\r\n",
        "\r\n",
        "        # prepare input for model\r\n",
        "        inputs = convert_to_input(last_token=last_token, \r\n",
        "                                  trunc_text=trunc_text,\r\n",
        "                                  max_len=max_len,\r\n",
        "                                  num_words=num_words)\r\n",
        "        \r\n",
        "        # pass forward final GRU layer state\r\n",
        "        GRU_layer = prediction_model.get_layer('GRU_OUTPUT')\r\n",
        "        GRU_layer.reset_states(initial_state)\r\n",
        "        \r\n",
        "        # run model and compute logits\r\n",
        "        output = prediction_model(inputs)\r\n",
        "        logits = output[:, -1, :]  # extract last character logits\r\n",
        "        logits = logits.numpy()\r\n",
        "       \r\n",
        "        # generate next character from logits distribution\r\n",
        "        # purturb probabilities (optional)\r\n",
        "        if precision_reduction != 0:\r\n",
        "            fuzz_factor = tf.random.normal(shape=logits.shape, mean=1, stddev=.2)\r\n",
        "            logits = logits * (1 + precision_reduction * fuzz_factor)\r\n",
        "\r\n",
        "        last_token = tf.random.categorical(logits=logits, num_samples=1)\r\n",
        "        last_token = last_token.numpy().tolist()\r\n",
        "        \r\n",
        "        # get input for next character prediction\r\n",
        "        input_text = tokenizer.sequences_to_texts(last_token)\r\n",
        "        input_text = input_text[0]\r\n",
        "\r\n",
        "        # record generated character\r\n",
        "        generated_text.append(input_text)\r\n",
        "\r\n",
        "        #  get GRU state for next character prediction\r\n",
        "        initial_state = GRU_layer.states[0].numpy()\r\n",
        "\r\n",
        "    # reset for next run\r\n",
        "    output_text = ''.join(generated_text)\r\n",
        "    \r\n",
        "    if print_result:\r\n",
        "        print(output_text)\r\n",
        "\r\n",
        "    return output_text"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5rFetpOUqwQ"
      },
      "source": [
        "Saving Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tf4x1q8UpfD"
      },
      "source": [
        "# Store trained model separate from checkpoints\r\n",
        "def save_model(model, model_dir):\r\n",
        "\r\n",
        "    # save model\r\n",
        "    model.save(model_dir)\r\n",
        "\r\n",
        "    # get tokenizer\r\n",
        "    prediction_tokenizer = create_character_tokenizer()\r\n",
        "    \r\n",
        "    # save tokenizer\r\n",
        "    with open(model_dir + 'tokenizer.pickle', 'wb') as file:\r\n",
        "        pickle.dump(prediction_tokenizer, file, pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "    return None"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oO27cbff77C4"
      },
      "source": [
        "# Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XJkLK0N8Bm-"
      },
      "source": [
        "Load and Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDSYqGeR78qg",
        "outputId": "9ab67809-f813-45a8-ac4c-20c87bc8d4e6"
      },
      "source": [
        "try:\r\n",
        "    if len(X_data_list) > 0:\r\n",
        "        print('dataset already loaded')\r\n",
        "except:\r\n",
        "    print('Preparing dataset')\r\n",
        "    X_data_list, Y_data_list = input_pipeline(DATA_FILES)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded saved pre-processed data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZigiJJno8Hm3"
      },
      "source": [
        "Initialize Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1Zd591P8Kvf",
        "outputId": "d5cf8122-139c-4dc0-d312-16c0fbf5cf0b"
      },
      "source": [
        "training_model = get_training_model(verbose=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "word_input (InputLayer)         [(32,)]              0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_tokenizer (KerasLayer)     (32, None, None)     0           word_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_input (InputLayer)         [(32, None)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bert_input_packer (KerasLayer)  {'input_word_ids': ( 0           bert_tokenizer[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "char_embedding (Embedding)      (32, None, 128)      12928       char_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "BERT_encoder (KerasLayer)       {'encoder_outputs':  13548801    bert_input_packer[0][0]          \n",
            "                                                                 bert_input_packer[0][1]          \n",
            "                                                                 bert_input_packer[0][2]          \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_1 (GRU)                (32, None, 128)      99072       char_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_GRU_1 (GRU)                (32, 5, 32)          27840       BERT_encoder[0][14]              \n",
            "__________________________________________________________________________________________________\n",
            "char_Dropout_1 (Dropout)        (32, None, 128)      0           char_GRU_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "word_Dropout_1 (Dropout)        (32, 5, 32)          0           word_GRU_1[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_1 (BatchNormali (32, None, 128)      512         char_Dropout_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_Batch_Norm_1 (BatchNormali (32, 5, 32)          128         word_Dropout_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "char_GRU_final (GRU)            (32, None, 128)      99072       char_Batch_Norm_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "word_Dense_pre_final (Dense)    (32, 5, 5)           165         word_Batch_Norm_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "char_Dropout_final (Dropout)    (32, None, 128)      0           char_GRU_final[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "word_pooling_final (AveragePool (32, 1, 5)           0           word_Dense_pre_final[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "char_Batch_Norm_final (BatchNor (32, None, 128)      512         char_Dropout_final[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "word_Dense_final (Dense)        (32, 1, 128)         768         word_pooling_final[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "merged_layers (Average)         (32, None, 128)      0           char_Batch_Norm_final[0][0]      \n",
            "                                                                 word_Dense_final[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "GRU_OUTPUT (GRU)                (32, None, 128)      99072       merged_layers[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Decoding (Dense)                (32, None, 101)      13029       GRU_OUTPUT[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 13,901,899\n",
            "Trainable params: 352,522\n",
            "Non-trainable params: 13,549,377\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfAjRSZ4WaDD"
      },
      "source": [
        "Load Latest Training Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuhcsgO3WZfK",
        "outputId": "0b7ebd8f-adf3-4c2f-c988-0daf8b03b4fe"
      },
      "source": [
        "try:\r\n",
        "    # load from checkpoint\r\n",
        "    checkpoint, checkpoint_manager = \\\r\n",
        "        create_checkpoint_manager(model=training_model, \r\n",
        "                                    checkpoint_dir=CHECKPOINT_DIR)\r\n",
        "\r\n",
        "    checkpoint_manager.restore_or_initialize()\r\n",
        "    print('loaded checkpoint')\r\n",
        "\r\n",
        "except:\r\n",
        "    print('No matching checkpoints')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KIYAQj8OZx"
      },
      "source": [
        "Train Model\r\n",
        "\r\n",
        "*Suggestion: overfitting can be a major problem where the model memorizes segments from the source material. Be careful to avoid this. A precision reduction factor of 0.0 - 1.5 can be used to partially compensate for this. (This paramater in my final prediction function randomly perturbs learned probabilities)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zacay__l8M4T",
        "outputId": "0430af95-a4e6-40a1-9a0e-b4f5331bf000"
      },
      "source": [
        "train_model_now=True\r\n",
        "if train_model_now:\r\n",
        "    num_epochs = 5\r\n",
        "\r\n",
        "    training_model = train_model(training_model, X_data_list, Y_data_list,\r\n",
        "                                num_epochs=num_epochs, \r\n",
        "                                num_datasets_to_use=None)  # note: num_datasets_to_use=None uses all of them"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "shuffled datasets\n",
            "dataset: 0\n",
            "30/30 [==============================] - 63s 2s/step - loss: 1.6342 - sparse_categorical_accuracy: 0.2655 - sparse_categorical_crossentropy: 4.0318\n",
            "dataset: 1\n",
            "60/60 [==============================] - 94s 2s/step - loss: 1.7425 - sparse_categorical_accuracy: 0.3567 - sparse_categorical_crossentropy: 3.4025\n",
            "dataset: 2\n",
            "19/19 [==============================] - 31s 2s/step - loss: 1.1666 - sparse_categorical_accuracy: 0.4033 - sparse_categorical_crossentropy: 3.3812\n",
            "dataset: 3\n",
            "15/15 [==============================] - 23s 2s/step - loss: 0.5867 - sparse_categorical_accuracy: 0.3648 - sparse_categorical_crossentropy: 3.1886\n",
            "dataset: 4\n",
            "15/15 [==============================] - 24s 2s/step - loss: 1.2378 - sparse_categorical_accuracy: 0.3547 - sparse_categorical_crossentropy: 3.2373\n",
            "dataset: 5\n",
            "120/120 [==============================] - 188s 2s/step - loss: 1.6303 - sparse_categorical_accuracy: 0.4587 - sparse_categorical_crossentropy: 3.1419\n",
            "dataset: 6\n",
            "55/55 [==============================] - 87s 2s/step - loss: 1.3037 - sparse_categorical_accuracy: 0.4993 - sparse_categorical_crossentropy: 3.2170\n",
            "dataset: 7\n",
            "72/72 [==============================] - 114s 2s/step - loss: 1.3687 - sparse_categorical_accuracy: 0.5048 - sparse_categorical_crossentropy: 3.1602\n",
            "dataset: 8\n",
            "285/285 [==============================] - 449s 2s/step - loss: 1.3943 - sparse_categorical_accuracy: 0.5827 - sparse_categorical_crossentropy: 3.1452\n",
            "dataset: 9\n",
            "71/71 [==============================] - 112s 2s/step - loss: 1.0169 - sparse_categorical_accuracy: 0.6528 - sparse_categorical_crossentropy: 3.1484\n",
            "dataset: 10\n",
            "1060/1060 [==============================] - 1599s 2s/step - loss: 1.1582 - sparse_categorical_accuracy: 0.6658 - sparse_categorical_crossentropy: 3.1066\n",
            "dataset: 11\n",
            "19/19 [==============================] - 29s 2s/step - loss: 1.0526 - sparse_categorical_accuracy: 0.5406 - sparse_categorical_crossentropy: 3.3425\n",
            "dataset: 12\n",
            "17/17 [==============================] - 25s 1s/step - loss: 1.1905 - sparse_categorical_accuracy: 0.4669 - sparse_categorical_crossentropy: 3.4182\n",
            "dataset: 13\n",
            "43/43 [==============================] - 64s 1s/step - loss: 1.0204 - sparse_categorical_accuracy: 0.6004 - sparse_categorical_crossentropy: 3.1633\n",
            "dataset: 14\n",
            " 50/240 [=====>........................] - ETA: 4:49 - loss: 1.7948 - sparse_categorical_accuracy: 0.4853 - sparse_categorical_crossentropy: 3.2095"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Shv0hEXg8YQ9"
      },
      "source": [
        "Create Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy9kNuIz8XoD"
      },
      "source": [
        "prediction_model = get_prediction_model(trained_model=training_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikV04Jc_85RI"
      },
      "source": [
        "Generate Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQpDzr0x81-3"
      },
      "source": [
        "input_text = 'She '\r\n",
        "\r\n",
        "generator(input_text=input_text, \r\n",
        "          prediction_model=prediction_model, \r\n",
        "          precision_reduction=0, \r\n",
        "          num_characters=250, \r\n",
        "          tokenizer=None, \r\n",
        "          max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "          num_words=NUM_TRAILING_WORDS, \r\n",
        "          print_result=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFowEIJgUHN8"
      },
      "source": [
        "Save Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQK0uZ4jUGbf"
      },
      "source": [
        "save_model_now = True\r\n",
        "\r\n",
        "if save_model_now:\r\n",
        "    # training model\r\n",
        "    save_model(training_model, model_dir=TRAINING_MODEL_DIR)\r\n",
        "\r\n",
        "    # prediction model\r\n",
        "    save_model(prediction_model, model_dir=PREDICTION_MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8-75YmvdqmG"
      },
      "source": [
        "Anvil Web App Server Integration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS26o45XdoiI"
      },
      "source": [
        "# get tokenizer\r\n",
        "tokenizer = create_character_tokenizer()\r\n",
        "\r\n",
        "@anvil.server.callable\r\n",
        "def generate_text(starting_text, precision_reduction,\r\n",
        "                    prediction_tokenizer=tokenizer, \r\n",
        "                    prediction_model=prediction_model,\r\n",
        "                    print_result=True,\r\n",
        "                    author='assorted'):\r\n",
        "\r\n",
        "    # set length of generated text\r\n",
        "    num_generation_steps = 350\r\n",
        "\r\n",
        "    # format user input\r\n",
        "    starting_text = starting_text.upper() + ': '\r\n",
        "\r\n",
        "    # get generated text\r\n",
        "    # very rarely this produces too few line breaks, \r\n",
        "    # causing an indexing error. The exception below reruns it as needed\r\n",
        "    while Tre:\r\n",
        "        try:\r\n",
        "            prediction = generator(input_text=starting_text, \r\n",
        "                                prediction_model=prediction_model, \r\n",
        "                                precision_reduction=precision_reduction, \r\n",
        "                                num_characters=num_generation_steps, \r\n",
        "                                tokenizer=prediction_tokenizer, \r\n",
        "                                max_len=PADDED_EXAMPLE_LENGTH, \r\n",
        "                                num_words=NUM_TRAILING_WORDS, \r\n",
        "                                print_result=print_result)        \r\n",
        "            break\r\n",
        "        except:\r\n",
        "            pass                      \r\n",
        "    \r\n",
        "    output = starting_text + '\\n'\r\n",
        "    split_on = ['?', '.', ',', ';', '!', ':']\r\n",
        "    splits = '([' + ''.join(split_on) + '])'\r\n",
        "    split_lines_prediction = re.split(splits, prediction)\r\n",
        "    \r\n",
        "    for line in split_lines_prediction:\r\n",
        "        line_update = line[0].upper()\r\n",
        "        try: \r\n",
        "            line_update += line[1:]\r\n",
        "        except:\r\n",
        "            pass\r\n",
        "        if line_update[-1] in split_on or line_update[-2:] == '\\n':\r\n",
        "            output= ''.join([output, line_update])\r\n",
        "        else:\r\n",
        "            output= '\\n'.join([output, line_update])\r\n",
        "    \r\n",
        "    return output + '... '\r\n",
        "\r\n",
        "if USE_ANVIL:\r\n",
        "    # start persistent connection to server\r\n",
        "    anvil.server.wait_forever()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f09DLIWc4xAM"
      },
      "source": [
        "starting_text = 'She walks the line'\r\n",
        "precision_reduction = .2\r\n",
        "\r\n",
        "gen = generate_text(starting_text=starting_text, \r\n",
        "              precision_reduction=precision_reduction,\r\n",
        "              print_result=False)\r\n",
        "\r\n",
        "print(gen)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}